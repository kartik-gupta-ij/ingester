[
  {
    "threadId": "1291767952600731720",
    "name": "How to trigger indexing of the payload index on an already populated collection?",
    "messages": "I have the filled collection, I've created the payload index, but the dashboard shows 0 points, and filter on this field so slow. Also not every point has this field in the payload."
  },
  {
    "threadId": "1291413324205920286",
    "name": "Can I store multiple image vectors for a point and use an average vector for search?",
    "messages": "Is it possible to store multiple vectors for one point, and when I search using another vector, it doesn't just compare two individual vectors, but creates some kid of average from the multiple vectors stored  at that point? Similar to how the recommendation API works, but applied in the database.\n\nI'm working on an image-based product recommendation system. Each product has multiple images, and using all of them might improve the search results. I've seen the recommendation api where you can provide multiple positive and negative examples during the search what is great, and I've also read that we can store multiple vectors and search them by selecting one. My question is: can I store multiple image vectors for one pint (product) and, when searching, have the system create an average of those vectors to improve the search accuracy like as positive examples?"
  },
  {
    "threadId": "1291140285442691112",
    "name": "Finding ways to improve search speed",
    "messages": "Hi Qdrant team!\n\nI'm recently trying to see if the search speed for some of our collections can be further improved. Currently the collection has P99 search latency ~90ms and P50 ~25ms, the qdrant cluster version is on v1.11.5, and we're calling search API via golang client version v1.11.1. The information of a typical search request is:\n\n```\nLimit:             640\nSearchFilter:      must:{field:{key:\"timestamp\"  range:{gte:1.727808543508965e+15}}}  must:{field:{key:\"topic_id\"  match:{integers:{integers:7249  integers:4202  integers:5319  integers:414366  integers:179097  integers:948  integers:15617  integers:2594  integers:78928  integers:76221  integers:65314  integers:1958  integers:2307  integers:4600  integers:40427  integers:786  integers:1797  integers:2224055  integers:1996  integers:2128  integers:9578  integers:918  integers:1672953  integers:788  integers:980  integers:376915  integers:7173  integers:87257  integers:19726  integers:1759927}}}}\nIncludeVectors:    false\nIncludePayload:    true\nSearchExact:       false\nHnswEf:            200\nScoreThreshold:    nil\nUseIndexedOnly:    true\nWithPayloadFields: [timestamp parent_id topic_id]\n```\n\nI'll provide some more details in the comment."
  },
  {
    "threadId": "1291464137301557371",
    "name": "Need Help Storing and Retrieving Q&A Data with SQL Table Integration for LLM Processing",
    "messages": "I am working on a project where I need to store Q&A data in CSV format, including questions and answers, along with SQL table definitions. The goal is to perform a similarity search on the stored Q&A when a user asks a question, then reference the relevant SQL table definitions to fetch data and feed it into my LLMs.\n\nCould someone assist me in finding the best approach to store this data and retrieve it in an optimized and efficient manner?"
  },
  {
    "threadId": "1290626871318548511",
    "name": "Use case and Limitations of Various Search Techniques - Discovery, Nearest, Context, Recommendation",
    "messages": "I have read about various searching techniques supported by qdrant, however I am specifically looking for use cases for each of the search techniques mentioned below, when to use particular technique, what are its limitations \n1) Discovery \n2) Nearest \n3) Context \n4) Recommendation \n\nIf I missed out any other prominent techniques, please include those its features and limitations as well"
  },
  {
    "threadId": "1290925252414537808",
    "name": "Slow Search Speed",
    "messages": "Hello!\n\nWe have some search performance issue.\nOur collection size is 327_167, each point has categories_ids (array of int) in payload and parametrized index for it (config below).\nWith small RPS (around 30) search operations take approximately _1 second_.\nExample of request (go client, version: v1.9.0):\n```\n&pb.SearchPoints{\n    CollectionName: \"some-collection-name\",\n    Vector:         customerEmbeddings,\n    Limit:          uint64(limit), // limit here is 50\n    Filter: &pb.Filter{\n        Must: []*pb.Condition{\n            {\n                ConditionOneOf: &pb.Condition_Field{\n                    Field: &pb.FieldCondition{\n                        Key: \"categories_ids\",\n                        Match: &pb.Match{\n                            MatchValue: &pb.Match_Integer{\n                                Integer: int64(categoryID),\n                            },\n                        },\n                    },\n                },\n            },\n        },\n    },\n    WithVectors: &pb.WithVectorsSelector{SelectorOptions: &pb.WithVectorsSelector_Enable{Enable: false}},\n    WithPayload: &pb.WithPayloadSelector{SelectorOptions: &pb.WithPayloadSelector_Enable{Enable: true}},\n}\n```\nImportant notice: is some cases we can send around 100 requests like above with different categoryID while processing single request to our API (if my calculations correct - it's about 3K RPS for Qdrant)\n\nFurther details in comment"
  },
  {
    "threadId": "1288791548531576833",
    "name": "What can impoove HNSW re-index performace?",
    "messages": "I want to undestand what can I execute to refresh index once a day, and re-indexing can happen as quick as possible.\nlets say I have 7M points with a single vector with some payload on disk. Each day I'm updating from 0.1% up to 1% of thgose vectors, and sometimes it takes 12 hours or even more. I have a cluster with 6 Nodes (3CPU, 16GB RAM).\nI'm using m and ef_construct quite high I suupose lowering them can improove indexing. But if I'd like to keep those parametrs high does adding more CPU or RAM can improove speed drastically?\nCollection:\n{\n  \"result\": {\n    \"status\": \"yellow\",\n    \"optimizer_status\": \"ok\",\n    \"indexed_vectors_count\": 3499854,\n    \"points_count\": 7408961,\n    \"segments_count\": 4,\n    \"config\": {\n      \"params\": {\n        \"vectors\": {\n          \"size\": 50,\n          \"distance\": \"Cosine\",\n          \"datatype\": \"float16\"\n        },\n        \"shard_number\": 2,\n        \"replication_factor\": 3,\n        \"write_consistency_factor\": 3,\n        \"on_disk_payload\": true\n      },\n      \"hnsw_config\": {\n        \"m\": 64,\n        \"ef_construct\": 256,\n        \"full_scan_threshold\": 10000,\n        \"max_indexing_threads\": 0,\n        \"on_disk\": false\n      },\n      \"optimizer_config\": {\n        \"deleted_threshold\": 0.2,\n        \"vacuum_min_vector_number\": 1000,\n        \"default_segment_number\": 2,\n        \"max_segment_size\": null,\n        \"memmap_threshold\": null,\n        \"indexing_threshold\": 20000,\n        \"flush_interval_sec\": 5,\n        \"max_optimization_threads\": null\n      },\n      \"wal_config\": {\n        \"wal_capacity_mb\": 32,\n        \"wal_segments_ahead\": 0\n      },\n      \"quantization_config\": null\n    },\n    \"payload_schema\": {}\n  },\n  \"status\": \"ok\",\n  \"time\": 0.001529609\n}"
  },
  {
    "threadId": "1290600454157504533",
    "name": "Will replication help me support more requests per second?",
    "messages": "Hi!\nI have a cluster with two nodes currently (might grow with time). The collection I am serving is in itself not very large (<100k vectors) but I get very bursty number of requests depending on the number of agents running and querying the collection. What is the preferred setup for supporting a large number of requests? Should I have more shards on many nodes or the same shards replicated to several nodes? How does the load balancing work and how should I think about this to get optimal performance?"
  },
  {
    "threadId": "1291067124969771071",
    "name": "Internal server error on Scroll API",
    "messages": "When I try to call api scroll, I get the following error.\nThis error happens a lot today. What is the subject?\n\nI want to say that I am using the business plan and 7 gigs of the 8 gigs of RAM are full. Could it be because of resources?\n\nThank you for your guidance\n\n\ncode:\n\n  const scrollRes = await qdrantClient.scroll(collectionName, {\n    filter: finalFilter,\n    limit: 20000,\n    with_payload: true,\n    with_vector: false,\n  });\n\n\nerror:\n\n\n[2024-10-02 19:19:54] тип l [Error]: Internal Server Error\n[2024-10-02 19:19:54] at Object.e [as scrollPoints] (/app/.next/server/chunks/7053.js:1:3288)\n[2024-10-02 19:19:54] at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\n.......\n[2024-10-02 19:19:54] status: 500,\n[2024-10-02 19:19:54] statusText: 'Internal Server Error',\n[2024-10-02 19:19:54] data: {\n[2024-10-02 19:19:54] status: {\n[2024-10-02 19:19:54] error: 'Service internal error: 1 of 1 read operations failed:\\n' +\n[2024-10-02 19:19:54] \" Timeout error: Operation 'scroll_by_id' timed out after 60 seconds\"\n[2024-10-02 19:19:54] },\n[2024-10-02 19:19:54] time: 60.006781951\n[2024-10-02 19:19:54] }\n[2024-10-02 19:19:54] }"
  },
  {
    "threadId": "1290798059890147380",
    "name": "Mass Deletion",
    "messages": "Hi, is it safe to delete tens of millions of vectors from a collection in a single request, or would it be better to batch these requests  (into 1000 deletions per request, for example)? By \"safe\", I mean will it cause the vector db to crash, hang, slow down, etc?"
  },
  {
    "threadId": "1288798684427059261",
    "name": "Distances when using multi vectors collections",
    "messages": "Hello, started playing with multi vector collection recently and noticed that in some cases cosine distance was > 1 even tho all my embeddings were being normalised.\n\nI suppose it comes from max_sim function that is used to sort between all the vectors of different points but I am not sure I got how it works in some use cases.\nIf I have a score of 3 does it mean that I had a cosine distance = 1 for 3 different vectors of the two points ? \nAlso how does it work if I have one point with one vector with cosine = 1 but an other one with two points with cosine = 0.9 and 0.9 ? Would it return the first one as the best match or the second one ?\n\nThanks again for the great work"
  },
  {
    "threadId": "1301594213535842358",
    "name": "How do I check my collection storage? How do I know how much space I have left?",
    "messages": "How do I check my collection storage? How do I know how much space I have left?"
  },
  {
    "threadId": "1288442253689229343",
    "name": "Deploy Qdrant on Prem using Docker Swarm with 3 nodes",
    "messages": "Hi Folks,\n\nWe are quite new to Qdrant. We have deployed the qdrant as a one node on prem server and now we want to deploy it as a docker swarm. \n\nCan you guys share any pre-requisites or any thing which we need to care about before deploying it as a docker swarm\n\nThanks"
  },
  {
    "threadId": "1290395646762684516",
    "name": "Principal Index issues",
    "messages": "Hi Folks, \nI am trying to create Principal Index as specified in the https://qdrant.tech/documentation/concepts/indexing/?q=delete_payload_index\nclient.create_payload_index(\n    collection_name=\"{collection_name}\",\n    field_name=\"timestamp\",\n    field_schema=models.KeywordIndexParams(\n        type=\"integer\",\n        is_principal=True,\n    ),\n)\nBut it gave error that no additional argument is_principal allowed. SInce by field is an integer I changes it to \n    client.create_payload_index(\n        collection_name = 'collectionName',\n        field_name = \"deptID\",\n        field_schema = models.IntegerIndexParams(type=\"integer\", is_principal=True, lookup=True, range=False)\n    )\nHad to add lookup and range to avoid exception, selected lookup as true as I needed exact match on the field.\nThis ran without error. It is fine or am I missing something?"
  },
  {
    "threadId": "1290629671398146091",
    "name": "Retrieve all nodes based on document_name",
    "messages": "Hello. Which is the best way to retrieve all the nodes based on a filter metadata? \nImagine that the service can be used simultaneously by 100+ users . \nWhat happens if the documents has 100 pages?\n\nThanks,\nAlex"
  },
  {
    "threadId": "1290579758261993524",
    "name": "Multi Node Creation",
    "messages": "Getting this error on docker : \n\n2024-10-01T07:37:31.819301Z WARN qdrant::consensus: Failed to recover from peer with id 3616897732423713 at http://qdrant-node-1:6335/ with error Err(Failed to add peer to known: status: Internal, message: \"Failed to add peer: Service internal error: Failed to propose operation: leader is not established within 10 secs\", details: [], metadata: MetadataMap { headers: {\"content-type\": \"application/grpc\", \"date\": \"Tue, 01 Oct 2024 07:37:31 GMT\", \"content-length\": \"0\"} }), trying others\n\nDockerfile :\n\nversion: '3.8'\n\nservices:\n\n  qdrant-node-1:\n\n    container_name: qdrant-node-1\n\n    image: qdrant/qdrant:latest\n\n    ports:\n\n      - \"6333:6333\"\n\n    volumes:\n\n      - ./qdrant_storage_1:/qdrant/storage\n\n    environment:\n\n      - QDRANT__CLUSTER__ENABLED=true\n\n    command: ./qdrant --uri 'http://qdrant-node-1:6335'\n\n  qdrant-node-2:\n\n    container_name: qdrant-node-2\n\n    image: qdrant/qdrant:latest\n\n    volumes:\n\n      - ./qdrant_storage_2:/qdrant/storage\n\n    environment:\n\n      - QDRANT__CLUSTER__ENABLED=true\n\n    depends_on:\n\n      - qdrant-node-1\n\n    command: ./qdrant  --bootstrap 'http://qdrant-node-1:6335' --uri 'http://qdrant_node-2:6335'\n\n\n  qdrant-node-3:\n\n    container_name: qdrant-node-3\n\n    image: qdrant/qdrant:latest\n\n    volumes:\n\n      - ./qdrant_storage_3:/qdrant/storage\n\n    depends_on:\n\n      - qdrant-node-1\n\n    environment:\n\n      - QDRANT__CLUSTER__ENABLED=true\n\n    command: ./qdrant  --bootstrap 'http://qdrant-node-1:6335' --uri 'http://qdrant_node-3:6335'\n\n\n  qdrant-node-4:\n\n    container_name: qdrant-node-4\n\n    image: qdrant/qdrant:latest\n\n    volumes:\n\n      - ./qdrant_storage_4:/qdrant/storage\n\n    environment:\n\n      - QDRANT__CLUSTER__ENABLED=true\n\n    command: ./qdrant  --bootstrap 'http://qdrant-node-1:6335' --uri 'http://qdrant_node-4:6335'"
  },
  {
    "threadId": "1290677221480071271",
    "name": "ramg@host1:~/yaml$ kubectl logs qdrant-demo-8 -n qdrantDefaulted container \"qdrant\" out of: qdrant,",
    "messages": "2024-10-01T13:53:03.658846Z  INFO storage::content_manager::consensus::persistent: Loading raft state from ./storage/raft_state.json    \n2024-10-01T13:53:03.682370Z  INFO storage::content_manager::toc: Loading collection: order_collection    \n2024-10-01T13:55:47.229857Z  INFO collection::shards::local_shard: Recovering collection order_collection: 0/5 (0%)    \n2024-10-01T13:55:47.302016Z  INFO collection::shards::local_shard: Recovered collection order_collection: 5/5 (100%)    \n2024-10-01T13:58:33.931667Z  INFO collection::shards::local_shard: Recovering collection order_collection: 0/5 (0%)    \n2024-10-01T13:58:34.014620Z  INFO collection::shards::local_shard: Recovered collection order_collection: 5/5 (100%)    \n2024-10-01T13:58:34.030594Z  INFO qdrant: Telemetry reporting enabled, id: 2f101bc1-528e-411c-a5c5-2d4d8fe9ab7f    \n2024-10-01T13:58:34.030678Z  INFO qdrant::tonic: TLS disabled for internal gRPC API    \n2024-10-01T13:58:34.031537Z  INFO qdrant::actix: TLS disabled for REST API    \n2024-10-01T13:58:34.031580Z  INFO qdrant::actix: Qdrant HTTP listening on 6333    \n2024-10-01T13:58:34.031594Z  INFO actix_server::builder: Starting 63 workers\n2024-10-01T13:58:34.031599Z  INFO actix_server::server: Actix runtime found; starting in Actix runtime\n2024-10-01T13:58:34.035211Z  INFO qdrant::tonic: Qdrant gRPC listening on 6334    \n2024-10-01T13:58:34.035233Z  INFO qdrant::tonic: TLS disabled for gRPC API    \n2024-10-01T13:58:34.204977Z  INFO storage::content_manager::consensus::persistent: Replaced metadata of peer 777035458253464 from PeerMetadata { version: Version { major: 1, minor: 11, patch: 3 } } to PeerMetadata { version: Version { major: 1, minor: 11, patch: 5 } }    \n\nWhen one pod starts it throws another pod out of running \nThis is happening for some time now. Not able to start cluster"
  },
  {
    "threadId": "1290307584082968576",
    "name": "If we try to insert 3 vectors at a time using batch insertion, and out of that o",
    "messages": "Any Guide/documentation link on the above topic would be highly appreciated"
  },
  {
    "threadId": "1289342515484426443",
    "name": "Issue with Point ID not being set when I specify it in my Document",
    "messages": "When I use langchain_qdrant QdrantVectorStore and langchain_core.documents Document and try to set the id in my Document with my own UUID and then build my collection with those Documents, Qdrant assigns its own Point ID instead of using my specified UUID. \n\nI'm not sure what I'm doing wrong, as I can use Postman to properly specify my Point IDs using the REST API, but it's not working with the python libs. I've included my code snippets below:\n\n                permid = result.get('internal_perm_id')\n                title = result.get('internal_title', 'No Title')\n                abstract = result.get('abstract_body', 'No Abstract')\n                journal = result.get('journal_journal_name', 'No Journal')\n                uuid = string_to_uuid(permid)\n                \n                doc = Document(\n                    id=uuid,\n                    page_content=abstract,\n                    metadata={\n                        'uuid': uuid,\n                        'internal_perm_id': permid,\n                        'internal_title': title,\n                        'journal_journal_name': journal\n                    }\n                )\n                documents.append(doc)\n\ndef create_vectorstore(documents, batch_size=5000):\n    # Create embeddings using OpenAI's smallest model\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", \n                                      dimensions=512)\n\n    # Function to process documents in batches\n    def start_batch(documents):\n        return QdrantVectorStore.from_documents(\n            documents=documents, \n            embedding=embeddings,\n            collection_name=\"recommendations_vector\",\n            url=\"<url>\",\n            port=6333,\n            api_key=os.getenv(\"QDRANT_API_KEY\")\n        )\n\n    # Function to process documents in batches\n    def process_batch(vectorstore, documents):\n        vectorstore.add_documents(\n            documents=documents \n        )"
  },
  {
    "threadId": "1290283826756583456",
    "name": "Why qdrant is not building hnsw index even if we decrease indexing_threshold_kb to 1.?",
    "messages": "I have been trying to insert 40 k vectors and I have set hnsw config but still when I checked segment.json file in storage directory it prints index plain instead of hnsw.! Why is it so.?"
  },
  {
    "threadId": "1286918729673740340",
    "name": "Optimizing search speed",
    "messages": "Hi, I am trying to optimize my search time for the following collection. I have around 80M embeddings, which will increase in the future. Currently, it takes about 10 seconds, and I would like to reduce this as much as possible. I am aware that quantization and increasing SSD speed on the server are potential improvements, which I will make anyway. However, I'm wondering if there are any other strategies to improve search speed?\n\nCollection Information:\n\n`\n{\n  \"result\": {\n    \"status\": \"green\",\n    \"optimizer_status\": \"ok\",\n    \"indexed_vectors_count\": 80166220,\n    \"points_count\": 79750290,\n    \"segments_count\": 200,\n    \"config\": {\n      \"params\": {\n        \"vectors\": {\n          \"size\": 1152,\n          \"distance\": \"Cosine\"\n        },\n        \"shard_number\": 1,\n        \"replication_factor\": 2,\n        \"write_consistency_factor\": 1,\n        \"on_disk_payload\": true\n      },\n      \"hnsw_config\": {\n        \"m\": 16,\n        \"ef_construct\": 100,\n        \"full_scan_threshold\": 10000,\n        \"max_indexing_threads\": 0,\n        \"on_disk\": true\n      },\n      \"optimizer_config\": {\n        \"deleted_threshold\": 0.2,\n        \"vacuum_min_vector_number\": 1000,\n        \"default_segment_number\": 0,\n        \"memmap_threshold\": 10000,\n        \"indexing_threshold\": 20000,\n        \"flush_interval_sec\": 5,\n        \"max_optimization_threads\": null\n      },\n      \"wal_config\": {\n        \"wal_capacity_mb\": 32,\n        \"wal_segments_ahead\": 0\n      },\n      \"quantization_config\": null\n    },\n    \"payload_schema\": {\n      \"tag_xxx1\": {\n        \"data_type\": \"keyword\",\n        \"params\": {\n          \"type\": \"keyword\"\n        },\n        \"points\": 0\n      },\n      \"tag_xxx2\": {\n        \"data_type\": \"keyword\",\n        \"params\": {\n          \"type\": \"keyword\"\n        },\n        \"points\": 0\n      }\n    }\n  },\n  \"status\": \"ok\",\n  \"time\": 0.010270819\n}`"
  },
  {
    "threadId": "1299406624515162112",
    "name": "Snapshot recovery issues for collection with custom sharding and created shards",
    "messages": "Hello, I'm testing snapshot recovery on the collection with custom sharding and added shards and recovered collection has `shard_count = 0`  which causing shards recovery to throw `Shard not found` errors. \n\nCollection set up with custom sharding and multiple shards created through `/collections/{collectionName}/shards` endpoint\n\nI would appreciate any advice on potential fix"
  },
  {
    "threadId": "1288744880998449278",
    "name": "Slow collection data loading",
    "messages": "Hi, I'm trying to load data of 35M vectors (dense with 768 D + sparse) using Pyspark connector. \nthe loading is prolonged it has been running for 36 hours and only 18M records have been loaded into the collection. \n\nAttached collection configuration:\n{\n\"params\":{\n\"vectors\":{\n\"dense_embeddings\":{\n\"size\":768\n\"distance\":\"Cosine\"\n}\n}\n\"shard_number\":3\n\"replication_factor\":1\n\"write_consistency_factor\":1\n\"on_disk_payload\":true\n\"sparse_vectors\":{\n\"sparse_embeddings\":{\n\"index\":{}\n}\n}\n}\n\"hnsw_config\":{\n\"m\":16\n\"ef_construct\":100\n\"full_scan_threshold\":10000\n\"max_indexing_threads\":0\n\"on_disk\":false\n}\n\"optimizer_config\":{\n\"deleted_threshold\":0.2\n\"vacuum_min_vector_number\":1000\n\"default_segment_number\":0\n\"max_segment_size\":\nNULL\n\"memmap_threshold\":\nNULL\n\"indexing_threshold\":0\n\"flush_interval_sec\":5\n\"max_optimization_threads\":\nNULL\n}\n\"wal_config\":{\n\"wal_capacity_mb\":32\n\"wal_segments_ahead\":0\n}\n\"quantization_config\":\nNULL\n}\n\nany help?\n<@791281231987343370>"
  },
  {
    "threadId": "1289676034127560767",
    "name": "How to filter points without vector?",
    "messages": "I need points which don't have vector"
  },
  {
    "threadId": "1289187259878477824",
    "name": "How to use qdrant in android",
    "messages": "I wanna use qdrant as a vector db on edge device. Whats the best way to do it ?"
  },
  {
    "threadId": "1288797518745829468",
    "name": "Payload structure",
    "messages": "Summarize the issue. Describe what you are trying to do and how you are being blocked. Let us know what should have been the result:\n\nI currently have one collection in which all the points have the below keys in the payload: \"content\", \"metadata\" & \"product_id\".\nMy question:\nCan I create new points in the same collection with different keys in the payload like: \"content\" & \"metadata\" (i.e. only 2 keys instead of 3 keys\nor 4 keys instead of 3 in another case).\n\nAre you using Qdrant in local mode, with Docker or Qdrant Cloud? If you are using Qdrant Cloud, please provide the cluster id.\n\nI am using Qdrant cloud. My cluster ID: 220c4b81-26d5-4bf6-a389-c96806271863\n\nWhich Qdrant version are you running? Sometimes the issue happens because you are not running the latest version.\n\nI am using Qdrant cloud. Where can I get the Qdrant version?\n\nHow are you connecting to Qdrant? Are you using a client?\n\nWe are using Python client.\n\nProvide your current database configuration. What are the current collection settings? How many vectors in a collection?In case of a distributed setup, describe shards, replication factor etc.\n\n{\"status\":\"green\",\"optimizer_status\":\"ok\",\"indexed_vectors_count\":0,\"points_count\":167,\"segments_count\":2,\"config\":{\"params\":{\"vectors\":{\"size\":1536,\"distance\":\"Cosine\"},\"shard_number\":1,\"replication_factor\":1,\"write_consistency_factor\":1,\"on_disk_payload\":true},\"hnsw_config\":{\"m\":16,\"ef_construct\":100,\"full_scan_threshold\":10000,\"max_indexing_threads\":0,\"on_disk\":false},\"optimizer_config\":{\"deleted_threshold\":0.2,\"vacuum_min_vector_number\":1000,\"default_segment_number\":0,\"max_segment_size\":null,\"memmap_threshold\":null,\"indexing_threshold\":20000,\"flush_interval_sec\":5,\"max_optimization_threads\":null},\"wal_config\":{\"wal_capacity_mb\":32,\"wal_segments_ahead\":0},\"quantization_config\":null},\"payload_schema\":{}}"
  },
  {
    "threadId": "1288785973634662402",
    "name": "PyStemmer broken ?",
    "messages": "Hello just got an issue rebuilding on docker 10min ago with PyStemmer package that FastEmbed that i use depends on ?\n```Collecting PyStemmer<3.0.0,>=2.2.0\n  Downloading PyStemmer-2.2.0.tar.gz (698 kB)\n     тФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБ 698.7/698.7 KB 224.2 MB/s eta 0:00:00\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'error'\n  error: subprocess-exited-with-error\n  \n  ├Ч python setup.py egg_info did not run successfully.\n  тФВ exit code: 1\n  тХ░тФА> [10 lines of output]\n      Traceback (most recent call last):\n        File \"<string>\", line 2, in <module>\n        File \"<pip-setuptools-caller>\", line 34, in <module>\n        File \"/tmp/pip-install-s9r0gttc/pystemmer_a0386bc2dbfa4b21ac07ec6b1491b579/setup.py\", line 199, in <module>\n          ['src/Stemmer.pyx'] + list(LIBRARY_SOURCE_CODE.source_code_paths()),\n        File \"/tmp/pip-install-s9r0gttc/pystemmer_a0386bc2dbfa4b21ac07ec6b1491b579/setup.py\", line 85, in source_code_paths\n          for line in self.iter_manifest_lines():\n        File \"/tmp/pip-install-s9r0gttc/pystemmer_a0386bc2dbfa4b21ac07ec6b1491b579/setup.py\", line 74, in iter_manifest_lines\n          with open(self.manifest_file_path) as file:\n      FileNotFoundError: [Errno 2] No such file or directory: 'libstemmer_c-2.2.0/mkinc_utf8.mak'\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: metadata-generation-failed\n\n├Ч Encountered error while generating package metadata.\nтХ░тФА> See above for output.\n\nnote: This is an issue with the package mentioned above, not pip.\nhint: See above for details.```\n\nAlso the latest release of pystemmer hes not 3.0.0 anymore on pip but 2.2.0.1.\n\nN.B. : this issue has just come 10min ago, but its working fine for weeks ( if you ask yourself its coming from me or not )"
  },
  {
    "threadId": "1288400882853089290",
    "name": "Output returned differ from official document",
    "messages": "Hi, sorry to bother if it is my misunderstanding. \nBy following the [official quickstart document](https://qdrant.tech/documentation/quickstart/), i found the output i get from `client.query_points` differs from the output given, as shown below. \n\n**Executed code**:\n```\nsearch_result = client.query_points(\n    collection_name=\"test_collection\", query=[0.2, 0.1, 0.9, 0.7], limit=3\n).points\n\nprint(search_result)\n```\n**Official response**;\n```\n[\n  {\n    \"id\": 4,\n    \"version\": 0,\n    \"score\": 1.362,\n    \"payload\": null,\n    \"vector\": null\n  },\n  {\n    \"id\": 1,\n    \"version\": 0,\n    \"score\": 1.273,\n    \"payload\": null,\n    \"vector\": null\n  },\n  {\n    \"id\": 3,\n    \"version\": 0,\n    \"score\": 1.208,\n    \"payload\": null,\n    \"vector\": null\n  }\n]\n```\n**My output**:\n```\n[ScoredPoint(id=4, version=0, score=1.362, payload={'city': 'New York'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=1, version=0, score=1.273, payload={'city': 'Berlin'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=3, version=0, score=1.208, payload={'city': 'Moscow'}, vector=None, shard_key=None, order_value=None)]\n```\n\nIs there any solution to make my output match the official one?"
  },
  {
    "threadId": "1288050570598416396",
    "name": "Streaming multi-vector insertion",
    "messages": "I am working with a ray processing job to compute embedding and insert them in my multi vector collection.\n\nIs there any way to insert vectors into this collection without having to group by point_id before calling the API so that I dont have materialize everything into memory and keep a streaming process ?\n\nThanks"
  },
  {
    "threadId": "1288488967611748514",
    "name": "Filter from list of uids",
    "messages": "I have a collection with payload ={uids: [Str,Str..]}\n\nI need to search using filtering with one or multiple uids so for example:\n\n\nVector a = payload {uid : [Maria, John, Steve]}\nVector b = payload {uid : [Maria]}\nVector c = payload {uid : [Helen, John, Matias]}\nVector d = payload {uid : [John, Maria]}\n\nAnd then search using filtering  Maria and John so that I get back vector A and  vector D\n\nWhich method can I use to do so??"
  },
  {
    "threadId": "1265631598448218173",
    "name": "ISS / Linux on-prem deployment w/o Docker",
    "messages": "Hi, is it possible to deploy Qdrant locally for IIS and Linux without using Docker"
  },
  {
    "threadId": "1288354540604756061",
    "name": "is there a way to filter based on payload containing a key?",
    "messages": "is there a way to filter based on payload containing a key?\n\nMy point have different payloads and I need points with a specific key. \n\nI looked here but couldn't find anything\nhttps://qdrant.tech/documentation/concepts/filtering"
  },
  {
    "threadId": "1286251834859589664",
    "name": "Can't connect to cluster on qdrant's cloud from self-hosted cloud using QdrantClient",
    "messages": "Using node + QdrantClient, requests will be timeout on container of our self-hosted k8s cluster. But if I docker run same image on my local PC, it can connect. Also I can connect using curl on the same container. Below is the error log: \n```\n[Nest] 1  - 09/19/2024, 5:38:01 PM   ERROR [ExceptionsHandler] fetch failed\nTypeError: fetch failed\n    at Object.fetch (node:internal/deps/undici/undici:11372:11)\n    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\n    at async fetchJson (/app/node_modules/@qdrant/openapi-typescript-fetch/dist/cjs/fetcher.js:135:22)\n    at async /app/node_modules/@qdrant/js-client-rest/dist/cjs/api-client.js:46:26\n    at async handler (/app/node_modules/@qdrant/openapi-typescript-fetch/dist/cjs/fetcher.js:156:16)\n    at async /app/node_modules/@qdrant/js-client-rest/dist/cjs/api-client.js:32:24\n    at async handler (/app/node_modules/@qdrant/openapi-typescript-fetch/dist/cjs/fetcher.js:156:16)\n    at async fetchUrl (/app/node_modules/@qdrant/openapi-typescript-fetch/dist/cjs/fetcher.js:162:22)\n    at async Object.fun [as getCollections] (/app/node_modules/@qdrant/openapi-typescript-fetch/dist/cjs/fetcher.js:168:20)\n    at async QdrantClient.getCollections (/app/node_modules/@qdrant/js-client-rest/dist/cjs/qdrant-client.js:836:26)\n```\nBelow is how I setup a QdrantClient:\n```\nexport class QdrantService {\n  private client: QdrantClient;\n\n  constructor() {\n    this.client = new QdrantClient({\n      url: 'https://d6d277a3-7d5a-4804-bc34-f20e2db404e5.europe-west3-0.gcp.cloud.qdrant.io',\n      apiKey: '{marked}',\n    });\n```"
  },
  {
    "threadId": "1270629514958999659",
    "name": "Is there any other way ( other than docker ) in which we can limit qdrant memory  usage.?",
    "messages": "Is there a way in which we can limit qdrant ram usage .?\nCan we stop qdrant from consuming available ram for caching.?"
  },
  {
    "threadId": "1270640780268273735",
    "name": "OrderBy or Sort (desc) data when get data from Collection",
    "messages": "Someone tell me tutorial to do sort data and pagination when get data from collection. Maybe use scroll or query_point... I try but I have this bug: \"grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n    status = StatusCode.INVALID_ARGUMENT\n    details = \"Wrong input: No range index for `order_by` key: `document`. Please create one to use `order_by`. Check https://qdrant.tech/documentation/concepts/indexing/#payload-index to see which payload schemas support Range conditions\"\n    debug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"Wrong input: No range index for `order_by` key: `document`. Please create one to use `order_by`. Check https://qdrant.tech/documentation/concepts/indexing/#payload-index to see which payload schemas support Range conditions\", grpc_status:3, created_time:\"2024-08-07T07:02:54.2954561+00:00\"}\"\n>\" I dont know how to fix this. please help me"
  },
  {
    "threadId": "1287691596732829717",
    "name": "disk usage",
    "messages": "Hey I would like to insert 50 million ,1028 dimensional vectors of float 16  and create hnsw index of m:16 ,Ef construction: 128. Everything will be stored on disk. How much disk do I need for that???\nThank you"
  },
  {
    "threadId": "1287397169107701850",
    "name": "Qdrant behind nginx reverse proxy",
    "messages": "I am using qdrant in a docker compose network and I have a nginx reverse proxy, that sends requests to my qdrant instance. \nwhat I want to achieve is, to have the UI available on my HA proxy. I have seen, that there is a known issue on the github page, that shows the exact behaviour, that I am experiencing:\nhttps://github.com/qdrant/qdrant-web-ui/issues/94\n\nSo basically whats happening, I am sending the request to \nhttps://myurl/qdrant/dashboard\n\nI am using the qdrant relative path to differentiate my ressources. \nIf I am removing the qdrqant in the proxy pass, what happens is, that the ui tries to find a ressource at:\nhttps://myurl/dashboard/manifest.json\n\nI was expecting that qdrant can handle such scenarios. Either by providing a url as environment variable or by using the forwarded headers of the proxy. I dont know if it does, but maybe there is a chance for improvement ЁЯЩВ \n\nAnyways, does someone have the same experience and knows how to handle this? Is there a hidden config or someone that has a good example of how to handle this in nginx?\n\nI would be very greatful for any help.\n\n\nNginx conf: \nlocation /qdrant {\n                rewrite ^/qdrant/(.*) /$1 break;\n                proxy_pass http://qdrant-db:6333;\n}"
  },
  {
    "threadId": "1286245163147988992",
    "name": "Qdrant front bug when using collections with multivectors",
    "messages": "Hello, it's not such a meaningful bug since it does not block from properly using the API but still, when using collection with multivectors search request by id do not seem to be working anymore. It automatically adds using multivector_config after id: X and returns\nтЪа Error: Wrong input: Not existing vector name error: multivector_config"
  },
  {
    "threadId": "1287682623593779232",
    "name": "hi folks",
    "messages": "How to delete a particular payload in qdrant?"
  },
  {
    "threadId": "1286729480714584075",
    "name": "Disable multi-tenant index during upload?",
    "messages": "I'm looking to configure a multi-tenant instance of qdrant with support for bulk uploads per tenant. I see the documentation for disabling the global index during bulk uploads [0], but I don't see any documentation for disabling a payload index during uploads [1]. Note that I want to disable a payload index for only one tenant during their bulk upload, while leaving other tenant indexes enabled. Is this possible, and if so how? \n\n[0] https://qdrant.tech/documentation/tutorials/bulk-upload/#disable-indexing-during-upload\n[1] https://qdrant.tech/documentation/guides/multiple-partitions/"
  },
  {
    "threadId": "1219814428753526904",
    "name": "Sparse Vectors on disk and other optimization tips for large scale processing?",
    "messages": "I'm running into trouble with resource limitations on my data processing pipeline.\nCurrently running a processing pipeline on computer with 64GB of RAM. Once you add in chunking / embedding processes while also hosting a qdrant image with a large amount of vectors the resource usage really starts to add up.\nI've tried to set everything to 'on disk'. Despite this the RAM still seems to increase. I am then experiencing various errors where things cannot process which appear to be related to limited resource availbility.\n\nSo far I have set\n- Dense vector to disk: True\n- Payload to disk: True\n- HNSW index to disk: True\n- Sparse vectors: Can sparse vectors be stored on disk rather than RAM? It's not clear what they are by default.\n\nCan anyone advise on the above or any other tips on how to manage data creation / upload pipelines while keeping resource usage low.\nScale of the dataset is in the order of (10M to 100M+ dense and sparse vectors). Can qdrant be set to an 'upload only' mode during processing?\n\nKind regards"
  },
  {
    "threadId": "1251053227047260160",
    "name": "sparse index is too large",
    "messages": "In our scenario, the sparse index is much larger than the dense index. Does the sparse index support compression, such as PForDelta, which is similar to dense quantization?"
  },
  {
    "threadId": "1285257057301172335",
    "name": "connection errors during embedding",
    "messages": "I try to load approx. 5 million documents into qdrant they are all very small in the range to up to 2000 characters. most are much smaller, after a while it gives me http connection errors and I have to start again since I have no idea what was embedded and what not, I load chunks of 2000 documents at once using llama-index. I was using 1.10 now I went to 1.11 to see if this solves the problem. can I do anything to ease the pain qdrant might have? my guess is it gets busy working on indexes and then drops http connections."
  },
  {
    "threadId": "1286674865067135058",
    "name": "100M 1024d vectors on self hosted qdrant server?",
    "messages": "Hi everyone.\nMy use case is a rag system recommending top 1000 documents from a 100million points database. I would like to know if this would be viable to have this on a self hosted qdrant server.\n\nI succeeded to upload a collection of all 100M embeddings of dim 1024 and fp16 (took around 100mn with proper multiproc) on disk, and tried to have quant indexes on memory (I tried int8 and binary), but I can't succeed to do a simple query search from a vector and always get timeout (that I set at 600s)\n\nI'm wondering if I was doing something bad, as I was expecting that with correct indexing and quantization, it should be possible to get results in less than a few seconds.\n\nHere is the config of my collection :\n```\n{\n  \"status\": \"green\",\n  \"optimizer_status\": \"ok\",\n  \"vectors_count\": null,\n  \"indexed_vectors_count\": 6436000,\n  \"points_count\": 80000000,\n  \"segments_count\": 50,\n  \"config\": {\n    \"params\": {\n      \"vectors\": {\n        \"size\": 1024,\n        \"distance\": \"Cosine\",\n        \"hnsw_config\": null,\n        \"quantization_config\": null,\n        \"on_disk\": true,\n        \"datatype\": \"float16\",\n      },\n      \"shard_number\": 1,\n      \"sharding_method\": null,\n      \"replication_factor\": 1,\n      \"write_consistency_factor\": 1,\n      \"read_fan_out_factor\": null,\n      \"on_disk_payload\": true,\n      \"sparse_vectors\": null\n    },\n    \"hnsw_config\": {\n      \"m\": 32,\n      \"ef_construct\": 32,\n      \"full_scan_threshold\": 10000,\n      \"max_indexing_threads\": 0,\n      \"on_disk\": false,\n      \"payload_m\": null\n    },\n    \"optimizer_config\": {\n      \"deleted_threshold\": 0.2,\n      \"vacuum_min_vector_number\": 1000,\n      \"default_segment_number\": 50,\n      \"max_segment_size\": null,\n      \"memmap_threshold\": null,\n      \"indexing_threshold\": 20000,\n      \"flush_interval_sec\": 5,\n      \"max_optimization_threads\": null\n    },\n    \"quantization_config\": {\n      \"binary\": {\n        \"always_ram\": true\n      }\n    }\n  },\n  \"payload_schema\": {}\n}\n``` \n\n\nThank you in advance for your help!"
  },
  {
    "threadId": "1286334206355443732",
    "name": "Error GET Collection",
    "messages": "When trying to use qdrant as a vector store for llama with the async client, I receive this error:\n`Service internal error: 0 of 0 read operations failed`\n\nI am using the latest docker image for qdrant.\n\nSteps to reproduce this:\n\n`import qdrant_client\nimport asyncio\nimport logging\nimport nest_asyncio\nfrom llama_index.core import StorageContext, VectorStoreIndex\nfrom llama_index.readers.file import CSVReader\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\nfrom dotenv import load_dotenv\nfrom pathlib import Path\n\nload_dotenv()\nnest_asyncio.apply()\nlogging.basicConfig(level=logging.INFO)\n\ncollection = \"test\"\n\nasync def index():\n    aclient = qdrant_client.AsyncQdrantClient(\n        url='http://localhost:6333'\n    )\n\n    vector_store = QdrantVectorStore(\n        aclient=aclient,\n        collection_name=collection\n    )\n\n    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n\n    documents = CSVReader().load_data(Path(\"test.txt\"))\n\n    VectorStoreIndex(\n        documents,\n        storage_context=storage_context,\n        use_async=True,\n    )\n\nasync def main(loop):\n    aclient = qdrant_client.AsyncQdrantClient(\n        url=\"http://localhost:6333\"\n    )\n    await aclient.delete_collection(collection)\n\n    loop.create_task(index())\n    loop.create_task(index())\n\n    await asyncio.sleep(5)\n\nloop = asyncio.get_event_loop()\nloop.run_until_complete(main(loop))\n`\n\nThis error is not deterministic, it happens most of the time.\nI am thinking this has to do with concurrently trying to create a collection.\n\nAny help will be appreciated !"
  },
  {
    "threadId": "1286585555907514388",
    "name": "Index Sparse Vectors",
    "messages": "Has anyone worked with elastic . I want to know if there is a way to store sparse vectors in elastic"
  },
  {
    "threadId": "1286511556586242048",
    "name": "Limits of filtering on payload data",
    "messages": "Does Qdrant have limitation on number of excluding elements inside filtering on payload?\n\nEXAMPLE:\nI need to query for all userIDs in dedicated geo location by radius but they can't be any of 500 other userID stored inside some array field.\n\nI would use MUST NOT + MATCH ANY filter.\n\nWhat is the limit of elements in that MATCH ANY field?"
  },
  {
    "threadId": "1286251113557004298",
    "name": "Combining grouping and filtering",
    "messages": "Is it possible to combine grouping and filtering in the same query? E.g., when having multiple vectors per document ID, group the results by that ID, and filter only the documents created after some given date?"
  },
  {
    "threadId": "1285415011694542859",
    "name": "Slow Ingestion Times",
    "messages": "I am inserting about 42m hybrid vector points. The insert time is very slow (about 7 days). I have set the thresholds to 0 like in the docs but its still very slow.\n\nWhat are the expectations around this and are there any other tricks to improving ingestion time?\n\nI am using llama-index to ingest the documents."
  },
  {
    "threadId": "1284407116387586082",
    "name": "Container loading points again after restarting and taking huge amount of time",
    "messages": "IтАЩm running a RAG application with qdrant docker container. From last few days IтАЩve to restart my server every day so the already created points are reloaded again and itтАЩs very slow. How to optimise this and what parameters are to be updated? My qdrant settings look like this:        \ncase \"qdrant\":\n                try:\n                    from llama_index.vector_stores.qdrant import (  # type: ignore\n                        QdrantVectorStore,\n                    )\n                    from qdrant_client import QdrantClient  # type: ignore\n                except ImportError as e:\n                    raise ImportError(\n                        \"Qdrant dependencies not found, install with `poetry install --extras vector-stores-qdrant`\"\n                    ) from e\n\n                if settings.qdrant is None:\n                    logger.info(\n                        \"Qdrant config not found. Using default settings.\"\n                        \"Trying to connect to Qdrant at localhost:6333.\"\n                    )\n                    client = QdrantClient()\n                else:\n                    client = QdrantClient(\n                        **settings.qdrant.model_dump(exclude_none=True)\n                    )\n                self.vector_store = typing.cast(\n                    BasePydanticVectorStore,\n                    QdrantVectorStore(\n                        client=client,\n                        collection_name=\"make_this_parameterizable_per_api_call\",\n                    ),  # TODO\n                )"
  },
  {
    "threadId": "1286195621891674164",
    "name": "Document token size supported by hybrid search fastembed",
    "messages": "When we run document vectorize in hybrid search, it accepts, what should be my document size for better feature extraction:\nIt accepts documents for longer text, do I need to do document splittinga? \nDocument size 400-800  tokens.\n\n```self.client.set_model(\"sentence-transformers/all-MiniLM-L6-v2\", providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])\nself.client.set_sparse_model(\"prithivida/Splade_PP_en_v1\", providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])```\n\n\n```self.client.add(\n            collection_name=self.collection_name,\n            documents=documents,\n            metadata=metadata,\n            batch_size=32,\n            ids=tqdm(range(len(documents))),\n        )\n```"
  },
  {
    "threadId": "1286121278859644949",
    "name": "Indexing UUID in Payload PROBLEM",
    "messages": "Hi, whenever I index uuid in payload, I can't see the \"filter\" button on Qdrant Cloud. But I see in other indexed fields. Is it normal or I can't index it properly?"
  },
  {
    "threadId": "1286134409530966098",
    "name": "Should I use uuid as point id or store in payload?",
    "messages": "Which one might be more efficient?"
  },
  {
    "threadId": "1283094176942002198",
    "name": "How to improve latency of search request with slighly bigger value of limit?",
    "messages": "I have a qdrant collection with 100M points in it.\n\nLatency of search request (with limit=500) = 3 * latency search request (with limit=50)\n\nIs there a way we can improve the latency for limit=500?"
  },
  {
    "threadId": "1285974755115012207",
    "name": "Migrate from the multinode to a single node",
    "messages": "Hi, I have the collection of 4.5m vectors on the setup with 3 nodes. Collection have the parameters\n\"shard_number\":1\n\"replication_factor\":3\n\"write_consistency_factor\":3\n\n1. Could I use the snapshot from current setup on the new one, but with one bigger node? \n2. As far as I understand shard number 1 says that all the data should be identical between the current instances, isn't it?  \nThank you"
  },
  {
    "threadId": "1285914760264683622",
    "name": "Appending vectors to multivector points",
    "messages": "In my database, I plan to organize a large number of \"entities\", each described by a set of vectors, with those entities getting new vectors over time (and new entities being added).\nIs it possible to append new vectors to multivector points after they are created, or is my only option to somehow retrieve the whole entity matrix, append the new vector client-side, and then overwrite the point with this expanded matrix?\nAlternatively, is there some workaround using other methods to do this? E.g., is it maybe possible to use a single-vector approach, and have each vector \"point\" to the entity it is describing, and then do some aggregation of similarities over each entity during search?\n\nFor reference, we're talking about ~1M entities, with ~10-100 vectors each."
  },
  {
    "threadId": "1285653061309825045",
    "name": "Closest point for each index value",
    "messages": "Hi, I have a use case where I'm looking to get the closest point to a particular vector for each of a few different sets of items (e.g., collection_a, collection_b, collection_c). Right now, this is small, so I have these sets in different collections. Is the best way to do this via a single collection with a batch request against a payload index? Is there a better way to do this?"
  },
  {
    "threadId": "1285110871298801694",
    "name": "Is it possible to implement TF-IDF or BM25 search in Qdrant?",
    "messages": "Hi everyone.\n\nLately IтАЩve been looking for a way to implement keyword search with Qdrant.\nSince my application supports not only English but also Japanese, I have to find a Japanese-compatible way to implement it.\n\nI found some good Qdrant articles, and they suggested me to implement it with qdrant/bm25, qdrant/bm42 or SPLADE.\nHowever, neither of qdrant/bm25 and bm42 seems to support Japanese.\nSPLADE, on the other hand, seems to support Japanese but I donтАЩt want to use it because it heavily dependent on GPU resources.\n\nSo I was wondering if there is a way to calculate BM25 or TF-IDF score manually, maybe on the application side.\n\nIтАЩm kinda new to this field so please let me know if you need any additional information. Thanks in advance.\n\nReferences:\n- Qdrant/BM25 : https://huggingface.co/Qdrant/bm25\n- Qdrant/BM42 : https://qdrant.tech/articles/bm42/\n- Keyword Search with SPLADE : https://qdrant.tech/documentation/fastembed/fastembed-splade/"
  },
  {
    "threadId": "1285219522281869394",
    "name": "How does BM25 work with the newest version.",
    "messages": "Hello, so I was reading the release of 1.10.0, and the support of BM25 with modifier=IDF in sparse vector params.\n\nI want to use BM25 hosted with another service (not fastembed), so I want to know how is the corpus calculated and where is it stored? Or if you can give me more implementation details on how it works?"
  },
  {
    "threadId": "1278037935395962931",
    "name": "using go client to create collection, how do I specify the vector name?",
    "messages": "In python, the code is something like this: https://github.com/orgs/qdrant/discussions/3213#discussioncomment-7829593\n` qdrantClient.create_collection(\n    collection_name=collection_name,\n    vectors_config= {\n        \"custom_vector\": rest.VectorParams(\n            distance=rest.Distance.COSINE,\n            size=768,\n        ),\n    }\n)`\n\nThus it is possible to specify the vector named \"custom_vector\"\n\nHowever, the example in go is something like this: https://github.com/qdrant/go-client/blob/master/examples/main.go#L58-L67\n\n`_, err = collections_client.Create(ctx, &pb.CreateCollection{\n        CollectionName: collectionName,\n        VectorsConfig: &pb.VectorsConfig{Config: &pb.VectorsConfig_Params{\n            Params: &pb.VectorParams{\n                Size:     vectorSize,\n                Distance: distance,\n            },\n        }},\n        OptimizersConfig: &pb.OptimizersConfigDiff{\n            DefaultSegmentNumber: &defaultSegmentNumber,\n        },\n})`\n\nTherefore I get the error \"Wrong Input : No existing vector name\"\nHow can I specify the vector name in the go client to get rid this error?"
  },
  {
    "threadId": "1285284520807104632",
    "name": "What is the best search method to find slighty/minimal  text differences from a collection?",
    "messages": "For example, I have two products:\n\nChocolate Garoto Meio Amargo Tablete (16X80g)\nChocolate GAROTO Meio Amargo Tablete 80g\n\nWhen I use the second product, \"Chocolate GAROTO Meio Amargo Tablete 80g,\" to find similar items, I get the first product with a score of 0.90395314. However, for other products, the scores range between 0.7 and 0.8. It seems like relying only on the score wonтАЩt get me the results I want without a lot of manual analysis. Any suggestions?"
  },
  {
    "threadId": "1285474675727667241",
    "name": "Issue with /points/search/ Endpoint Returning Duplicate Results",
    "messages": "I am experiencing issues with the `/points/search/`  endpoint. The first time I use it with the following parameters, it works as expected:\n```\n[\n  \"vector\" => \"...\", // truncated for brevity\n  \"limit\" => 500,\n  \"offset\" => 0,\n  \"filter\" => null,\n  \"with_payload\" => true,\n  \"with_vector\" => false,\n  \"score_threshold\" => 0\n]\n```\nHowever, when I make a second request with an `offset` of `500`\n```\n[\n  \"vector\" => \"...\", // truncated for brevity\n  \"limit\" => 500,\n  \"offset\" => 500,\n  \"filter\" => null,\n  \"with_payload\" => true,\n  \"with_vector\" => false,\n  \"score_threshold\" => 0\n]\n```\nI notice that it returns duplicate results from the first request (indices 0-500). Specifically, there are about 18 duplicated entries, which is approximately 3.6% of the results in the second request.\n\nCould you please help me resolve this issue?\n\nThank you for your assistance.\n\nBest regards,\nAlex"
  },
  {
    "threadId": "1285160771659108402",
    "name": "upload to shards with spark",
    "messages": "Hello Folks,\n\nWe have an issue when we upload to a collection with shards on Qdrant. The shards are getting created. I can see that on qdrant UI as well with a get request. If I try to upload the data with python, it's getting uploaded into appropriate shards.\n\nBut when we try to upload data with spark into the same collection, shard-wise. Using the shard-key selector option, it says that \"no shard key exists with that name\"\n\nA little help would be appreciated!"
  },
  {
    "threadId": "1278294549197754459",
    "name": "How data is stored in segments.?",
    "messages": "How the segment size is determined and how segments are selected for storing data.?"
  },
  {
    "threadId": "1285328533178159205",
    "name": "Improvement of Hybrid Search Results",
    "messages": "Hello Everyone,\n\nI have locally deployed a Qdrant 1.10 Docker container, where I created a few collections. Initially, I ingested dense vectors and was able to perform semantic search for retrieval. However, I wanted to improve my existing search capabilities, so I recreated the collections using both dense and sparse vectors (BM25), enabling hybrid search.\n\nAfter setting up hybrid search, the matches and retrieved chunks are significantly worse than what I was getting previously. Could the Qdrant team or anyone who has worked on search improvements provide insights on how I can enhance my search results from here?\n\nI look forward to hearing from you.\n\nRegards,\nNauyan"
  },
  {
    "threadId": "1285150400319590513",
    "name": "Is there a way to get the storage used by a collection",
    "messages": "Hey, as the title states, I have a collection `A` and `B`, and I would like to know how much storage they both use. Is there a way to do that without looking at the database directory myself?"
  },
  {
    "threadId": "1284876447608999946",
    "name": "Is there a way to prewarm memmap type collection?  I'm running on kubernetes 4 nodes 50% ram is free",
    "messages": "When I query with limit 1000 it works fast, then after limit 3000 I need to query it 5 times and then it works the 6th time I guess it taks time to bring in points to the memory? can I speed this process?"
  },
  {
    "threadId": "1284127880447655959",
    "name": "Migrate from single qdrant to cluster in k8s",
    "messages": "Hello,\nWe are just starting to work with Qdrant. Our instance is running in single mode in a Docker container, so all collections were created with the parameters shard_number=1 and replication_factor=1.\nWe want to migrate all the data to a Qdrant cluster deployed in Kubernetes. From the documentation, I understand that when restoring a snapshot on a new node, our parameters shard_number=1 and replication_factor=1 will prevent the system from replicating data across nodes. I also found in the API documentation the Replicate Shard Operation method, which allows copying a shard from one node to another, with the new shard retaining the original shard's ID and being marked as Recovery. If I use the update_collection method to increase the replication_factor, the new shard is marked as Active, but its ID still matches the original.\nCould you please advise whether replication will work in this scenario? Or is recreating the collection unavoidable?\nWe were able to transfer small collections through the functionality of snapshots and the api of the new cluster, but collections with a size of 5 GB cannot be imported into the new cluster via the api. \nCan you tell me what is the current migration method from single decker to the k8s cluster?\nThanks!"
  },
  {
    "threadId": "1285192615754596402",
    "name": "Hybrid Search Setup for later usage",
    "messages": "Hi everyone,\n\nThanks to the Qdrant team for the great work on this tool!\n\nIтАЩm looking for your insight on the best approach for the following situation:\n\nIтАЩm building a document search engine and currently only using embedding dense vectors (no sparse vectors yet). IтАЩm preparing for production but plan to add hybrid search later **without having to recreate the collections again or migrate data**.\n\nMy idea is to initially add a sparse vector field with zero values, then, once I choose the right sparse model, I would update the existing points by upserting them with the proper sparse vectors.\n\nWould do you think? Any better approach?"
  },
  {
    "threadId": "1283006573332926516",
    "name": "How are scores calculated.?",
    "messages": "I'm trying to understand hnsw indexing but i couldn't figure out how raw scores are calculated.! Will it differ for different distance metrics.?"
  },
  {
    "threadId": "1285129355885936651",
    "name": "Filtering Results Based on a Value in a Payload Array Key",
    "messages": "I am looking to filter results based on a specific value within an array inside my data payload. The payload structure is as follows:\n\npayload: \n  tags: [\n    'car',\n    'audi',\n    'highway'\n  ]\n\nMy objective is to retrieve only the points where the tags array includes the tag 'audi'.\n\nCould you please advise if it's possible to filter results based on a value within an array key in the payload? If so, I would appreciate guidance on how to achieve this.\n\nThank you for your assistance.\n\nBest regards,\nAlex"
  },
  {
    "threadId": "1284804713472196690",
    "name": "Bug - b'{\"status\":{\"error\":\"Format error in JSON body: Invalid PointInsertOperations format\"},\"time\"",
    "messages": "Old code suddenlly doesn't works.\nImage version - 1.10\nclient version - 1.11.1\n\n`from qdrant_client import QdrantClient, models\nfrom uuid import uuid4\nfrom qdrant_client import models\nimport numpy as np\n\nqd_client = QdrantClient(\"http://localhost:6333\", timeout=1000)\n\ncollection_name = \"test-collection\"\n\nqd_client.recreate_collection(\n    collection_name=f\"{collection_name}\",\n    vectors_config={\n       \"text-dense\": models.VectorParams(\n           size=1152,\n           distance=models.Distance.COSINE\n       )\n    },\n)\n\n\np = models.PointStruct(\n            id=str(uuid4()),\n            vector={\"text-dense\":np.random.randn(1152).tolist()},\n        )\n\nqd_client.upsert(collection_name=f\"{collection_name}\",points=p)`"
  },
  {
    "threadId": "1284790463832002655",
    "name": "Hybrid Search",
    "messages": "Hi, I have manually generated Dense and Sparse vectors, I Want to load them into a collection using Pyspark connector to create a hybrid search, how can I do it?"
  },
  {
    "threadId": "1284011295821004863",
    "name": "Recommended python API calls for query / search by id and by text",
    "messages": "Just going thru"
  },
  {
    "threadId": "1284015176768557066",
    "name": "fast-bge-small-en, what is the equivalent model for TextEmbedding as this is not in the list.",
    "messages": "Hi, \nI created a collection with default Fastembed using the .add() method. The collection shows the Vector Configuration as (fast-bge-small-en, 384, Cosine). My client.query(..) is able to fetch the data correctly. But I want to use client.query_points(тАж) because of additional filters like тАЬscore_thresholdтАЭ. For query_points it does not take text hence for vector I am using TextEmbedding(model_name=\"BAAI/bge-small-en\"). But I am getting the error \"Wrong input: Vector params for  are not specified in config\".\nPlease let me know what is the model used for default тАЬfast-bge-small-enтАЭ and any issue with my approach? Need help on this."
  },
  {
    "threadId": "1284157651529367632",
    "name": "return similarities from multiple collections for custom rerank function",
    "messages": "Hey! I am looking to feed outputs from qdrant into a custom reranking model built in sklearn. \n\nIs it possible to return a P x D matrix including the following using the query API?\n\nP: Number of pooled results from hybrid search (TOP 10 image results UNION TOP 10 text results, for ex)\nS: Number of distance metrics (retrieved from qdrant) to be used in the model\n\nRight now I'm pooling the ids, fetching the vecs, then computing distances manually"
  },
  {
    "threadId": "1284110199270932480",
    "name": "INVALID_ARGUMENT: Wrong input: No shards found for shard key, I am  facing this error while loading",
    "messages": "help me out"
  },
  {
    "threadId": "1284119461774884904",
    "name": "No shards found for shard key,,, I am getting this while load,",
    "messages": "How to resolve this"
  },
  {
    "threadId": "1284125551241596969",
    "name": "Multiple vectorizers",
    "messages": "I have a database that stores embeddings for text chunks. I would like to add the option to switch between different vectorizers. Is it possible to create separate indices for multiple vectorizers and, when adding a new vectorizer, create a new vector field in the database and populate it with the corresponding embeddings? Then, based on the selected vectorizer parameter, perform searches using the relevant vector field?"
  },
  {
    "threadId": "1283286786046558218",
    "name": "b'{\"status\":{\"error\":\"Wrong input: Conversion between sparse and regular vectors failed\"},\"time\":0.0",
    "messages": "I am getting this error when trying to insert the sparse vectors in collection \n\nI initially tried to store my dense and sparse vectors in the same collection but then I was getting the following error:\n\npydantic_core._pydantic_core.ValidationError: 2 validation errors for PointStruct\nvector\n  Field required [type=missing, input_value={'id': 0, 'sparse_vector'... 0.17926642546791496])}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/missing\nsparse_vector\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'sparse': SparseVector(i..., 0.17926642546791496])}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden\n\nSo I first tried to store them in different collections and then try to merge the two \n\nFollowing are the steps I took  :\n\nsparse_collection = \"rag_sparse\"\nsparse_embedding_model = SparseTextEmbedding(model_name=\"Qdrant/bm42-all-minilm-l6-v2-attentions\")\n\ndef extract_content_from_pdf(file: str) -> List:\n  \n    loader = PyPDFLoader(file)\n    docs = loader.load()\n    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n    chunks = splitter.split_documents(docs)\n    return chunks\n\ndef create_qdrant_client():\n\n    return QdrantClient(url=qdrant_url)\ndef create_sparse_collection(client: QdrantClient):\n  \n    if not client.collection_exists(collection_name=sparse_collection):\n        client.create_collection(\n            collection_name=sparse_collection,\n            vectors_config= {},\n            sparse_vectors_config={\n                \"sparse\": models.SparseVectorParams(),\n            }\n        )\n\n    logging.info(f\"Created sparse vector collection '{sparse_collection}' in Qdrant.\")"
  },
  {
    "threadId": "1284038608319152212",
    "name": "Difficulty Handling Sparse Vectors in Qdrant with Alibaba-NLP/gte-multilingual-base",
    "messages": "Hi,\n\nI'm using the Alibaba-NLP/gte-multilingual-base model, and Alibaba has provided a script to simplify working with the model's dense vectors (https://huggingface.co/Alibaba-NLP/gte-multilingual-base/blob/main/scripts/gte_embedding.py)). After extracting and inserting the dense vectors into the vector store, everything works perfectly.\n\nHowever, I'm having trouble manually extracting and inserting the model's sparse vectors. I've never worked with sparse vectors in Qdrant before, and I'm not sure how to approach this.\n\nHas anyone else worked with sparse vectors in Qdrant or faced a similar issue? Any guidance would be greatly appreciated!\n\nThanks in advance!"
  },
  {
    "threadId": "1280851113603760208",
    "name": "Qdrant Showing previous collections in Dashbaord",
    "messages": "I initially had v1.7.4, I started having issue with no of ope files so I tried to del previous container and image and install a new one. \n\nWhich i managed to do successfully, Now for sometime it showed me correct newly created collections but then it starts showing previous container collections. \n\nI checked the `storage` folder it shows the newly created collections but they are not being displayed on the dashbaord.\n\nis there some sort of caching which is pulling previous collections?"
  },
  {
    "threadId": "1250804015961341992",
    "name": "How to rename payload",
    "messages": "i want to rename the payload key names. can we do it? if yes how to?"
  },
  {
    "threadId": "1281408176796995726",
    "name": "\"Your payment connection is not setup yet.\" after subscribing to QDrant AWS Marketplace subscription",
    "messages": "Running into an issue:\n\nAWS marketplace as payment method has been completed but still seeing: \"Your payment connection is not setup yet.\""
  },
  {
    "threadId": "1281218928730837074",
    "name": "difference between search and query apis - functionality, performance",
    "messages": "Hello, we noticed that there are two kinds of apis on points - `/search` and `/query`  - when should we use which? is `/search` a subset of `/query`? Is there a difference in performance of either for doing the same thing (assuming some of the apis would do the same thing from each set)\nThanks in advance!"
  },
  {
    "threadId": "1283671994893860925",
    "name": "How is raw scoring mechanism works in qdrant and is there any guide regarding the same",
    "messages": "Can anyone share more details on raw scoring mechanism"
  },
  {
    "threadId": "1282686513145970738",
    "name": "Self Hosted Qdrant Instance TLS Issue",
    "messages": "Hi all,\n\nI am trying to add api-key and TLS support to my Qdrant instance.  I am hosting my qdrant instance using Qdrant binaries and using python to interact with my instance. Also, I am using macOS Sonoma 14.4.\n\nI want to enable TLS and api-key for security by following the guidelines here => https://qdrant.tech/documentation/guides/security/#tls . Here is my related config.json part.\n\n```\n\"service\": {\n      \"enable_cors\": true,\n      \"enable_tls\": true,\n      \"verify_https_client_certificate\": false,\n      \"api_key\": API_KEY\n},\n...\n\"tls\": {\n    \"cert\": CERT_PATH,\n    \"key\": KEY_PATH\n    \n}\n```\n\nI generated key.pem and cert.pem files using makecert and include them in related paths. Then I create QdrantClient with\n\n```\nclient = QdrantClient(url=\"https://localhost\", port=6333, api_key = api_key, https = True)\n```\n\nWhen I call a method using client\n\n```\npoint = client.retrieve(\n    collection_name=<some-collection-name>,\n    ids=[<some-point-id>],\n)\n```\n\n I get the following error\n\n```\nqdrant_client.http.exceptions.ResponseHandlingException: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\n```\n\nIs there something that I missed?"
  },
  {
    "threadId": "1283098558899290206",
    "name": "Are there any ways to track how frequently specific points were accessed in Qdrant?",
    "messages": "I want to know if points were called many times or not, or at least if Qdrant stores these statistics so I can access them. I'm working on a multi-tenancy solution and want to implement dynamic sharding (for example, using in-memory collections for hot data and on-disk storage for cold data)."
  },
  {
    "threadId": "1283095552464125983",
    "name": "{\"status\":{\"error\":\"Wrong input: Vector dimension error: expected dim: 1536, got 512\"}",
    "messages": "Hi folks,\n\nCan anyone help me solve this error \n\n{\"status\":{\"error\":\"Wrong input: Vector dimension error: expected dim: 1536, got 512\"},\"time\":0.00068408}"
  },
  {
    "threadId": "1197201727087190137",
    "name": "Batch group search",
    "messages": "Hi!\nIs it possible to combine multiple group searches in a single request?\nSomething like:\n\nclient.search_batch(\n    collection_name=db_config.collection.collection_name,\n    requests=[\n        models.SearchGroupsRequest(\n            vector=vector,\n            filter=filters,\n            with_payload=payload_fields,\n            group_by=field_name,\n            group_size=10,\n            limit=4,\n        ),\n        ...\n    ]\n)"
  },
  {
    "threadId": "1283160829654728825",
    "name": "Accessing Qdrant Monitoring Metrics v1.9.1",
    "messages": "Hi Qdrant Team, \n\nI am try to access the /metrics endpoint to visualise using prometheus/grafana, but I seem to only be able to access a few limited attributes. I have a suspicion this could be to do with my qdrant_config settings. Any advice on how to access the full list of metrics would be much appreciated. \n\nI seem to be able to access through the /telemetry endpoint, but I would prefer not to have to write a scraping function.\n\nCurrently accessible from the /metrics endpoint:\n```\n# HELP app_info information about qdrant server\n# TYPE app_info counter\napp_info{name=\"qdrant\",version=\"1.9.1\"} 1\n# HELP app_status_recovery_mode features enabled in qdrant server\n# TYPE app_status_recovery_mode counter\napp_status_recovery_mode 0\n# HELP collections_total number of collections\n# TYPE collections_total gauge\ncollections_total 2\n# HELP collections_vector_total total number of vectors in all collections\n# TYPE collections_vector_total gauge\ncollections_vector_total 52446\n# HELP cluster_enabled is cluster support enabled\n# TYPE cluster_enabled counter\ncluster_enabled 0\n```\nIdeally, I would be able to access the REST API metrics as well. \n\nMy qdrant config file is unaltered from the default settings shown here:\nhttps://qdrant.tech/documentation/guides/configuration/\n\nI can't upgrade from 1.9.1 at this stage due to it currently being used in a production setting."
  },
  {
    "threadId": "1281315051663523931",
    "name": "Problem with geo spatial queries [JS]",
    "messages": "Hello, code below doesnt return anything, even though I am sure that provided coordinates are inside desired radius.\nThe coordinates that I am expecting are equal to the coordinates in query.\n\nWhen I don't use filter, I get good, expected response.\n\n let radius = 30.0; // Initial radius\n        let res1;\n\n        while (radius <= 500) {\n            console.log(`Searching within radius: ${radius} km`);\n\n            res1 = await qdrantClient.query(userCollection, {\n                vector: targetVector,\n                limit: 30,\n                with_payload: true, // Get Firestore UID from the payload\n                filter: {\n                    must: [\n                        {\n                            key: 'loc',\n                            geo_radius: {\n                                center: {\n                                    lon: userInfo.lon, // User longitude\n                                    lat: userInfo.lat  // User latitude\n                                },\n                                radius: radius // Current search radius\n                            }\n                        }\n                    ]\n                }\n            });\n\n            // If users are found, break the loop and return results\n            if (res1.points && res1.points.length > 0) {\n                console.log(`Found ${res1.points.length} users within ${radius} km.`);\n                return res1.points;\n            }\n\n            // No users found, increase the radius and search again\n            radius *= 1.5;\n        }\n\n        // If no users are found within the maximum radius, return an empty array\n        console.log(\"No users found within 500 km.\");\n        return [];"
  },
  {
    "threadId": "1282989402737606656",
    "name": "Failing to upload specific document only to Qdrant collection via qdrant_client ApiClient",
    "messages": "When uploading specific text document only , receiving below error from qdrant \n\"<html><head><title>Request Rejected</title></head><body>The requested URL was rejected. Please consult with your administrator.<br><br>Your support ID is: 12545983678008490193<br><br><a href='javascript:history.back();'>[Go Back]</a></body></html>\""
  },
  {
    "threadId": "1283066492379992076",
    "name": "Beginner Question: Can Qdrant Help Analyze Trends in Facebook Recipe Posts?",
    "messages": "Hello! I'm new to Qdrant and vector databases in general, and I'm wondering if a vector database would fit my use case.\n\nI manage a Facebook page where I post recipes. I want to store the content of each post along with metrics like engagement rate, like count, share count, comment count, and post date.\n\nMy goal is to analyze trending patterns and identify any common traits between the posts that go viral. Would Qdrant help me achieve this?\n\nAny advice or guidance would be appreciated. Thanks!"
  },
  {
    "threadId": "1213007862385737758",
    "name": "Migrating qdrant from single Docker container to a multi-node  Kubernetes cluster",
    "messages": "Hi, I have just started testing qdrant, currently am running a single  docker container on a EC2 server, it only has one single collection (with some 2-3M vectors), (https://qdrant.tech/documentation/guides/multiple-partitions/ is used to achieve multitenacy) , Say after sometime , if i want to migrate the data from the docker container to a multi-node kubernetes cluster in Azure, Is there any migration guide for that? Or do i just have to create a snapshot and restore it ?"
  },
  {
    "threadId": "1282989862407897141",
    "name": "Rescoring, oversampling and pagination",
    "messages": "Could you clarify how pagination is handled when rescoring quantized vectors? \n\nFor example, if I'm working with binary vectors and I want `offset=200` and `limit=50` (i.e. 200-250 nearest vectors) and I query with rescoring enabled, does the rescoring happen over all 250 vectors? \n\nWhat happens if I specify `oversampling=4`? Does the rescoring run over 1000 vectors?"
  },
  {
    "threadId": "1282641791622844479",
    "name": "Bulk Upload of Vectors",
    "messages": "I have around 100,000 documents to upload, which include both text and image embeddings to about 4 collections. However, uploading them one by one is taking too much time. How can I optimize this process?"
  },
  {
    "threadId": "1282589278957735946",
    "name": "Are there any maximum number of points that each collection has?",
    "messages": "I want to apply `multi-tenancy` feature using the single collection. Are there any maximum number of points that each collection has or at least it can get affected in terms of performance?"
  },
  {
    "threadId": "1282229472946552853",
    "name": "self hosted",
    "messages": "I am trying to run Qdrant in a self hosted docker container. I want to enable api key and tls. \n\n.env :\n```bash\nQDRANT__SERVICE__ENABLE_TLS=1\nQDRANT__TLS__CERT=\"/qdrant/tls/cert.pem\"\nQDRANT__TLS__KEY=\"/qdrant/tls/key.pem\"\nQDRANT__TLS__CA_CERT=\"/qdrant/tls/cacert.pem\"\nQDRANT__SERVICE__API_KEY=\"123456789\"\nQDRANT_URL=\"https://localhost:6333\"\n```\ndocker compose.yaml :\n```yaml\nservices:\n  qdrant:\n    image: qdrant/qdrant:latest\n    volumes:\n      - ../qdrant_data:/qdrant/storage\n      - ./cert.pem:/qdrant/tls/cert.pem  # Mounting the certificate file\n      - ./key.pem:/qdrant/tls/key.pem  # Mounting the key file\n      - ./cacert.pem:/qdrant/tls/cacert.pem # Mounting the Certificate Authority Certificate\n    env_file:\n      - .env\n    restart: always\n    ports:\n      - \"6333:6333\"\n```"
  },
  {
    "threadId": "1237774085975707753",
    "name": "Qdrant API/TLS issues",
    "messages": "Hey, I am running a selfhosted qdrant server inside a docker container on my cloud server which is build from the official image and started like this: \n\n```docker run -p 6333:6333 -p 6334:6334 -v ${PWD}/qdrant_storage:/qdrant/storage:z -v ${PWD}/qdrant/qdrant_config.yaml:/qdrant/config/production.yaml -v ${PWD}/tls:/etc/tls:ro qdrant/qdrant```\n\nWith following config.yaml:\n\n```\nservice:\n  # Set an api-key.\n  # If set, all requests must include a header with the api-key.\n  # example header: `api-key: <API-KEY>`\n  #\n  # If you enable this you should also enable TLS.\n  # (Either above or via an external service like nginx.)\n  # Sending an api-key over an unencrypted channel is insecure.\n  api_key: \"612334\"\n  enable_tls: true\n\ntls:\n  cert: /etc/tls/qdrant-cert.pem\n  key: /etc/tls/qdrant-key.pem```\n\nNow I want to connect to qdrant from another docker container on the same cloud server where I have a llm application running. I'm connecting like this:\n\n```cafile = '/etc/tls/rootCA.pem'\nssl_context = ssl.create_default_context(cafile=cafile)\ntry:\n    response = urllib.request.urlopen('https://url.com:6333', context=ssl_context)\n    print(response.read())\nexcept Exception as e:\n    print(f\"Failed to connect: {e}\")\n#further code\n# Qdrant connection details\nurl = \"https://url.de\"\nport_qdrant = 6333\n\n# Initialize the Qdrant client\nqdrant_client = QdrantClient(\n    url=url,\n    port=port_qdrant,\n    api_key=qdrant_local_api_key,\n    https=True)```\nWithout TLS everything was fine. But I cannot get it running.  I was using mkcert for the creation of the certificate and key. And I even tried a suggestion from discord ```qdrantClient = QdrantClient(\n        host,\n        api_key,\n        https=True,\n        cert=(cert_file, key_file, passphrase), \n        verify=cacert_file)```\nwhich isn't working neither.\n\nDo you have an idea?"
  },
  {
    "threadId": "1281928982509522955",
    "name": "Sort search result by payload key",
    "messages": "Does qdrant provide any order by functionality on vector search results ?\n\nWe are performing paginated search and we want sorted results from qdrant based on a payload key."
  },
  {
    "threadId": "1291320949416660993",
    "name": "debug",
    "messages": "Hi All, \n\nI am using AKS to deploy my qdrant cluster using helm chart. However, I am unable to enable debug log. I have tried all different option and still see only INFO level logging is captured in pod logs. Any help is much appreciated\n\n# Top level log setting\nlogLevel: DEBUG\n\n# Explicit environment variable setting\nenv:\n  - name: QDRANT__LOG__LEVEL\n    value: DEBUG\n\n# Configuration section\nconfig:\n  log:\n    level: DEBUG"
  },
  {
    "threadId": "1280796840643530785",
    "name": "Backing up a cluster - volume inconsistencies?",
    "messages": "hello!\n\nWe are backing up our qdrant cluster by backing up each volume individually twice per day. We haven't needed to use these backups or recover. But we all know that won't last for ever. \n\nSo my concern revolves around inconsistencies across the backups. We cannot trigger the backups on all the volumes at the exact same moment, and the volumes are always being written to. So my sense is that a recovery will lead to some serious problems since the volumes will all have different states.\n\nAnyone here know if this is a problem to be worried about?\n\nHere is a comment I made on this in another thread, but the thread is likely too old to get any attention (https://discord.com/channels/907569970500743200/1149188948141297765/1279805564108406787)"
  },
  {
    "threadId": "1281671052149391407",
    "name": "Free Tier Cluster Crashed and Deleted the data upon reaching max RAM usage",
    "messages": "I am evaluating Qdrant for my startup, and I tried to create a free tier cluster and upload a subset of my pre existing points and payloads. I must have batch uploaded too much, because once it reached 1GB ram usage (1.03GB), the entire cluster crashed, disk shows 0B and the cluster isn't available. I tried restarting, but it has already been around half an hour and the cluster hasn't come up. My cluster ID is:\n\n300b3605-b79a-4b00-b011-2d955492e8c1\n\nLogs show this but it's stuck, basically forever:\n\nStarting initializing for pod 0\n2024-09-06T17:36:48.982344Z INFO storage::content_manager::toc: Loading collection: items\n2024-09-06T17:36:48.976721Z INFO storage::content_manager::consensus::persistent: Loading raft state from ./storage/raft_state.json\nAccess web UI at http://localhost:6333/dashboard\nVersion: 1.11.3, build: 9fa86106"
  },
  {
    "threadId": "1280901378595622993",
    "name": "queries per second - ef search relation.",
    "messages": "Hey guys,\n\n\nI am running multiple benchmarks for assessing Qdrant.\n\nFor the following parameters:\nNumber of vectors 10 million\nNumber of search vectors 1024 ( sequential search)\nHnsw: m=16 and ef construction= 128\nLimit = 1000\nInfo : 8 cores 64 ram\n\nWe have the following the scenarios\n1. ef search = 24\nQps = 70\n2. ef search = 48\nQps = 70\n3. ef search= 64\nQps = 2!!!!!!! \nWhere qps is queries per second in search.\n\nHow is this possible?"
  },
  {
    "threadId": "1281614383792783412",
    "name": "Searching Multiple Vector Fields",
    "messages": "hello!\n\nI would like to do a search in qdrant with multiple vectorfields, each weighted by a different factor, is there currently no such feature?\n\nFor example, let's say I have 3 vector fields in 1 collection\nVectorize an arbitrary query with 3 vector fields тАЬtitleтАЭ, тАЬdetailsтАЭ, and тАЬreviewsтАЭ respectively, and then use the\nInstead of querying against the 3 vector fields, I want to search against each field with a weighting of title-1, details-6, and reviews-3\n\nFor example, milvus has this feature and I would like to see an example if it is possible to do this in qdrant.\n\n    collection = Collection(name=collection_name)\n    search_params = {тАЬmetric_typeтАЭ: тАЬL2тАЭ, тАШparamsтАЩ: {тАЬnprobeтАЭ: 10}}\n    requests = [\n        AnnSearchRequest(data=[query_vector], anns_field=тАЬdetail1тАЭ, param=search_params, limit=limit),\n        AnnSearchRequest(data=[query_vector], anns_field=тАЬreviewтАЭ, param=search_params, limit=limit)\n    ]\n    collection.load()\n    res = collection.hybrid_search(requests, WeightedRanker(0.6, 0.4), limit=limit,\n                                   output_fields=[тАЬproduct_noтАЭ, тАЬoriginal_reviewтАЭ])\n    return format_results(res[0])"
  },
  {
    "threadId": "1281259522362576947",
    "name": "Qdrant docker crash: no reason",
    "messages": "Hey there, i have a qdrant docker running on a small machine.\nWhen trying to create a new collection it just stops and logs:\n\n2024-09-04T21:57:37.792953Z  INFO qdrant::actix: TLS disabled for REST API    \n2024-09-04T21:57:37.793727Z  INFO qdrant::tonic: Qdrant gRPC listening on 6334    \n2024-09-04T21:57:37.793871Z  INFO qdrant::tonic: TLS disabled for gRPC API    \n2024-09-04T21:57:37.794376Z  INFO qdrant::actix: Qdrant HTTP listening on 6333    \n2024-09-04T21:57:37.794696Z  INFO actix_server::builder: Starting 1 workers\n2024-09-04T21:57:37.794808Z  INFO actix_server::server: Actix runtime found; starting in Actix runtime\n./entrypoint.sh: line 25:     7 Killed                  ./qdrant $@\n\nIs there way to have a log about why stopped ? \nWhat can i do to get to know why is crashing ?"
  },
  {
    "threadId": "1280531213517586513",
    "name": "Slow qdrant response time",
    "messages": "I'm building an application using Qdrant that requires very fast response times. Currently, I'm using the Python Qdrant client to perform operations. My setup involves a threadpool of workers (range 0-100) making search requests using the Qdrant Python client.\nQdrant version : 1.11.3(latest docker image)\nQdrant-client(python): 1.11.1\n\n\nI'm performing searches as follows:\n```\nstart_time = time.perf_counter()\nresponse = client.search(\n    collection_name=f\"{collection_name}\", \n    query_vector=vector, \n    limit=n,\n    search_params=models.SearchParams(\n        quantization=models.QuantizationSearchParams(\n            ignore=False,\n            rescore=True,\n            oversampling=2.0,\n        )\n    )\n)\nend_time = time.perf_counter()\n```\nIssue\nThe main problem is the high  execution time for each query when running 1000 queries sent using n workers. \n\nFor example:  fetching 1000 results (limit=1000) with 50 concurrent workers, it takes about 300ms average per query, which is too slow for my use case.\ntest dataset size : 182724\nvector size: 1024\n\nQuestions\n1.Am I missing any optimizations in my configuration or search implementation?\n2.Are there any best practices for achieving faster response times in this scenario?\n3.Could switching to gRPC potentially improve performance?\nAny insights or suggestions would be greatly appreciated. Thank you!"
  },
  {
    "threadId": "1280461262152663091",
    "name": "hnsw - configuration",
    "messages": "Hey I am search for the default value of ef_search in qdrant client in Python but I do not find it. Do you know which one qdrant uses as default?"
  },
  {
    "threadId": "1280871855430832231",
    "name": "Different results in different environments for identical request",
    "messages": "Hello,\nI face an odd behaviour when an identical request from different machines (local and cloud) gets different results while sending request to the same Qdrant instance. I'm searching for a text which is stored in Qdrant in exact form, so I expect for exact-match.\n\nOn local machine when requests are being sent repeatedly - all responses are identical. However, when a request being sent from kubernetes app I get different responses in specific pattern.\n - First requests receives results where identical match is at the top position (similarity is around 1.01),\n- Second and Third requests receives results where identical match is absent - neither in first position, neither in later but remaining results seem to be identical. Increasing results limit does not solve this problem.\n\nThis pattern of match - no match - no match affects around 2-3 % of records for some collections, for others more.\n\nI have tried to:\n- recreate collection\n- reinitialize indexing by changing ef parameter after uploading points\n- wait for green status before testing again\n- update Qdrant version to the most recent\n- update Python Qdrant client to the most recent\n- use grpc and html port to see which solves problem.\n\nDo you have any more ideas what can be a root cause of this behaviour and maybe you will give more ideas that could solve this odd issue?"
  },
  {
    "threadId": "1154038949199745055",
    "name": "Thoughts on Disk Snapshots for Backups?",
    "messages": "I know we can follow this guide for taking snapshots and restoring the data - https://qdrant.tech/documentation/concepts/snapshots/ - but I'm curious to know what if I went ahead and snapshotted the underlying disks of a Qdrant cluster running multiple shards/peers? What would be the best practises to do this (assuming it would be faster on large clusters compared to the snapshot API)?\n\nAlso using the link above, what happens if I take a backup/snapshot of N node cluster but restore in an M node cluster?"
  },
  {
    "threadId": "1281193676273356870",
    "name": "Facets Count",
    "messages": "Hello, I am using the qdrant dev image, Can Someone guide me on how to get the facet counts for a search using python client or just a curl request.\n\nThe usecase is : I have a collection with payload. On performing a search query operation, I want to fetch details of first n(around 1000) vectors and and factes count of first m(100000) vectors."
  },
  {
    "threadId": "1281165562394710027",
    "name": "Hybrid Cloud with GKE auto_pilot not working",
    "messages": "Getting this issue that raft fails concensus on the very first startup.\n\nIs this a known issue? This is the terraform I used to create the gke cluster.\n```tf\nterraform {\n  required_providers {\n    google = {\n      source  = \"hashicorp/google\"\n      version = \"5.30.0\"\n    }\n  }\n}\n\nvariable \"cluster_name\" {\n  default = \"test-cluster\"\n}\n\nvariable \"project\" {}\n\nvariable \"region\" {\n  default = \"us-west1\"\n}\n\nvariable \"zone\" {\n  default = \"us-west1\"\n}\n\nprovider \"google\" {\n  region  = var.region\n  project = var.project\n}\n\nprovider \"google-beta\" {\n  region = var.region\n  zone   = var.zone\n  project = var.project\n}\n\nresource \"google_container_cluster\" \"cluster\" {\n  name             = var.cluster_name\n  location         = var.zone\n\n  enable_autopilot = true\n\n  vertical_pod_autoscaling {\n    enabled = true\n  }\n}\n```\n\nThe only non standard thing I did was enabled CSI driver\n```sh\ngcloud container clusters update test-cluster --update-addons=GcePersistentDiskCsiDriver=ENABLED\n```\nAs a sidenot the concensus overall seems to be a lot less stable in qdrant 10+"
  },
  {
    "threadId": "1281178979394523167",
    "name": "What determines the the leader role to shuffle between nodes?",
    "messages": "We're running in distributed mode with 11 nodes, and as part of an job I'm writing I check the leader node is as expected (matches the bootstrapNode in the helm chart). However, for the cluster I'm working with currently, the leader node seems to change all the time, for example it's have five different values in the last 30 minutes. We have other clusters that appear much more stable, and the leader node has only changed once in 12 hour. Just wondering what factors would influence the rate at which the leader node changes?"
  },
  {
    "threadId": "1280264826257539215",
    "name": "Enabling on_disk broke queries",
    "messages": "Hi, after enabling on_disk for existing collections queries do not return any results anymore. I'm using Qdrant 1.10.1 and this is the query that updated the collection:\n\n```\n   data = {\n        \"vectors\": {\n            \"\": {\n                \"on_disk\": True,\n            }\n        },\n        \"hnsw_config\": {\n            \"on_disk\": True,\n        },\n        \"on_disk_payload\": True,\n    }\n    response = requests.patch(\"/collections/collection_name\", json=data, headers={\"api-key\": \"API_KEY\"})\n```\n\nAlso setting these values to false again nor restarting qdrant made a difference. Is there any way to reindex the existing data again?"
  },
  {
    "threadId": "1280872876957896714",
    "name": "matchAny filtering condition with the python grpc client",
    "messages": "Hello everyone, is there a way to use the matchAny filtering condition with the python grpc client ?"
  },
  {
    "threadId": "1280429413334515773",
    "name": "Deleted vector still showing results during search",
    "messages": "just curious to know if anyone faced issues while deleting files from the Qdrant cloud? We are getting 200 from Qdrant but still seeing the points exist in the Qdrant cloud. We can see Cache for Ram increase and then respond with an answer even though files have been deleted."
  },
  {
    "threadId": "1291316099022852167",
    "name": "kubernetes",
    "messages": "Hi All, \n\nI am using AKS to deploy my qdrant cluster using helm chart. However, I am unable to enable debug log. I have tried all different option and still see only INFO level logging is captured in pod logs. Any help is much appreciated\n\n# Top level log setting\nlogLevel: DEBUG\n\n# Explicit environment variable setting\nenv:\n  - name: QDRANT__LOG__LEVEL\n    value: DEBUG\n\n# Configuration section\nconfig:\n  log:\n    level: DEBUG"
  },
  {
    "threadId": "1280543005840769044",
    "name": "Why does recall suddenly decrease",
    "messages": "Hey, I played with Qdrant a little bit and tried to make a small benchmark.\nI noticed a strange result while using the default config. The recall suddenly drops when querying less than 50 vectors.\nAny idea why?\n\nIs qdrant doing some kind of optimization automatically when we reach a certain number of dimensions?\n\n```rust\n                CreateCollectionBuilder::new(collection_name)\n                    .vectors_config(VectorParamsBuilder::new(\n                        dimensions as u64,\n                        D::QDRANT_DISTANCE,\n                    ))\n                    .quantization_config(ScalarQuantizationBuilder::default()),\n```"
  },
  {
    "threadId": "1280550847650070708",
    "name": "Is it possible to set a minimum distance when using similarity search for image vectors?",
    "messages": "I have a working prototype that identifies similar ring images, but the data set contains many images that are nearly identical. The similarity search is effective at finding these close matches, but I would like to use a minimum distance/score. This would allow the search to return images that are similar, but not the closest or most identical matches. Is this possible?"
  },
  {
    "threadId": "1280468823769874453",
    "name": "How to fix: \"operation was cancelled\"",
    "messages": "Hey, IтАЩm trying out qdrant. I wrote a quick script that starts a docker container with the latest version of qdrant, list the collections, creates a collection, sends a few vectors, and thatтАЩs all for now.\n\nWhen I list the connection for the first time,\n```rust\n        let collections_list = client.list_collections().await.unwrap();\n```\n\nHereтАЩs the error I get:\n```rust\ncalled `Result::unwrap()` on an `Err` value: ResponseError { status: Status { code: Unknown, message: \"h2 protocol error: http2 error\", source: Some(tonic::transport::Error(Transport, hyper::Error(Http2, Error { kind: GoAway(b\"\", FRAME_SIZE_ERROR, Library) }))) } }\n```\n\nI didnтАЩt find much info on this discord ЁЯШЦ"
  },
  {
    "threadId": "1280489379114123354",
    "name": "Evaluate text quality for RAG",
    "messages": "Hello everyone, I am searching how to mesure automatically text quality of a corpus of document ? If the threshold of quality is triggered it would mean that i could set up a RAG pipeline for my client minimizing the risk to he would be disappointed"
  },
  {
    "threadId": "1278785742595952785",
    "name": "How to work with Qdrand, now comfortable with its documentation?",
    "messages": "Hi <@893511736441843785>  <@1275833106473025567> I am little confused about my usecase to use Qdrant and not able to utilize it. For some usecase I need to store high dimension embeddings of some text (probably 4-7 words) along with some informations like id, country, name, etc for example. \n\nNow while searching if I get some text to serach, and need to find top 10 similar results with some filters on the tuple as well like id, country, name, etc. during serving side (should be very less latency)\n\nQuestions:\n1) Will storing payload along with the embeddings be better solution when asked about the latency?\n2) How will I use Qdrant just like any other database where in they have their cloud clusters like mongoDB etc. Here I am using via inmemory like this: ***client = QdrantClient(\":memory:\")***\n\nI also have more doubts but I will ask once I get answers of above ones."
  },
  {
    "threadId": "1280234131606867978",
    "name": "Filter by score?",
    "messages": "Can you filter by score? For example only return results where the score is >= x?"
  },
  {
    "threadId": "1279745603882192918",
    "name": "monthly payment",
    "messages": "hello\nI want to charge my cluster monthly.\nI don't want an hour to be deducted from my account.\nOr I want to charge the wallet to reduce the amount from the wallet.\nThank you for your guidance"
  },
  {
    "threadId": "1279886427923943544",
    "name": "Query hybrid collection using only one dense or only sparse embeddings",
    "messages": "Is it possible to query a collection created with both sparse and dense embeddings using only one of those?"
  },
  {
    "threadId": "1273203132586790923",
    "name": "A couple of problems regarding Random Sampling api",
    "messages": "Hey there, first thank you a lot for this api, it serves a lot more than just testing. \nBut for using it, I have couple of problems. First, in the docs you wrote:\n```\nfrom qdrant_client import QdrantClient, models\n\n\nsampled = client.query_points(\n    collection_name=\"{collection_name}\",\n    query=models.SampleQuery(sample=models.Sample.Random)\n)\n``` \nbut this gives error that `Random is not true, did you mean RANDOM`, (the error message is not precisly this, but you get the ideaЁЯШЕ ).\nSecond question is about async functionality, does this api provide async also? because when I used it in this manner:\n```\nvideo_list = await async_qd_client.query_points(\n        collection_name=f\"lang{lang_int}_products\",\n        query=models.SampleQuery(sample=models.Sample.RANDOM),\n        query_filter=models.Filter(must_not=constants.NEGATIVE_FILTERS),\n        with_payload=constants.VIDEO_LIST_PAYLOAD,\n        limit=per_page,\n    )\n```\nIt gives this error:\n```\nqdrant_client.http.exceptions.UnexpectedResponse: Unexpected Response: 404 (Not Found)\nRaw response content:\nb''\n````"
  },
  {
    "threadId": "1288997141347434537",
    "name": "uuid in payload schema",
    "messages": "I'm creating two collections-- one for pdfs document and one for their pages. The page-level collection contains a payload field intended to uniquely identify the parent pdf's ID in the other collection.  But when I run this code to create the payload index, the id field generates an error:\n\n```\nclient.create_collection(\n    collection_name=\"pdf_pages\",\n    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n    shard_number=1,  \n    on_disk_payload=True  \n    \n    \npage_fields_to_index = [\n    (\"content\", PayloadSchemaType.TEXT),  \n    (\"title\", PayloadSchemaType.TEXT),  \n    (\"page_number\", PayloadSchemaType.INTEGER),\n    (\"parent_pdf_id\", PayloadSchemaType.UUID), # <<< adding this line results in an error\n]\n\n# Create indexes for relevant fields\nfor field_name, field_type in page_fields_to_index:\n    client.create_payload_index(\n        collection_name=\"pdf_pages\",\n        field_name=field_name,\n        field_schema=field_type,\n    )\n```\n\nHere's the error that results:\n\n```\nUnexpectedResponse: Unexpected Response: 400 (Bad Request)\nRaw response content:\nb'{\"status\":{\"error\":\"Format error in JSON body: data did not match any variant of untagged enum PayloadFieldSchema at line 1 column 49\"},\"time\":0.0}'\n```\nCode runs without error with the parent_pdf_id line removed.\nI'm running  qdrant-client   Version: 1.11.3"
  },
  {
    "threadId": "1290007751774441482",
    "name": "Avoiding duplicates when using upsert",
    "messages": "I have been kicking around with qdrant for a few days now, and I really like it.\n\nOne thing that really bugs me though, is that the `id` has to be a number or uuid. \nI ingest data from various external providers, and none of them use numbers or uuid's, they use strings such as `1706.03762` *(arXiv)* and `W4402489297` *(OpenAlex)*.\nI can obviously generate the uuid's, but if I need to re-run a batch, those generated uuid's are not idempotent, and I can easily end up with duplicates.\n\nI would rather not maintain a mapping between uuid's and actual id's during ingestion.\n\nHas anyone solved this?"
  },
  {
    "threadId": "1290277819191922761",
    "name": "Reindexing on changing the indexes",
    "messages": "I am changing the previously defined indexes and adding more fields for indexing. I assume qDrant will automatically reindex in this scenario or there is something else that I need to take care of. Also, how will I know that reindexing of old data is completed with no issues?"
  },
  {
    "threadId": "1278023087194902611",
    "name": "Cluster on suspend? NOOOO",
    "messages": "Hello guys, today I received the email with the content:\n\n```\nThank you very much for using Qdrant!\n\nWe realized you did not use your free tier cluster recently and will put it into a suspend mode in 2 days.\n\nTo prevent this from happening, either:\n\nUse your cluster by sending any kind of API request to it, or\nUpgrade your cluster to a paid cluster.\nPaid clusters have multiple benefits, such as:\n\nHorizontal Scaling and Replication for High Availability\nSupport for automatic Snapshots and Restores\nIncluded support with response time SLAs\nNo automatic suspension or deletion\nGuaranteed resources for increased performance\nShould your cluster be suspended, your data will stay intact, and you'll have another 3 weeks to reactivate your cluster. If you do not reactivate your cluster, or upgrade to a paid cluster, it will be permanently deleted.\n```\n\nI bet, I make the API calls to it all the time, I have my clients data there but it's hard to migrate somewhere (because of the changes in the data format), it's also too small to move it to the paid one. Can you please check and prevent it from being deleted? Thanks!!"
  },
  {
    "threadId": "1279014383380467755",
    "name": "payload integer 0 is not scrolled",
    "messages": "Hi, I have the following schema on my qdrant.\n\n```\nPoint 86ce43bf-d995-4ca7-b686-1ff43006eb34\nPayload:\nchunk\nhello\ndhash\n1738147103896277805\nfilename\nhello\npage_name\nhello\npoint_id\n0\nprocessing_recipe\nchunk_size_128_overlap_32\n```\n\nHowever, when trying to scroll with payload then it doesn't find the points with point_id = 0. The qdrant client starts to find in point_id = 1\n\nwhat's wrong with here?\n\n```\ndef get_points_collection1(client: QdrantClient, collection_name: str) -> Dict[str, Record]:\n    points = {}\n    offset = None\n    while True:\n        batch, offset = client.scroll(\n            collection_name=collection_name,\n            limit=1,\n            offset=offset,\n            with_payload=True,\n            with_vectors=False,\n            order_by='point_id',\n        )\n        for point in batch:\n            point_id = point.payload.get('point_id')\n            if point_id:\n                points[point_id] = point  # Using point_id from payload as the key\n        if offset is None:\n            break\n    return points\n```"
  },
  {
    "threadId": "1278625256323682386",
    "name": "Query metadata over all collections",
    "messages": "Hi all,\nis there a built-in method to query across collections simultaneously and extract relevant metadata information (e.g. filepaths)?\n\nMy current workaround is looping over all collections like this:\n````\nfrom qdrant_client import QdrantClient\n\nqdrant_client = QdrantClient(host='localhost', port=6333)\n\ncollections = qdrant_client.get_collections().collections\n\nunique_file_paths = set()\n\nfor collection in collections:\n    collection_name = collection.name\n    # Scroll through the collection\n    scroll_result = qdrant_client.scroll(\n        collection_name=collection_name,\n        with_payload=True\n    )\n    \n    while scroll_result:\n        points, scroll_id = scroll_result\n        for point in points:\n            if 'filepath' in point.payload:\n                unique_file_paths.add(point.payload['filepath'])\n        \n        if not scroll_id:  \n            break\n        \n        # Continue scrolling with the scroll_id\n        scroll_result = qdrant_client.scroll(\n            collection_name=collection_name,\n            scroll=scroll_id\n        )\nunique_file_paths_list = list(unique_file_paths)\n```"
  },
  {
    "threadId": "1278623162548097094",
    "name": "conventions for payload key names - camelCase, UpperCamelCase, snake_case?",
    "messages": "What generally are the conventions for payload key names in the qdrant world? If it's all the same I would prefer to use `camelCase` unless there is a compelling reason to switch to something else. Thanks in advance!"
  },
  {
    "threadId": "1278454142385389679",
    "name": "Failed to open snapshot archive, malformed format",
    "messages": "I have two Qdrants, one in K8 and other in Google Cloud. I exported some snapshots from the second and send to the first, but i am facing the error Failed to open snapshot archive, malformed format. \n\nApparently  the error only happens when i try to export a collection that has few points (in my case, just one).\nThe collections with a lot of points worked just as expected."
  },
  {
    "threadId": "1277646811175194634",
    "name": "Suggested indexes are of different type",
    "messages": "QDrant Notifications in Configuration issues are suggesting I create keyword payload indexes for three fields.\n```Unindexed field 'src_oid' is slowing queries down in collection 'production'\n\nCreate an index on field 'src_oid' of schema \"keyword\" in collection 'production'. Check the documentation for more details: https://qdrant.tech/documentation/concepts/indexing/#payload-index```\nThese fields are meant to be of type UUID and I created UUID indexes for them. They detect valid counts for each of these fields. i.e \n```\"src_oid\":{\n\"data_type\":\"uuid\"\n\"points\":113005204\n}```\n\nHave I done something wrong when upserting them? \nI create index like this: \n```\nqdrant_client.create_payload_index(\n    collection_name=QDRANT_COLLECTION,\n    field_name=\"src_oid\",\n    field_schema=models.PayloadSchemaType(\"uuid\"),\n    wait=False,\n)\n```\nThe payload values are created from bson ObjectIDs to UUID with this function: \n```\n    def objectid_to_uuid(obj_id):\n        padded_bytes = obj_id.binary + b\"\\x00\" * 4\n        return UUID(bytes=padded_bytes)\n```\nand added to the payload as is (i.e as UUID not str).\n\nWhen I filter with them in searches I am using a str conversion of the UUID as the requests do not allow me to send UUIDs. Is that where I am going wrong?\n\nI have another payload index issue where it is suggesting I create an integer payload index for a field where I only do matchAny and matchValue (no range) so I created a partial index: \n```\"aff\":{\n\"data_type\":\"integer\"\n\"params\":{\n\"type\":\"integer\"\n\"lookup\":true\n\"range\":false\n\"is_principal\":\nNULL\n}\n\"points\":113005200\n``` \nI get this message however: ```Unindexed field 'aff' is slowing queries down in collection 'production'\n\nCreate an index on field 'aff' of schema \"integer\" in collection 'production'. Check the documentation for more details: https://qdrant.tech/documentation/concepts/indexing/#payload-index```"
  },
  {
    "threadId": "1278231904323239987",
    "name": "RAG result matching requires narrowing of filtering?",
    "messages": "Hello I am doing queries to Qdrant Cloud with a metafilter set to limit the date range searched.\nI am searching for notes and they have tags.\nWhat I noticed is unless I limit the tags further on top of already limiting date range, I cannot get results back that match what I am looking for.\nI am already making the chunk size around 100 chars. \n\nAny advice on how this could be improved in Qdrant settings or something. Is that just the nature of things, that I have to limit as much as possible in metafiltering?\ni.e doing this gives me nodes I want, but removing `tag` in the metafilter doesn't return me the results I want.\n```\nmust=[\n    FieldCondition(\n        key=\"practice_date\",\n        range=Range(\n            gte=variable_data['admission_date_unix'],\n            lte=variable_data['discharge_date_unix']\n        )\n    ),\n    FieldCondition(\n        key=\"tag\",\n        match=MatchAny(any=[\"food\", \"plants\"])\n    ),\n]\n```"
  },
  {
    "threadId": "1278084802800517141",
    "name": "Problem with 'Must not' filter in search method",
    "messages": "Good afternoon!\n\nI'm working on a retrieval-augmented generation (RAG) system using Qdrant, and I'm facing an issue with redundant knowledge entries in my collection. For instance, I have multiple points that have the same content, like:\n\n\"In our platform, if you are banned, it is permanent.\"\n\nThese entries have different vectors but the same payload. When I perform a top K search, I retrieve one vector, store its payload, and then iterate to find other vectors with different content. Here's my current approach:\n\n```python\nCopy code\nresult = [first_element[0]]\nseen_fish = {result[0].payload[\"fish\"][\"text_content\"]}\n\nfor _ in range(1, top_k):\n    next_element = client.search(\n        collection_name=collection,\n        query_filter=Filter(\n            must=[\n                FieldCondition(\n                    key=\"bait_text_locale\",\n                    match=MatchValue(value=bait_text_locale),\n                ),\n            ],\n            must_not=[\n                FieldCondition(\n                    key=\"fish\",\n                    match=MatchValue(value=value),\n                ) for value in seen_fish\n            ],\n        ),\n        search_params=SearchParams(hnsw_ef=512, exact=False),\n        query_vector=vector,\n        limit=1,\n    )\n    if next_element:\n        new_fish = next_element[0].payload[\"fish\"][\"text_content\"]\n        if new_fish not in seen_fish:\n            result.append(next_element[0])\n            seen_fish.add(new_fish)\n    else:\n        break\n```\nThe first vector works fine, but subsequent searches return empty results, even though the web UI shows entries with different payloads. I'm using the latest Qdrant version on Google Cloud."
  },
  {
    "threadId": "1276494022923390977",
    "name": "EC2 instance",
    "messages": "Can anyone help me to figure out, What are the recommended EC2 instance types in terms of memory and vCPUs for handling 1000 concurrent users connecting to a Qdrant database?"
  },
  {
    "threadId": "1277903042309525514",
    "name": "Recommend api with multiple named field",
    "messages": "According to documentation there is no way but I want to confirm. Do we have a way to get result with combining multiple named vector with one request on recommend endpoint? \n\nThere is a parameter which is \"using\" but accepting string. Maybe batch request could be solution but it requires merging resultset on application side."
  },
  {
    "threadId": "1277921710628409428",
    "name": "Unexpected warning for Suspend Mode",
    "messages": "Hi there! My team and I got an email this morning saying \"We realized you did not use your free tier cluster recently and will put it into a suspend mode in 2 days\". We've been actively using both our paid and free tier clusters in the last couple of days so we weren't sure if there was something else we needed to do to signify activity. Let us know, thank you!"
  },
  {
    "threadId": "1277964765418749983",
    "name": "Self-hosted Dify on Railway",
    "messages": "I'm facing issue of knowledge embedding to Qdrant (Both cloud and self-hosted) from self-hosted Dify deployed on Railway.\n\nHere is the error message shown on Dify. \n----\n<_InactiveRpcError of RPC that terminated with: status = StatusCode.PERMISSION_DENIED details = \"Received http2 header with status: 403\" debug_error_string = \"UNKNOWN:Error received from peer {created_time:\"2024-08-27T11:57:37.309339172+00:00\", grpc_status:7, grpc_message:\"Received http2 header with status: 403\"}\" >\n\nThis is current env config related to Qdrant on Railway\n----\nQDRANT_API_KEY=<Qdrant API Key>\nQDRANT_URL=<Qdrant endpoint URL>\nQDRANT_CLIENT_TIMEOUT=\"20\"\nQDRANT_GRPC_ENABLED=\"true\"\nQDRANT_GRPC_PORT=\"6334\"\n\nDoes anyone has succeeded on similar on Railway?"
  },
  {
    "threadId": "1277938478780518451",
    "name": "Adding element to list in Payload",
    "messages": "Hi,\n\nI'm looking to add a new element to a list within the payload of a specific point in my collection, without needing to fetch the entire list first and then re-upload it.\n\nFor example, I currently send the following PUT request to update the points:\n\n`\nPUT /collections/tests/points\n{\n    \"points\": [\n        {\n            \"id\": 1,\n            \"payload\": {\"data\": [\"a\", \"b\"]},\n            \"vector\": [0.9, 0.1, 0.1]\n        }\n    ]\n}`\nThe expected result would be:\n\n`\n{\n    \"id\": 1,\n    \"payload\": {\"data\": [\"a\", \"b\", \"c\"]},\n    \"vector\": [0.9, 0.1, 0.1]\n}`\nI want to append a new element to the payload field of a specific point, but I don't want to fetch the entire list first, modify it, and then upload it again one by one. This process seems inefficient, especially when dealing with a large number of points.\n\nIs there an alternative method to directly append an element to the payload of a point, ensuring that the lists do not have identical elements?"
  },
  {
    "threadId": "1277735658311520277",
    "name": "Multitenancy with LLamaindex and Qdrant",
    "messages": "Hello everyone,\n\nI'm currently working on a plugin for a startup, which essentially functions as an advanced Retrieval-Augmented Generation (RAG) system, with a strong focus on the search aspect.\n\nI'm exploring the idea of implementing multitenancy in this system using LLamaIndex with Qdrant. Has anyone here had experience or insights on using Qdrant with LLamaIndex in a multitenant setup? Specifically, I'm interested in best practices, potential pitfalls, and any performance considerations that might come into play when managing multiple tenants.\n\nAny advice or pointers to resources would be greatly appreciated!\n\nThanks in advance for your help!"
  },
  {
    "threadId": "1277725562181390517",
    "name": "full text index not exact",
    "messages": "Hey!\n\nWe were using keyword index before with large text blocks, and without the full text index, the `MatchText` would fall back to a keyword substring match. It worked well for our particular use-case where an exact match is important.\n\nNow, we moved to testing out the full text index in RAM, and it is fast and working quite well, except it no longer is exact. For example, when we pass `MatchText(text=\"Google Tag Manager\")`, we get documents that have all 3 words, but not all 3 words in exact order. \n\nIs this by design? Are there any parameters we can tweak for forcing docs that have the full string exactly as requested in the text?"
  },
  {
    "threadId": "1277452167497650227",
    "name": "Upload points timed out",
    "messages": "Hi, currently I'm testing to upload multi-vectors into my qdrant collection, i have a total of 80000 instances that need to be upload\n\nHere is the code:\n```\nself.async_client = AsyncQdrantClient(\n            host=self.host, port=self.port, timeout=600\n        )\n...\nasync def upsert_batch(\n        self,\n        collection_name: str,\n        ids: List,\n        embeddings: Dict,\n        metadatas: Optional[Iterable[Dict[Any, Any]]],\n        batch_size: int = 64,\n        num_parallel_processes: int = 1,\n        wait: bool = True,\n    ):\n        all_vectors = []\n        for (dense, token,metadata, uid) in zip(embeddings['dense-vector'], embeddings['output-token-embeddings'],metadatas,ids):\n            all_vectors.append(\n                models.PointStruct(\n                    id=uid,\n                    payload=metadata,\n                    vector={\n                        \"dense-vector\": dense,\n                        \"output-token-embeddings\":token.cpu().numpy().tolist()\n                    },\n                )\n            )\n        self.async_client.upload_points(\n            collection_name=collection_name,\n            points=all_vectors,\n            batch_size = batch_size,\n            parallel = num_parallel_processes,\n            wait = wait,\n        )\n```\n\nAt around 70000, it always output `qdrant_client.http.exceptions.ResponseHandlingException: timed out `. All the configurations are shown as above (batch_size = 64, wait = True..etc)\n\nI have latest qdrant-client `v1.11.0` and docker image\n\nCould anyone provide some insight on why this occur?"
  },
  {
    "threadId": "1277495045590880377",
    "name": "Cant send message over raft channel",
    "messages": "Qdrant version 1.10. \n\n2/4 qdrant nodes got cycled 1 after another within kubernetes during an upgrade. After a few shards were marked as dead. The collection is green, and is able to do search requests. Upsert and Delete are both not working with error message\n\n`2024-08-26T05:08:58.546349Z ERROR qdrant::tonic::logging: gRPC /qdrant.Raft/Send unexpectedly failed with Internal error \"Can't send Raft message over channel\" 0.000069`"
  },
  {
    "threadId": "1277487523601317888",
    "name": "Regarding path in Qdrant client in python",
    "messages": "Hi I am am trying to save some data with qdrant for a simple application that will be used by less than 10 people. The data volume is around 200mb and is not any sensitive data as well.\nI create an instance of qrant client like this.\n# Initialize Qdrant client and collection\nclient = QdrantClient(path=\"./qdrant_data\")\n\nI can see a folder named '/qdrant_data' in my projects folder and is able to perform similarity search as well.\nDoes this mean I dont need to install a separate server of Qdrant in docker ?\ncan I use this folder as the database and ship this app to the users?"
  },
  {
    "threadId": "1276522981790715986",
    "name": "Is there a way to pass a custom scoring function to qdrant search endpoint.",
    "messages": "We want to pass a custom sorting function to qdrant so as to take other factors into account in addition to the cosine similarity score like the recency of the point record and the authenticity of the record.\nSo, score function should be something like: Score = A* similarity_score + B* recency of record + C* authenticity_source_of_record.                                         \n(Similar to function_score or script in Elasticsearch)"
  },
  {
    "threadId": "1286241772615499798",
    "name": "Set/remove payload with dot",
    "messages": "Hey I try to set and removed nested payload but I run into troubles.\nNow after doing mess I can't delete a paylaod called meta.meta.group_id.\n\n```python\nfor key, value in my_dict.items():\n    resp2 = client.delete_payload(\n        collection_name=COLLECTION_NAME,\n        keys=[\"meta.meta.group_id\"],\n        points=qdrant_client.models.Filter(\n            must=[\n                qdrant_client.models.FieldCondition(\n                    key=\"meta.meta.group_id\",\n                    match=qdrant_client.models.MatchValue(value=value),\n                ),\n            ],\n        ),\n    wait=True,\n    )\n    print(resp2)\n```\nIt doesn't want to get deleted"
  },
  {
    "threadId": "1276950597362516008",
    "name": "hybrid search",
    "messages": "how can i implement hybrid search using qdrant ?"
  },
  {
    "threadId": "1276839912083161130",
    "name": "facebook/dinov2-base or Qdrant/Unicom-ViT-B-16 for Image Recommendation",
    "messages": "Hi everyone, I am building recommendation system based on images. I was using dinov2-base model but today I discovered Qdrant/Unicom-ViT-B-16\nIf you ever had chance to compare these models? Which one is better in your opinion?"
  },
  {
    "threadId": "1273629789143568496",
    "name": "How do I properly run the qdrant test suite after a build",
    "messages": "Hi all,  I want to fiddle around with the qdrant source and have checked out the qdrant dev branch, after which I've built qdrant from source using the Dockerfile. According to the docs/CONTRIBUTING.md i should now make sure the test suite passes. Is there one entrypoint for running all the relevant tests? I understand that there is some shell scripts and some poetry projects under tests/ that contain various tests but I can't seem to find an entrypoint to run the tests."
  },
  {
    "threadId": "1276257454455525438",
    "name": "v1.11.0 causing Service internal error: Tonic status error: status: Unknown",
    "messages": "When upgrading to v1.11.0 qdrant there're errors in k8s log: `Service internal error: Tonic status error: status: Unknown, message: \"h2 protocol error: http2 error: connection error detected: frame with invalid size\", details: [], metadata: MetadataMap { headers: {} }` which seems to be related to the update of the tonic lib version from 0.9.2 to 0.11.0  as part of qdrant v1.11.0. And seems to be related to this issue https://github.com/hyperium/tonic/issues/1496#issuecomment-1708191438 \nAs result collection creation is failing with `Service internal error: Failed to propose operation: leader is not established within 10 secs`\nI would appreciate any guidance on the potential solution"
  },
  {
    "threadId": "1276541759366758602",
    "name": "cannot pickle sqlite3.connection",
    "messages": "Trying to use the db through persist storage on disk and then accessing it, following is the code i use\nif client.collection_exists(collection_name = f'{collection_name}'):\n      # client.delete_collection(collection_name = collection_name)\n      vector_store = QdrantVectorStore.from_documents(\n          client = client,\n          documents = documents,\n          embedding=embeddings,\n          sparse_embedding=sparse_embeddings,\n          location=\"/content/drive/MyDrive\",\n          collection_name=collection_name,\n          retrieval_mode=RetrievalMode.HYBRID,\n          vector_name = \"BAAI/bge-base-en-v1.5\",\n          sparse_vector_name = \"bm25\"\n      )\nelse:\n      client.create_collection(\n          collection_name = collection_name,\n          vectors_config={\n            \"BAAI/bge-base-en-v1.5\": VectorParams(size=embedding_size, distance=Distance.COSINE)\n          },\n          sparse_vectors_config={\n              \"bm25\": models.SparseVectorParams(\n                  modifier=models.Modifier.IDF,\n              )\n          }\n      )\n\n      # Create QdrantVectorStore instance\n      vector_store = QdrantVectorStore(\n          client=client,\n          collection_name=collection_name,\n          embedding=embeddings,  # Use our custom embedding function\n          sparse_embedding=sparse_embeddings,\n          retrieval_mode=RetrievalMode.HYBRID,\n          vector_name = \"BAAI/bge-base-en-v1.5\",\n          sparse_vector_name = \"bm25\"\n      )\n\n      # Add documents to the vector store\n      vector_store.add_documents(documents=documents)\n\ni have already used from_existing_collection"
  },
  {
    "threadId": "1276429993949528199",
    "name": "Unexpected search results",
    "messages": "Hello, I'm trying to implement my first RAG Chatbot but I've got stuck on retrieval. I added points to Qdrant. I created embeddings with nomic-embed-text (ollama). Then I also created an embedding for a user query using the same model. When I check cosine similarity between those two embeddings I get completely different results (expected) from qdrant scores (unexpected). I saw documentation that cosine similarity is implemented as a dot product but I still don't understand why there are so huge differences . Could you help me to understand how to use qdrant so that I get expected points returned from search?\n\nExample:\n```\n[Document] ICD-O coding: 9870/3 Acute basophilic leukaemia\n[Query] What disease could be observed by features of bone marrow failure? [Cosine] 0.5440049728967676\n[Qdrant Result] Patients usually present with features of bone marrow failure [Cosine] 0.5627227418187919 [Score (much higher than cosine)] 0.9036882\n\n[Document] None\n[Query] cell cycle checkpoints after DNA damage [Cosine] 0.3020438283641733\n[Qdrant Result] Clonal haematopoiesis of indeterminate potential (CHIP) [Cosine] 0.2831620883133988 [Score (much higher than cosine)] 0.6108711\n```"
  },
  {
    "threadId": "1276101671868567633",
    "name": "Add payload index",
    "messages": "is it possible to create a Payload Index on an already populated collection ? \nif not what is the correct procedure to add a new payload index to a collection ? (recreate => scroll/upsert + alias ?)"
  },
  {
    "threadId": "1276020930937622559",
    "name": "How to rebalance shards",
    "messages": "I have a Qdrant cluster with 4 nodes and 1 shard per node. 1 node is using significantly more RAM than the other. Is there a way to rebalance the shards?"
  },
  {
    "threadId": "1276182798247530538",
    "name": "Upsert vectors by payload",
    "messages": "I'm using an upsert API for vector operations. When I submit vectors with the same ID (point ID or index), the API updates the existing vector.\n\nInstead of using the ID to determine whether to update or insert, I want the API to check for duplicates based on certain fields in the payload. If these specific fields match an existing vector, I want the API to update that vector rather than creating a new one.\n\nIs it possible to implement this kind of conditional update based on payload fields rather than just the ID?"
  },
  {
    "threadId": "1273683413781577779",
    "name": "Stopping and restarting a distributed deployment",
    "messages": "I was playing with a 3 node distributed qdrant deployment using the latest helm charts (1.11), and I ran into this error on only one of the nodes. The cluster has no collections, it is completely fresh, I just started it, stopped it, and restarted it, and ran into this error.\n\nAny ideas? I assume I can fix this by simply deleting the volumes, but what are the best practices around restarting a distributed deployment in k8s?"
  },
  {
    "threadId": "1276097143119024200",
    "name": "Optimizing on disk index",
    "messages": "Hello! \n\nI am testing the new on disk indexing in v1.11, but seeing that it doesnt pair well with the search API. \n\nThe collection has around 60M vectors, the vector indices are on disk. I've added a keyword index on one of the payload fields, and that is indexed on disk as well (with the new functionality). The observations follow:\n\n- Running search API without a filter - we see a response within seconds\n- Running scroll API with filter - we see a response within seconds\n- Running search API with a filter - it times out, even giving it 3 minutes to respond\n\nAny idea why this would be the case?"
  },
  {
    "threadId": "1276070800583294986",
    "name": "Restoring a collection snapshot in another machine.",
    "messages": "Hi team,\nI created a snashot for one of the collections and transfered the snapshot folder to another machine where I wanted to start Qdrant.\nThe snapshpt folder on the new machine has various directories (with names of the collection existing on the orginal machine) however only one of the collection(Test India) has the .snashot file ('Test India-3963224971621705-2024-08-22-05-59-31.snapshot'). \n\nI am trying to recover this with the command: \ndocker run -v ~/qdrant_storage:/qdrant/storage -p 6333:6333 -v ~/qdrant_snap:/qdrant/snapshots qdrant/qdrant ./qdrant --storage-snapshot /qdrant/snapshots/Test\\ India/Test\\ India-3963224971621705-2024-08-22-05-59-31.snapshot\n\nBut getting error: 2024-08-22T06:43:06.666181Z ERROR qdrant::startup: Panic occurred in file src/snapshots.rs at line 104: called `Result::unwrap()` on an `Err` value: Error(\"missing field `collections_mapping`\", line: 1, column: 708) \n\nCould you please help how to recover a particular snapshot from a particular collection?"
  },
  {
    "threadId": "1275944806258053122",
    "name": "how do I index documents with LateInteractionTextEmbedding(\"colbert-ir/colbertv2.0\")?",
    "messages": "I'm trying to do a basic test of the colbert model, but there isn't a complete example I can find.\nGetting the embedding for documents returns a Nx128 multidimensional vector and as I'm brand new to qdrant I'm not sure how to insert this into the DB."
  },
  {
    "threadId": "1199819783772848189",
    "name": "distance_feature",
    "messages": "Does qdrant support a similar function as https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-distance-feature-query.html that can boost doc relevance score based on e.g. recency?"
  },
  {
    "threadId": "1274042603347771463",
    "name": "Docker secrets with Qdrant",
    "messages": "Hi everyone,\nit seems that qdrant does not unpack docker compose secrets but only stores the secrets `/run/secrets/secret-name` and not the actual content.\n\nI want to store my qdrant api key for simple client authentication (see here: https://qdrant.tech/documentation/guides/security/#) in a docker compose secret. Maybe this is wrong in the first place, so please feel free to also recommend a better approach. When I do this (via setting it as an environment variable), however, I cannot actually access the service with the key I set but only with the path that docker compose uses.\n\nI follow the qdrant guide and Docker compose guide on secrets\n - https://qdrant.tech/documentation/guides/security/#\n - https://docs.docker.com/compose/use-secrets/\n\nI share my code below:\nmy `docker-compose.yml`\n```\nservices:\n  # From https://qdrant.tech/documentation/guides/installation/#docker-compose\n  qdrant:\n    image: qdrant/qdrant:latest\n    restart: always\n    container_name: qdrant\n    ports:\n      - 6333:6333\n      - 6334:6334\n    expose:\n      - 6333\n      - 6334\n      - 6335\n    configs:\n      - source: qdrant_config\n        target: /qdrant/config/production.yaml\n    volumes:\n      - ./qdrant_data:/qdrant/storage\n    environment:\n      QDRANT__SERVICE__API_KEY: \"/run/secrets/qdrant_api_key\"\n    secrets:\n      - qdrant_api_key\n\nconfigs:\n  qdrant_config:\n    content: |\n      log_level: INFO\n\nsecrets:\n  qdrant_api_key:\n    file: qdrant_api_key.txt\n```\n\nWhere `./qdrant_api_key.txt` is\n```\n12345\n```\n\nI would expect to be able to reach my service via:\n`curl   -X GET http://localhost:6333/collections   --header 'api-key: 12345'`\n\nHowever, instead I only reach my service via:\n`curl   -X GET http://localhost:6333/collections   --header 'api-key: /run/secrets/qdrant_api_key'`\n\nMany thanks for your help!!"
  },
  {
    "threadId": "1275716951720726528",
    "name": "when downgrading from v1.11.0 to v1.10.0, I got this:",
    "messages": "2024-08-21T07:22:46.324087Z ERROR qdrant::startup: Panic occurred in file /qdrant/lib/collection/src/shards/replica_set/mod.rs at line 275: Failed to load local shard \"./storage/collections/jaksim_1_document_metadata/0\": Service internal error: RocksDB get_pinned_cf error: Not implemented: Unsupported compression method for this build: LZ4"
  },
  {
    "threadId": "1275720085981499453",
    "name": "Is it possible to order by ID in scroll point?",
    "messages": "Hi, I want to scroll points order by ID - which is to find the maximum point id by Filtering Option.\n\n```\nself.client.scroll(\n            collection_name=collection_name,\n            scroll_filter=Filter(\n                must=[\n                    FieldCondition(key=\"dhash\", match=MatchValue(value=dhash))\n                ]\n            ),\n            with_payload=True,\n            with_vectors=False,\n            order_by=OrderBy(key=\"id\", direction='desc'),\n        )\n```\n\nbut the error message seems not supporting the usage of id. anyone knows about it?\n```\nb'{\"status\":{\"error\":\"Wrong input: No range index for `order_by` key: `id`. Please create one to use `order_by`. Check https://qdrant.tech/documentation/concepts/indexing/#payload-index to see which  ...'.\n```"
  },
  {
    "threadId": "1282636406505603154",
    "name": "Changing Embedding model in prod without downtime",
    "messages": "I have a collection with vectors from model-v1. Now I got a better version of the model, say model-v2. Both the models have same embedding dimensions. Now I want the vectors to be created from model-v2. How do I do this without downtime? In elasticsearch (talking only about text search), I normally create a new index and use alias to atomically switch. But there, my inference doesn't depend on any models so that atomic switch works. In case of qdrant, when I do the atomic switch to the new collection with model-v2 embeddings, my inference also need to be updated to use model-v2. How do I solve this? Or is there no way for this to happen without downtime / temporary inconsistent state?"
  },
  {
    "threadId": "1182591138524311614",
    "name": "Delete Quantization config for collection",
    "messages": "Hello team,\nI just create a scalar quantization for my collection like this, and watch how many RAMS  used.\n```\nPATCH /collections/t_feed_image_hot\n{\n\n    \"quantization_config\": {\n        \"scalar\": {\n            \"type\": \"int8\",\n            \"quantile\": 0.99,\n            \"always_ram\": false\n        }\n    }\n}\n```\nAnd Then I want to remove Quantization for releasing RAM, I try to use this request to do it.\n```\nPATCH /collections/t_feed_image_hot\n{\n\n    \"quantization_config\": null\n}\n```\nIt returns OK, but quantization of collection still remains, I get this by GET my collection info.\n\nCould u help me?"
  },
  {
    "threadId": "1273366781574123616",
    "name": "Only dense search working on hybrid collection",
    "messages": "I have a retriever set up like so: \n`qdrant = QdrantVectorStore.from_existing_collection(\n        \n        embedding=self.embedding,\n        sparse_embedding=sparse_embeddings,\n        sparse_vector_name=\"bm42\",\n        collection_name=\"bc-bm42\",\n        url=self.url,\n        vector_name=\"fast-jina-embeddings-v2-small-en\",\n        retrieval_mode=RetrievalMode.DENSE)`\nand bc-bm42 is a hybrid Qdrant collection. When retrieval mode is set to dense, source docs are retrieved, but none are retrieved if I use the HYBRID retrieval mode. Has anyone run into this before?"
  },
  {
    "threadId": "1250089958140543047",
    "name": "Setting score_threshold within a range",
    "messages": "How can I set the scrore threshold to be within a specific range. For example I want to get all points where the score is between 0.4 and 0.8"
  },
  {
    "threadId": "1272516323124973619",
    "name": "apikey with helm chart without hardcoding into values.yaml",
    "messages": "Hi All,\n\nIm developing a system that will use the qdrant helm chart as a helm dependancy. In the chart, there is a parameter for requiring an API key. I want to be able to be able to use a preset API key but id rather not hard code it into my values.yaml so i cannot accidentally commit it. I have tried manually creating the secret the chart would expect but it expects it to have the labels and annotations a helm created secret would and id really rather not have to faf with that every time it changes.\n\non a side point, the redis helm chart handles this really well, they have the following config (among other ways of defining a key)\n\n```yaml\n  auth:\n    existingSecret: <secret name>\n    existingSecretPasswordKey: <key in secret>\n```\njust a suggestion for the next iteration if there is currently no other way to approach this. ЁЯЩВ"
  },
  {
    "threadId": "1273233525365276756",
    "name": "Indexing payload or using MongoDB",
    "messages": "Dear all,\n\nI have a question regarding indexing. Currently, we don't use and index and qdrant is obviously relatively slow; we're fully aware of this issue.\nwe're thinking about migrating the payload data to mongodb, since some of the queries are based on the the payload (similar to select * from publications where label = sdg3)\ndo you think this would help or is it already sufficient to use an index? we dont need to have super fast performance but it should be faster. Im asking this because we're not sure if we're using qdrant as it is intended. The main reason why I post this question is that we are not sure if\n\n- We're using qdrant in an way as it it not designed for (putting to much into payload) and therefore migrate to MongoDB (only keeping the vectors in qdrant and basically querying two times, 1x qdrant for vectors and then 1x Mongodb or vice versa)\nOR\n- We did not fully use the potential of qdrant (missing indices)\n\n\nour 2 main queries are (example for sdg 4)\n\n1.a ) Payload based: Give me all documents that have the key \"4\" in predict_goals (this is basically already an abstraction since we created this payload based on the vector \"goal\"; we might change this if we have a working index on the vectors)\n1.b ) Vector based: Give me all documents that have a value of >= 0.95 in the n-th (4th) vector in the \"goal\" vector.\n~~1~~2. ) Payload based: Give me all documents that have the key \"sdg4\" in the \"labels\" key in payload.\n\n\n\nThank you very much for your opinion.\n\n\nAdditional Information:\n\n- Using Qdrant in a docker container: \"version\":\"1.10.1\",\"commit\":\"4aac02315bb3ca461a29484094cf6d19025fce99\"\n- Using Qdrant client for python"
  },
  {
    "threadId": "1274298580383039508",
    "name": "Best way to search for all the vectors having a specific value in payload",
    "messages": "Here is the use case:\n\n1. I have file name and summary. I have stored that in 2 different collections. Both of them has a payload called doc_id which refers to document db _id value.\n\n2. I have a set of doc_ids, we can say lets 10k. I want to filter all the records, if the doc_id in payload is a member of the the doc_id sets.\n\nfor eg:\n\ndoc_ids for filteration is [1,2,3,4,5]\n\nfile_name collection in qdrant_db:\n\n{_id: <uuid>, key: file_name, vector: <vector>, payload: {doc_id: 4}}\n\n\nAfter search I should get the above record in response.\n\n\nThanks!\n\n<#1149327864936808529>"
  },
  {
    "threadId": "1275077322659201074",
    "name": "Leader is not established within 10 secs",
    "messages": "Hi Guys, I had scaled the pods for my qdrant cluster to 4 to test out , and then scaled back to 1, now for any operation like restoring snapshot or removing cluster peer its throwing the error: \"Service internal error: Failed to propose operation: leader is not established within 10 secs\", how to go about solving this issue?(I am using helm chart for deployment)\nI tried changing the raft_state.json also, but its continuously being replaced."
  },
  {
    "threadId": "1275070734296682496",
    "name": "HNSW index on disk",
    "messages": "Is there any way to change the settings of created collections  to set HNSW index on disk true ?"
  },
  {
    "threadId": "1275056494261305416",
    "name": "`Query` vs `Search` APIs",
    "messages": "What is the difference between \"Nearest Neighbors Search\" in [Query API](https://qdrant.tech/documentation/concepts/search/#query-api) and search by vector in [Search API](https://qdrant.tech/documentation/concepts/search/#search-api) ?"
  },
  {
    "threadId": "1265681864279068683",
    "name": "Recover snapshot from s3",
    "messages": "Hey folks, I'm looking to set up a data recovery pipeline for our qdrant clusters. We can take collection snapshots in s3, but it looks like the APIs to recover data from snapshots expect either a file to upload (on the /snapshots/upload API), a local file or a url (on the snapshots/recover API). It doesn't look like it accepts an s3 path to recover the snapshot from. Am I missing something?"
  },
  {
    "threadId": "1275008960864456744",
    "name": "search in",
    "messages": "[2024-08-19 11:32:05] url: 'https://1aaaaaaa.us-east4-0.gcp.cloud.qdrant.io:6333/collections/allLarge/points/search',\n[2024-08-19 11:32:05] status: 400,\n[2024-08-19 11:32:05] statusText: 'Bad Request',\n[2024-08-19 11:32:05] data: {\n[2024-08-19 11:32:05] status: {\n[2024-08-19 11:32:05] error: 'Wrong input: Vector dimension error: expected dim: 3072, got 1536'\n[2024-08-19 11:32:05] },\n\nhello i want to use large openai embeding but after uploading in asRetriver i get above error!\nwhy? qdrant dont support large embeding for search query?"
  },
  {
    "threadId": "1275009909855096946",
    "name": "Search multiple collections in one request",
    "messages": "I'm using LangChain and want to search multiple collections simultaneously for a query.\nHow to do this?\n\n  const vectorStore = new QdrantVectorStore(new OpenAIEmbeddings(), {\n    url: config.uri,\n    apiKey: config.token,\n    collectionName: \"all\",\n  });"
  },
  {
    "threadId": "1274310671001190443",
    "name": "\"error\":\"optimizations pending, awaiting update operation\". How can I optimize my storage?",
    "messages": "Hi, team!\n**Environment**\nQdrant version: v1.11.0\nDeployment: Docker (self-hosted)\nHardware specifications for QDrant:  4 Core, 20G RAM\n\n**Issue Description**\nI am encountering high RAM usage when filling data in Qdrant. Here are the details of my collection (optimization config is default):\n```\nstatus: green \noptimizer_status: { \"error\":\"optimizations pending, awaiting update operation\" }\nindexed_vectors_count: 165722\npoints_count: 75396\nsegments_count: 25\nconfig: {\n  \"params\": {\n    \"vectors\": {\n      \"all-MiniLM-L6-v2\": {\n        \"size\": 384,\n        \"distance\": \"Cosine\"\n      },\n      \"colbertv2.0\": {\n        \"size\": 128,\n        \"distance\": \"Cosine\",\n        \"multivector_config\": {\n          \"comparator\": \"max_sim\"\n        }\n      }\n    },\n    \"shard_number\": 1,\n    \"replication_factor\": 1,\n    \"write_consistency_factor\": 1,\n    \"on_disk_payload\": true,\n    \"sparse_vectors\": {\n      \"bm25\": {\n        \"modifier\": \"idf\"\n      }\n    }\n  },\n  \"hnsw_config\": {\n    \"m\": 16,\n    \"ef_construct\": 100,\n    \"full_scan_threshold\": 10000,\n    \"max_indexing_threads\": 0,\n    \"on_disk\": false\n  },\n  \"optimizer_config\": {\n    \"deleted_threshold\": 0.2,\n    \"vacuum_min_vector_number\": 1000,\n    \"default_segment_number\": 0,\n    \"max_segment_size\": null,\n    \"memmap_threshold\": null,\n    \"indexing_threshold\": 20000,\n    \"flush_interval_sec\": 5,\n    \"max_optimization_threads\": null\n  },\n  \"wal_config\": {\n    \"wal_capacity_mb\": 32,\n    \"wal_segments_ahead\": 0\n  },\n  \"quantization_config\": null\n}\npayload_schema: {}\n```\nMy code  to insert into a vector database is https://gist.github.com/kumancev/66ef7e32292003187307d38089a5b625, this code was written following this example of hybrid search implementation https://github.com/qdrant/workshop-ultimate-hybrid-search/tree/main"
  },
  {
    "threadId": "1274465388305846343",
    "name": "Creating a collection from collection with init_from",
    "messages": "Hello,\n\nI was looking to test some alternative configurations for a distributed collection. When I use the `\"init_from\":` with the collection creation endpoint, I end up with this message:\n\n```\n{\n  \"error\": \"Service internal error: Waiting for consensus operation commit failed. Timeout set at: 10 seconds\"\n}\n```\n\nAre there any additional points to consider when creating a collection from another collection in a distributed qdrant environment?"
  },
  {
    "threadId": "1274308365601734687",
    "name": "Adding documents take too long",
    "messages": "Im doing the process of adding document to qdrant and sometimes it crashes.\nCould it be possible that it takes too long and crashes when adding more than 175 documents (sometimes)\ntry:\n                vector_store.add_documents(\n                    chunks,\n                    ids=list_ids,\n                )\n                client.set_payload(\n                    collection_name=folder_id,\n                    payload={\"name_file\": file_name},\n                    points=list_ids,\n                )\n            except Exception as e:\n                logging.error(\"Too long loading to qdrant \", str(e), exc_info=True)"
  },
  {
    "threadId": "1247478448734343301",
    "name": "Filter best practice for List of keys in Payload",
    "messages": "The data payload in my qdrant vector database appears as follows: \n```\n{\"color\": \"red\", \"occasion\": [\"casual\", \"formal\"]}\n```\n How can I filter the query to retrieve items where the color is **blue** or **yellow** and the occasion includes **formal** or **preppy**?\n\nIs the following code snippet the correct way to accomplish this?\n```\nfilter = models.Filter(\n    must=[\n        models.FieldCondition(\n            key=\"color\",\n            match=models.MatchAny(any=[\"blue\", \"yellow\"])\n        ),\n        models.FieldCondition(\n            key=\"occasion\",\n            match=models.MatchAny(any=[\"formal\", \"preppy\"])\n        )\n    ]\n)\n```"
  },
  {
    "threadId": "1274045316689301545",
    "name": "General guidelines for multiple tables with duplicates",
    "messages": "Hi there! New to this community but have LOVED using Qdrant so far in comparison to any other vector DB. Anyway, my team and I are working with regulatory alerts. We will soon be needing to create many tables (potentially one per user). This is because there will be private alerts that are only for some users. My worry is that we might have to duplicate a ton of these alerts across tables. Does anyone have suggestions/guidelines for how to handle these duplicated items or maybe how to better handle these private items?"
  },
  {
    "threadId": "1273952131388276756",
    "name": "Timeout error when running Search Quality",
    "messages": "I have a qdrant database with 300 mln indexed vectors and with quantization. The common search time is arround 1-2 seconds. But when I run \"Search Quality\" in qdrant ui, I get such exceptions in logs:\n```\n2024-08-16T10:20:19.334594Z  INFO actix_web::middleware::logger: 192.168.0.1 \"POST /collections/demo/points/query?timeout=20 HTTP/1.1\" 500 146 \"http://192.168.0.1:6333/dashboard\" \"Chrome ...\" 20.001614\n2024-08-16T10:20:39.484898Z ERROR qdrant::actix::helpers: error processing request: 1 of 1 read operations failed:\n Timeout error: Operation 'Search' timed out after 19 seconds\n```\n\nIt looks like \"Search Quality\" runs full-scan (`exact` search) which take a more than 20 seconds in my database. To be exact, full-scan take even more than 60 seconds. How to fix this issue and run \"Search Quality\"? Should I increase `timeout` param and how?"
  },
  {
    "threadId": "1272884690147217489",
    "name": "GetConsensusCommit request failed when enabling p2p TLS",
    "messages": "Im using the qdrant helm chart and am running into errors when I try to enable p2p tls. Here is my qdrant values config\n\n```yaml\nqdrant:\n  config:\n    cluster:\n      enabled: true\n      p2p:\n        enable_tls: true\n    tls:\n      cert: ./tls/qdrant.crt\n      key: ./tls/qdrant.key\n      ca_cert: ./tls/ca.crt\n  replicaCount: 3\n  fullnameOverride: \"fido-qdrant\"\n  additionalVolumes:\n    - name: tls\n      secret:\n        secretName: certificates-tls-secret\n        defaultMode: 0400\n  additionalVolumeMounts:\n    - name: tls\n      mountPath: /qdrant/tls\n```\nSee next message for results of GET `cluster` and the instance logs"
  },
  {
    "threadId": "1273636368274165840",
    "name": "Bench/Profiling `try` versions of `debug_assert` heavy functions",
    "messages": "I'd like to have a play with the source code: I'm seeing `debug_assert!`s sanity-checks in `unsafe` wrapping fns, and would like to write \"try_...\" versions of them.\n\nThis would most likely add a non-zero cost, thus benching and profiling them would need to be done. I haven't been able to find any discussion/documentation/style-guide with respect to this.\n\nfor example, in mmap_ops.rs:\n\n```rust\npub fn transmute_from_u8<T>(v: &[u8]) -> &T {\n    debug_assert_eq!(v.len(), size_of::<T>());\n\n    debug_assert_eq!(\n        v.as_ptr().align_offset(align_of::<T>()),\n        0,\n        \"transmuting byte slice {:p} into {}: \\\n         required alignment is {} bytes, \\\n         byte slice misaligned by {} bytes\",\n        v.as_ptr(),\n        std::any::type_name::<T>(),\n        align_of::<T>(),\n        v.as_ptr().align_offset(align_of::<T>()),\n    );\n\n    unsafe { &*v.as_ptr().cast::<T>() }\n}\n```\n\nmy thinking would be to add something like this to the api:\n\n```rust\npub fn try_transmute_from_u8<T>(v: &[u8]) -> Result<&T, ???> {\n    if v.len() != size_of::<T>() {\n        return Err(??);\n    }\n\n    if v.as_ptr().align_offset(align_of::<T>()) != 0 {\n        return Err(???);\n    }\n    \n    Ok(transmute_from_u8(v))\n}\n```\n\nFor me, seeing debug-only sanity checks, without a means to\n\n a) opt-in to the sanity checks in a `--release` context and \nb) Not using the type-system and/or language tooling (such as clippy lints) to help prevent 'oopsies' \n\n...tickles a part of my brain in an uncomfortable way.\n\nThe snippets above should address a), and I have a seed of an idea around b) that involves using the never type, doc comments, and setting up lints.\n\n**My current blockage** is lack of project-wide context: If this is something that has already been looked at, or discussed, I would prefer to find something to do that won't be redundant effort.\n\nFor context, I'm motivated by my desire to support my application for the Core Rust Engineer position, and decided to take some initiative."
  },
  {
    "threadId": "1270004949119537192",
    "name": "HTTP headers error",
    "messages": "We've been getting the following error 7 times between July 25th and July 30th.\n```failed to upsert in qdrant collection: rpc error: code = Unknown desc = unexpected HTTP status code received from server: 500 (Internal Server Error); malformed header: missing HTTP content-type```\n\nHere is when these occurred precisely:\n- July 25th at 10:59 PM\n- July 26th at 1:44 PM\n- July 26th at 7:23 PM\n- July 29th at 4:58 PM\n- July 29th at 7:13 PM\n- July 30th at 4:59 PM\n- July 30th at 6:56 PM\n\nOur worker automatically retries and it goes through so these are transient errors. But I want to understand where it's coming from.\nWas there anything going on on your side at these times? Could it be a simple gateway issue?"
  },
  {
    "threadId": "1273434938229919896",
    "name": "Remove value from payload list",
    "messages": "I have a payload field that is a list of values. I want to do something like \"for any points matching a filter, remove value X from the list\". \n\nFrom what I can see, the only option I currently have is to fetch all points matching the filter, remove the value from the payload list field, and then update the payload per point."
  },
  {
    "threadId": "1267587921473437738",
    "name": "Point Deletion Efficiency",
    "messages": "Hi all,\nI am looking to delete points as part of updating content from documents that exist. As I understand it there are two main ways to do this and I was hoping someone could point me in the right direction as to what will be completed most efficiently:\n1. Use the scroll function and filters to get all the UUIDs of the points I would like to delete and perform the deletion command on these ids or \n2. Use the delete command directly and use the filter argument within this function\n\nThe second part of my question is related to the size of deletion made. \n\nI've split out the points to delete based on a integer payload field that has seven options. My plan was to iterate through the seven categories of this payload field and perform my deletion and upsert sequentially (seven times) because it matches how our text scraping pipeline operates. \n\nHowever, I am wondering if it is more efficient for qdrant to have one large deletion and upsertion action rather than seven back to back."
  },
  {
    "threadId": "1272918145686044742",
    "name": "Old data can not be retrieve",
    "messages": "I set up a multi-node Qdrant vector database deployment with Docker Compose on my EC2 instance. I configured this setup with auto-scaling and chose EFS for storage. However, if my existing server goes down and a new server starts, the new cluster cannot retrieve the data, even though the old data is present in the EFS. Does it normal or do we any alternative for this situation?\n\nThanks for your help..."
  },
  {
    "threadId": "1273047753672359967",
    "name": "fastembed TextEmbedding cannot download BAAI/bge-small-en-v1.5 from any source",
    "messages": "I tried to use fastembed, I successfully was able to pip install fastembed, and then run the file that has the class that has from fastembed import TextEmbedding, but when I actually used that class by importing it from a different file, I got an error that said that bge-small-en-v1.5 couldn't download, which is within fastembed. I have the traceback for that error if that would be helpful"
  },
  {
    "threadId": "1272788635682344990",
    "name": "how can I find the exact amount of memory on disk being used by a qdrant point ?",
    "messages": "I was trying to play around with the insertion of a document embeddings as vectors and caption as a payload into a qdrant collection and wanted to find any kind of relation b/w the actual size of the document against the disk usage by it on qdrant but I was to not able to find any kind of docs regarding this other than the formula based calculation of text and integer bytes.I have a docker setup for qdrant and tried to find the disk usage of a collection but It was not accurate according to me."
  },
  {
    "threadId": "1272874001923969114",
    "name": "creating a collection using the golang client - examples?",
    "messages": "I noticed on https://qdrant.tech/documentation/concepts/collections/#create-collection that there is a document that explains how to create a collection using various clients.\nExcept the golang client. \nIs there a reason why the golang client examples are missing there? Are they located elsewhere? Thanks in advance!"
  },
  {
    "threadId": "1272851422819520604",
    "name": "Error searching for points. RocksDB get_pinned_cf error.",
    "messages": "Hello everyone. I have a qdrant instance deployed on Azure Container Apps (ACA), with a collection called \"technicalreports\".\nWhen I search points in that collection, both with the qdrant client or with the HTTP API, I get the following error:\n\n*Service internal error: RocksDB get_pinned_cf error: IO error: While pread offset 236186 len 3383: ./storage/collections/technicalreports/0/segments/3d03d8f2-3c65-4ff3-b56c-60f6720353a8/000011.sst: No such device. Same error using the HTTP client.*\n\nBut it looks like that the file exists.\n\nHere the github issue: [4866](https://github.com/qdrant/qdrant/issues/4866)"
  },
  {
    "threadId": "1272823794913771594",
    "name": "Updating a 27GB collection created through local client (private-gpt/llamaindex)",
    "messages": "**Backgrond**\nI have created a huge collection of documents (the sqlite file is 27 GB) through private-gpt (which uses llamaindex I guess). I came to realise afterwards that it cannot be loaded to memory in local mode (that is the default setting in private-gpt for small RAG question answer setups). I am now trying to update the collection so that it can be used in on_disk mode. I tried to do it through server mode (docker image running on my laptop). However I am unable to make the server recognize the collection. It creates new folder \"collections\" in the storage directory when docker container is started. I copied the said collection to this folder, but it did not reconize any collection. The only way I found was to upload the collection through the web interface (http://localhost:6333/dashboard#/collections). Only .snapshot and .tar files can be uploaded (private-gpt creates a \"qdrant/collection/make_this_parameterizable_per_api_call/storage.sqlite\" and \"qdrant/meta.json\"). I created a .tar from the meta.json and the sqline and tried to upload it through web ui (took 30+ GB of memory, it started some kind of processing in my nvidia GPU [Ada2000 8 GB vram]). I killed the VM (as it would have taken many hours for this with no gurantee of results).\n**Question**:\nHow can I make this collection useable through qdrant server? Is the upload through web-ui the only way to do it? Do I need to change the directory structure of the two files? I see that the test collections available in the web ui have a different structure (multiple folders and files), while the private-gpt (=llamaindex) created collections has only 1 file (sqlite, meta.json, and other jsons for docs and index). I would really appreciate any help if this collection can be modified somehow and made useable. Otherwise I will have to redo all 900 PDFs through a serve mode again."
  },
  {
    "threadId": "1271147111240695901",
    "name": "fastembed textembedding import erros",
    "messages": "I have a file that has \"from fastembed import TextEmbedding\", however, when I run this file, I get an error that says: \"ImportError: cannot import name 'TextEmbedding' from 'fastembed' (unknown location)\". However, if I run \"pip install fastembed\", it says requirement already satisfied. I tried pip show fastembed, and it shows the details including the path, which does point to the venv in my working directory. I tried pip uninstall fastembed and then pip install fastembed again and then immedietely running the file, but it keeps showing the same error. What could be going on?"
  },
  {
    "threadId": "1271535067268907200",
    "name": "Any reason why the query call may fail while search call works on the same collection?",
    "messages": "I am currently trying to implement the new QdrantVectorStore for a hybrid search function, replacingmy old retriever which was based on the Qdrant class. I can make /search queries to the collection, but get a 404 error with the /query endpoint. Why could this be happening?\n`sparse_embeddings = FastEmbedSparse(model_name=\"Qdrant/bm25\")\n        qdrant = QdrantVectorStore.from_existing_collection(\n        \n        embedding=self.embedding,\n        sparse_embedding=sparse_embeddings,\n        sparse_vector_name=\"bm25\",\n        collection_name=\"bc-sparse\",\n        url=self.url,\n        vector_name=\"fast-jina-embeddings-v2-small-en\",\n        retrieval_mode=RetrievalMode.HYBRID,\n        \n            )\n        doc_store = Qdrant(  # pylint: disable=not-callable\n            client=self.qdrant_client,\n            collection_name=\"bc-sparse\",\n            embeddings=self.embedding,\n            vector_name=self.vector_name,\n            content_payload_key='document',\n        )\n        retriever = doc_store.as_retriever(\n            search_type='similarity',\n            search_kwargs={'k': 8},\n        )\n        compression_retriever = BaseChain.create_compression_retriever(\n            self,\n            chunk_size=400,\n            chunk_overlap=100,\n            similarity_threshold=0.81,\n            separators=[' ', ',', '\\n'],\n            retriever=retriever,\n        )`"
  },
  {
    "threadId": "1271984414947278942",
    "name": "search_groups + hybrid search",
    "messages": "Is it possible to use `search_groups`  with hybrid search  `prefetch`  capabilities that seem to be only available with `query_points`?  I can't find anything out of the box, wondering if there's a workaround. Thanks!"
  },
  {
    "threadId": "1272492226425131059",
    "name": "creating geo payload value",
    "messages": "Hello,\nI am trying to set a payload value of type geo coordinate (lat, lon) using the java SDK when inserting a point.\nBasically doing this example https://qdrant.tech/documentation/concepts/payload/#geo but using the SDK instead of json REST call. So far I have been using the ValueFactory https://github.com/qdrant/java-client/blob/master/src/main/java/io/qdrant/client/ValueFactory.java but there is no method that accepts latitude and longitude parameters.\n\nAny idea how to create such a value ? \nThanks"
  },
  {
    "threadId": "1268928511083675711",
    "name": "Update and Migration of a distributed environment",
    "messages": "Hello!\n\nDisclaimer: We have what might not be an ideal distributed environment of qdrant. We have it like this to lower the expenses.\n\nWe are running our own distributed environment of Qdrant on AWS EC2. We only have 3 nodes: 1 in EU, 1 in Asia, and 1 in US. Each of them has its own EC2 instance, and runs through a Docker container. Qdrant version is 1.2.2.\n\nThere is only 1 shard (the one initially created in the first node, which was EU). We then replicated the shard onto the other 2 nodes. This makes it so that all the data is in sync and present in all nodes (and we need that).\n\n1st question: how do you recommend updating the version of Qdrant? 1 node at a time, and we update every minor, correct? Basically we could start by simply building a new docker image using 1.3.0 (the version immediately after 1.2.2), deploy it in Asia, and wait for it to have an ok status, be in sync, etc. Then we would move on to US, etc. Is this the way to go? Is there a specific version of Qdrant we should aim to update to?\n\n2nd question: We have a new ML model and we basically have to migrate all our data to the new format. For this we thought about uploading a snapshot to s3 (backup), creating a new collection through a separate server that we boot up just during the migration, recalculate all the vectors, replicate in the other two nodes, and then swap the alias to the new collection. Thoughts? Are there easier ways or something we might be missing?\n\nThanks!"
  },
  {
    "threadId": "1260529217423147058",
    "name": "FastEmbed skip SSL",
    "messages": "Hello\n\nI want to use FastEmbed in my company network.\nWhen I use it I get the following error:\n\n`bm25 = SparseTextEmbedding(model_name=\"Qdrant/bm25\")`\n```python\npython .\\5_old_bm25_to_new_bm25.py\n2024-07-10 11:30:08.943 | ERROR    | fastembed.common.model_management:download_model:250 - Could not download model from HuggingFace: (MaxRetryError(\"HTTPSConnectionPool(host='hugging\nface.co', port=443): Max retries exceeded with url: /api/models/Qdrant/bm25/revision/main (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)')))\"), '(Request ID: 9b9fcef0-0adb-4126-8ce0-127c4e06dd47)')Falling back to other sources.\n```\n\nHow can I bypass the SSL check made by FastEmbed or how can I manually download the model and put it somewhere ?\nWhen I check in a docker container on a PC outside this network it's located here\n```\nroot@98ef463f459a:/# find / -name \"stopwords.txt\" 2>/dev/null\n/tmp/fastembed_cache/models--Qdrant--bm25/snapshots/7ae3a4f436af2849dfa49da9aec5132caac38f0c/stopwords.txt\n```\n\nMaybe I can just create this path but the issue it's on a linux machine as root and I work on Windows, so not the same path (and maybe not the same snapshot ID).\nIf I could just skip this SSL Check it would be better\n\nThanks !"
  },
  {
    "threadId": "1271428356474277963",
    "name": "Configuration no updating as per the parameters",
    "messages": "I have created qdrant in local with below-mentioned configuration\n\n```client = QdrantClient(path=local_db)\nclient.recreate_collection(\n            collection_name=self.collection_name,\n            vectors_config=models.VectorParams(\n                size = 1024,\n                distance=models.Distance.COSINE,\n            ),\n            optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000),\n            hnsw_config=models.HnswConfigDiff(\n                m = 128,\n                ef_construct = 1024,\n                on_disk=True\n            ),\n        )```\n\nBut **meta.json** file is not updated with all the parameters, Is there anything wrong with my configuration or it is correct?\nWhy is other parameters like hnsw_config in null\n```{\n    \"collections\": {\n        \"backgrounds\": {\n            \"vectors\": {\n                \"size\": 1024,\n                \"distance\": \"Cosine\",\n                \"hnsw_config\": null,\n                \"quantization_config\": null,\n                \"on_disk\": null\n            },\n            \"shard_number\": null,\n            \"sharding_method\": null,\n            \"replication_factor\": null,\n            \"write_consistency_factor\": null,\n            \"on_disk_payload\": null,\n            \"hnsw_config\": null,\n            \"wal_config\": null,\n            \"optimizers_config\": null,\n            \"init_from\": null,\n            \"quantization_config\": null,\n            \"sparse_vectors\": null\n        }\n    },\n    \"aliases\": {}\n}```"
  },
  {
    "threadId": "1271582108900659244",
    "name": "huggingface bm42",
    "messages": "Could i use this serverless inference point to generate sparse bm42 vectors?\nhttps://huggingface.co/Qdrant/all_miniLM_L6_v2_with_attentions"
  },
  {
    "threadId": "1271542736008515745",
    "name": "Error: Connection terminated unexpectedly",
    "messages": "Using my endpoint as usual using the Endpoint + API Key, suddenly its giving errors. We are not using client (it doenst work for us on Node) so we are using the url directly\n\nStarted Today, someone please check/verify?\nQDrant Cloud Cluster ID: c8881e7d-82ee-4431-951d-1b51e99d9218\n\nExample Code:\n`\nasync function deleteVectorsByName(collectionName, name) {\n  try {\n    const response = await axios.post(\n      `${CLUSTER_URL}/collections/${collectionName}/points/delete`,\n      {\n        filter: {\n          must: [\n            {\n              key: \"name\",\n              match: {\n                value: name,\n              },\n            },\n          ],\n        },\n      },\n      {\n        headers: {\n          'Content-Type': 'application/json',\n          'Authorization': `Bearer ${CLUSTER_APIKEY}`,\n        },\n      }\n    );\n    if (response.status === 200) {\n      //console.log(`Deleted vectors with name: ${name}`);\n    } else {\n      console.log(`Failed to delete vectors with name: ${name}`);\n    }\n  } catch (error) {\n    console.log(`Error deleting vectors with name ${name}: ${error.toString()}`);\n  }\n}\n`"
  },
  {
    "threadId": "1271035828767952991",
    "name": "Qdrant self hosted Error",
    "messages": "Hi,\n\ni┬┤m trying to host a qdrant instance using Docker. I┬┤m doing it on a virtual Machine with Windows Server x64 based,  by following this article:\n\nhttps://medium.com/@fadil.parves/qdrant-self-hosted-28a30106e9dd\n\nWhen i run the command: docker run -p 6333:6333 -v C:\\Users\\xxx\\qdrant_storage:/qdrant/storage qdrant/qdrant\n\nI get this error:\n\ndocker: no matching manifest for windows/amd64 10.0.20348 in the manifest list entries.\n\nCan someone help me here?"
  },
  {
    "threadId": "1271277806218772531",
    "name": "Could anyone give me an example of a cURL command recreating a collection with sparse vector?",
    "messages": "Currently trying to do this, but getting an error related to VectorsConfig--what is VectorsConfig?: \n`PUT http://qdrant:6333/collections/bc-sparse \\\n  -H 'Content-Type: application/json' \\\n  --data-raw '{\n    \"vectors\": {\n        \"fast-jina-embeddings-v2-small-en\": {\n            \"size\": 512,\n            \"distance\": \"Cosine\"\n        },\n        \"sparse_vectors\":{\"bm25\":{\"modifier\":\"idf\" }}\n      },\n    \"init-from\": {\n        \"collection\": \"[test_collection]\"\n      }\n    }'\n{\"status\":{\"error\":\"Format error in JSON body: data did not match any variant of untagged enum VectorsConfig at line 8 column 7\"},\"time\":0.0}% `"
  },
  {
    "threadId": "1270773827323953224",
    "name": "S3 Snapshot storage issue",
    "messages": "Hi ! Recently in out company, we have been trying to do a qdrant setup on ec2 with snapshots to be generated and saved to s3 via the config file as mentioned in https://qdrant.tech/documentation/concepts/snapshots/\n\nAlthough the config contains all the necessary field and the access key has full access to S3 we are encountering the following error \n\n```\nINFO object_store::client::retry: Encountered transport error backing off for 1.1301689 seconds, retry 10 of 10: error sending request for url (https://com.amazonaws.us-west-1.s3/{bucketname}/{foldername}?list-type=2&prefix=snapshots%2Fdemo_collection%2F)\n```\n\nCould someone help me with this issue ?"
  },
  {
    "threadId": "1271305559697526866",
    "name": "Qdrant Setup to Different Port",
    "messages": "Is it possible to use a different port for Qdrant instead of using the default 6333 port and can we have multiple Qdrant being setup in the same local host?"
  },
  {
    "threadId": "1271010570354823231",
    "name": "is there any way through which I can clear memory for my qdrant pod?",
    "messages": "I'm using on_disk for vectors, index as well as for payload. But still, there's constant increase in memory consumption.\nCan you tell is there a way to have TBs of vector data in qdrant with limited 100 GB RAM consumption in cluster?"
  },
  {
    "threadId": "1259856735384895529",
    "name": "[Langchain] Hybrid search",
    "messages": "Hey, some clues to do hybrid search with Langchain in Qdrant ?"
  },
  {
    "threadId": "1270802978890059786",
    "name": "Large ef_search and offset",
    "messages": "I have a query use-case to get 200,000 nearest vectors from the database. I do realized that large offset and large `ef_search` will have significant performance impact but I wonder if there could settings that could improve it. My database has 150M vectors, 1152 dimensions, float16, 32 vCPU, 256 GB RAM. \n\nI'm seeing about 30-40 seconds for the query to be returned.\n\n1. Will it be faster if I increase vCPU?\n2. I experimented with `ef_search` , I had it at 500_000, 1_000_000, and 5_000_000, each request seems to returned with similar latency, but I would expect 500k to be faster than 5 mil\n3. I had another database with the same set up but just 75M vectors, the same query took about half the time. I'm not an expert on HNSW, but is that expected?  I thought search performance would be more related to `ef_search` value.\n\n\nHere is the config\n```py\n    collection_name=collection_name,\n    hnsw_config=rest.HnswConfigDiff(\n        on_disk=False,\n        m=16,\n        ef_construct=128,\n    ),\n    vectors_config={\n        \"\": rest.VectorParamsDiff(\n            on_disk=True\n        ),\n    },\n    quantization_config=rest.ScalarQuantization(\n        scalar=rest.ScalarQuantizationConfig(\n            type=rest.ScalarType.INT8,\n            quantile=0.99,\n            always_ram=True,\n        ),\n    ),\n    optimizers_config=rest.OptimizersConfigDiff(\n        indexing_threshold=20000,\n    ),\n```\n\nThe query is \n```py\nsearch_result = client.search(\n    collection_name=collection_name, \n    query_vector=vector, \n    limit=10000,\n    search_params=SearchParams(\n        hnsw_ef=1_000_000,\n        indexed_only=True,\n        exact=False\n    ),\n    offset=100_000\n)\n```\n\nThanks!"
  },
  {
    "threadId": "1270658970394300436",
    "name": "Deleting points and its associated vectors from qdrant collection",
    "messages": "I am using below code to delete points using its point default id, which uniquely identifies points in collection in qdrant vector db.\n\n#Collecting point ids in a list:\nfor doc in doc_to_delete[0]:\nids_to_delete.append(doc.id)\n\n#deleting points from collection using point ids\nvector_client.delete(\ncollection_name=collection_name,\npoints_selector=models.PointIdsList(points=ids_to_delete)\n\nCan someone confirms me will it delete both points and its associated vectors from collection?\n\nAfter running this code, I verified in qdrant UI and I can see points are getting deleted, but the vectors count remain the same.\n\nCounts before Deletion:\nvectors_count = 27\npoints_count = 27\n\nI deleted 4 points, Counts after deletion in qdrant UI:\nvectors_count = 27\npoints_count = 23\n\nisn't both vectors_count and points_count should be same if above method deletes both points and its associated vectors as well?\n\nThanks,\nDeepika"
  },
  {
    "threadId": "1267591733852508232",
    "name": "Crash on Create Collection in 1.9.5 and later",
    "messages": "This is odd... when running 1.9.4 or earlier on Windows, everything works fine. But from 1.9.5 through latest (1.10.1 at this time), creating a collection crashes the app. Is this a known issue?"
  },
  {
    "threadId": "1270165763545628702",
    "name": "Issues with upload_collection",
    "messages": "I should preface with: I suspect I'm missing or unaware of something, this is most likely a user issue.\n\nI'm running the latest docker image, and I wrote some super simple tests to check some basic functionality. However, I'm running into weird behavior that I have spent the past couple hours trying to debug and am getting nowhere. At this point I'm desperate so any help would be much appreciated.\n\n```python\nQdrantClient('localhost:6333').create_collection(\n    collection_name='test',\n    vectors_config=models.VectorParams(\n        size=1024,\n        distance=models.Distance.COSINE\n    )\n)\n\nQdrantClient('localhost:6333').upload_collection(\n    collection_name='test',\n    payload=[{'data': 'text1'}, {'data': 'text2'}, {'data': 'text3'}, {'data': 'text4'}, {'data': 'text5'}, {'data': 'text6'}, {'data': 'text7'}],\n    vectors=[[0.0]*1024, [0.1]*1024, [0.25]*1024, [0.5]*1024, [0.75]*1024, [0.9]*1024, [1.0]*1024],\n    parallel=4\n)\n\nsearch_result = QdrantClient('localhost:6333').search(collection_name='test', query_vector=[0.0]*1024, with_payload=True, limit=1) # Expecting to find 'text1'\n\nprint(search_result)\n```\nThe print returns `[]` which is of course unexpected.\n\nWhen I look in the dashboard and copy each vector to clipboard, I get:\ntext1: [0, 0, ..., 0, 0]\ntext2: [0.03125, 0.03125, ..., 0.03125, 0.03125]\ntext3: [0.03125, 0.03125, ..., 0.03125, 0.03125]\ntext4: [0.03125, 0.03125, ..., 0.03125, 0.03125]\ntext5: [0.03125, 0.03125, ..., 0.03125, 0.03125]\ntext6: [0.031250004, 0.031250004, ..., 0.031250004, 0.031250004]\ntext7: [0.03125, 0.03125, ..., 0.03125, 0.03125]\nwhich is also not expected."
  },
  {
    "threadId": "1270424870735450293",
    "name": "indexed_vectors_count vs. vectors_count vs. points_count",
    "messages": "Hi all, \n\nI created two collections using Qdrant client with the following implementation:\n```\nQdrant.from_documents(\n    doc_chunks,\n    embedding_model,\n    collection_name=os.getenv(\"QDRANT_INDEX\"),\n    location=os.getenv(\"VECTOR_BASE_URL\"),\n    force_recreate=True,\n)\n```\nThe first one (smaller) contains 970 `points` because `doc_chunks` contains 970 chunks, the second one (larger) contains 99885 `points` because `doc_chunks` contains 99885 chunks.\n\nWhen I check the collections I get \nFor 1:\n- indexed_vectors_count = 0\n- vectors_count = None\n- points_count = 970\n\nFor  2:\n- indexed_vectors_count = 98368\n- vectors_count = None\n- points_count = 99885\n\nCan someone explain to me why `indexed_vectors_count` is 0 for 1? I mean every point has a multidimensional vector attached when I check a single point object.\nAlso I don't get why `vectors_count` = None in both cases?\n\nAccording to the documentation:\n- *vectors_count* - total number of vectors in a collection, useful if you have multiple vectors per point\n- *indexed_vectors_count* - total number of vectors stored in the HNSW or sparse index. Qdrant does not store all the vectors in the index, but only if an index segment might be created for a given configuration.\n\nNot sure if I got the difference ... \n\nWould greatly appreciate some enlightening explanation! ЁЯЩП \nThanks!"
  },
  {
    "threadId": "1268960107484545135",
    "name": "Name matching with abbreviations, initials, et",
    "messages": "I am trying to build a matching solution for names fo companies. A full name might be Texas Instruments Inc, but common names might be \"TI\", or \"Ti Inc\" or \"Texas Instr\", etc. Incoming names may also include typos \"Texus Instruments\", or \"Texas Instrooments\"... \n\nI'm looking to build a solution that would take an input, and find the best match, or a high-probability match.\n\nI think the key is in the embedding solution, or, perhaps in having multiple vectors?? I'm not sure. Seems like there a a few ways to go.\n\nI also thought maybe I'd use an LLM type of \"Text\" that is embedded, like:\n\"The company name is {name}, with valid abbreviations of AA,BB,CC\"\n\nLooking for some advice from experienced users"
  },
  {
    "threadId": "1270097624757829642",
    "name": "QDrant: Find similar by ID or filter by payload key:value pair",
    "messages": "Best Regards to All,\n\nI would like to know, when you enter into a collection in the QDrant Dashboard. There is a placeholder with the message:\n\n\nFind similar by ID or filter by payload key:value pair. Example: 'name: John Doe, age: 25, id: c0847827-d005-4e46-b328-887f72373d2d , id: 1234567890'\n\n\nQueries can be made in this field, using a prompt. ?\n\nI tried searching by ID and it works, but I would like to check the above doubt."
  },
  {
    "threadId": "1268472259845034025",
    "name": "Promotheus metrics missing for response time of scroll method",
    "messages": "Good (European) Morning,\n\nWe are running Qdrant in docker (version 1.9.1), and are creating dashboards using the exposed prometheus metrics.\nSpecifically, we are interested in the avg and max response times of the various requests that are sent to our qdrant database using grpc.\n\nFrom the metrics, we do get timings for the Recommend endpoint, but not for the Scroll calls.\nWe use GRPC to call the endpoint, however some manual tries with HTTP client also yielded no stats on the scroll endpoint in the metrics.\n\nIs the scroll endpoint excluded from these metrics?"
  },
  {
    "threadId": "1269965496321577014",
    "name": "How to Not Use NamedVector in go-client MultiVectors.",
    "messages": "I am trying to use MultiVectors in one of my collections, but I was not able to understand how to use normal MultiVectors of other language clients like in C# in Go.\n\nI have the following setup for my collection:\n\n```go\nfunc (qe *QdrantEngine) CreateMediaCollection() error {\n    ctx, cancel := context.WithTimeout(context.Background(), time.Second*10)\n    defer cancel()\n\n    req := &pb.CreateCollection{\n        CollectionName: \"Media\",\n        VectorsConfig: &pb.VectorsConfig{Config: &pb.VectorsConfig_Params{\n            Params: &pb.VectorParams{\n                Size:     384,\n                Distance: pb.Distance_Cosine,\n                MultivectorConfig: &pb.MultiVectorConfig{\n                    Comparator: pb.MultiVectorComparator_MaxSim,\n                },\n            },\n        }},\n    }\n\n    _, err := qe.collections.Create(ctx, req)\n    return err\n}\n```\n\nThe `Upsert` here in the `vectors_vectors` method is only using the `NamedVectors` datatype:\n\n```go\nnamedVectors := &pb.NamedVectors{\n    Vectors: make(map[string]*pb.Vector),\n}\n\nnamedVectors.Vectors[\"\"] = &pb.Vector{\n    Data: make([]float32, 384),\n} // Creates a slice with 384 zeroes\n\ncustomLogger.Info(\"namedVectors\", namedVectors)\n\nupsertPoints := []*pb.PointStruct{\n    {\n        Id: &pb.PointId{\n            PointIdOptions: &pb.PointId_Num{Num: uint64(pointIdNum)},\n        },\n        Vectors: &pb.Vectors{VectorsOptions: &pb.Vectors_Vectors{\n            Vectors: &pb.NamedVectors{Vectors: map[string]*pb.Vector{\n                \"\": {Data: make([]float32, 384)},\n            }},\n        }},\n    },\n}\n```\n\n---\n\nIn this setup, the collection creation and upsert methods are shown, where the `NamedVectors` datatype is used for handling vectors."
  },
  {
    "threadId": "1265820547783069796",
    "name": "fastembed using qdrant",
    "messages": "from fastembed.sparse.bm25 import Bm25\nfrom fastembed.late_interaction import LateInteractionTextEmbedding"
  },
  {
    "threadId": "1268870710579953715",
    "name": "Confused between Lambda, EC2 and ECS for my slack RAG chatbot",
    "messages": "Hello everyone! ЁЯСЛ\n\nI'm currently developing a Slack bot using Retrieval-Augmented Generation (RAG) to answer HR and company-related queries. HereтАЩs the tech stack IтАЩm using:\n\n> LLM: AWS Bedrock (Mixtal 8*7b)\n> Embeddings: OpenAI (text-embedding-3-small)\n> Vector Store: Zilliz (serverless?) or Qdrant\n> Documents Storage: AWS S3\n\nThe bot will serve multiple users in our Slack organization, allowing them to interact with it simultaneously. Additionally, it needs to store conversation history for each user, which will be used by the LLM to provide contextually relevant responses. However, IтАЩm trying to decide between AWS Lambda, EC2, or ECS for hosting the backend, and I'm unsure which option best fits my requirements. \n\nI'd love to hear your experiences or recommendations for similar scenarios. What factors should I consider most, and are there best practices for these services? How do you handle storing conversation history in a scalable manner, especially when it's used by the LLM?\n\nThanks for your insights! ЁЯШК"
  },
  {
    "threadId": "1269058445966053406",
    "name": "Time intelligent vector search",
    "messages": "Hi there, \n\nI need some help on searching for vectors. There is data where content is duplicated year on year. How do I make vector db to search for the latest data. If data is available for the current year return it otherwise search for the previous year? Latest and relevant results."
  },
  {
    "threadId": "1280192914470862848",
    "name": "Error: No Space Left on Device",
    "messages": "Hi,\n\nReceiving an error. I checked <@1016739049496662126>'s thread on a smilar topic & it did not help.\n\n```\ngrpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n    status = StatusCode.INTERNAL\n    details = \"Service internal error: No space left on device: WAL buffer size exceeds available disk space\"\n    debug_error_string = \"UNKNOWN:Error received from peer  {created_time:\"2024-09-02T10:31:31.256552945+00:00\", grpc_status:13, grpc_message:\"Service internal error: No space left on device: WAL buffer size exceeds available disk space\"}\"\n>\n```\n\nThis is the specific error I'm getting. My qdrant configuration is a docker-compose with the official image, and looks like this:\n\n```\n    qdrant:\n      image: qdrant/qdrant:v1.9.6\n      ports:\n        - \"127.0.0.1:6333:6333\"\n        - \"127.0.0.1:6334:6334\"\n      volumes:\n        - ./qdrant_data/storage:/qdrant/storage\n        - ./qdrant_data/config:/qdrant/config/custom_config.yaml\n    # Cria-Back Criadex\n```\n\nDoing a little googling, WAL is the write-ahead log. Is this a simple case of no storage left on device? Because the folder seems to have like 1TB available on the mounted drive."
  },
  {
    "threadId": "1268213652104675418",
    "name": "Import a snapshots to new Qdrant cluster.",
    "messages": "How long it takes to prepare new Qdrant cluster from snapshots (about 10 GB of few collections. each with 3 payload indexes).\nLooks like import itself takes just few minutes, but question is what king of backgroud tasks  (optimisation, indexes and other) jobs have to finish before I can use new Cluster. \nWhat real time should I calculate for that 10 minutes 1 hour or even half a day?"
  },
  {
    "threadId": "1268537895665274950",
    "name": "Bulk Upload Performances",
    "messages": "Hi everyone\nI am trying to upload around 25M points in my qdrant collection\nI followed the instructions in the bulk upload vectors webpage but even without indexing, upload is really fast in the beginning but then goes very slowly (30% in 1h10, 50% 2h30, 75% 6h40, i'm at 99% in 20h)\nWhat explains this behavior ?\nI'm using 4 shards and uploading with 4 processes at the same time\nAlso I just turned indexing back on and my qdrant container only uses 5 to 10 % CPU why is it so low ?"
  },
  {
    "threadId": "1212382720206966864",
    "name": "Need advice related to Bulk Upsert (around 200M points)",
    "messages": "Hello,\n\nI was using an instance with low IOPS (around 5K), and Qdrant was taking a long time to optimize the collection. Recently, we started using a local SSD volume disk with higher IOPS (around 300k), hoping it would solve the optimization issue.\n\nCurrent Instance Config:\n- 64 vCPU\n- 512GB RAM\n- 300k IOPS\n\nI have around 200M+ points, with both vector and payload data. What's the best way to upsert the data?\n\n1. How many segments should I have?\n2. While creating the collection, should I keep `indexing_threshold=0` and `memmap_threshold=0`?\n3. Should I create the payload index while creating the collection or after uploading all the points?\n4. I am keeping `on_disk_payload=0` and `on_disk=0` ... I think that should help?"
  },
  {
    "threadId": "1268225775895838790",
    "name": "New to Qdrant",
    "messages": "Hello, I created a new 2-node Qdrant cluster via Helm on Kubernetes. When I do a search, all load is on the first node. No matter what I do. Even connect directly to the 2nd node. \nAny idea what is going on/what am I missing? \n\nCollection Cluster Info\nShard ID    Location                        Status\n0            Local (2294480009010235)        Active\n1            Remote (4714318486540000)    Active"
  },
  {
    "threadId": "1268198684651425863",
    "name": "Nearest Neighbour Search",
    "messages": "A bad question i know,\n\nBut is there a way to search for the closest pairs with k number of results without giving any query vectors as parameters?\n\nThanks"
  },
  {
    "threadId": "1278387395602939998",
    "name": "Does the python qdrant_client work with self-signed certs?",
    "messages": "Hi all!\n\nI have a Qdrant cluster deployed to a k8s cluster which is behind an internal nginx ingress controller lb. I need to pass the ca cert that is terminated on the ingress controller to be able to connect. Is it possible to pass self signed certs  with the  qdrant_client? I was looking through the docs and didnt seem to find anything. \n\nThank you for your time."
  },
  {
    "threadId": "1267806078813474868",
    "name": "least similar hits",
    "messages": "Is there a way in which I can find the least similar hits in collection of large number of points ? take care that if I am gonna use offset to get the last similar points, I would need to rise the number of limit= number of points in my collection, which will consume too much time in retrieving points."
  },
  {
    "threadId": "1267681360408084527",
    "name": "issue with similarity search",
    "messages": "I was implementing similarity search for my project but I noticed an issue with a particular dataset. The search query doesn't seem to work unless I use these params   \n\n\"params\": {\n    \"hnsw_ef\": 128,\n    \"exact\": true\n    },\n\nCould someone help me better understand this issue? \n\nContext: \nQdrant Cloud\ncosine similarity \nOver 17k vectors in this dataset\n\nOriginal Query: \n{\n    \n    \"with_payload\": true,\n     \"filter\": {\n        \"must\": [{\"key\": \"dataset_id\",  \"match\": { \"value\": \"aac1e345-50ec-4b77-8e4f-5c0c8083c94d\"}}]},\n    \"offset\": 10,\n    \"limit\": 10,\n    \"vector\": query_vector}"
  },
  {
    "threadId": "1266539815776555089",
    "name": "\"The read operation timed out\" (ALREADY CHECKED OUT DOCS RE THIS)",
    "messages": "I currenly have about 600k embeddings on a free tier that are taking up most of my disk тАФ upserting them works fine but search doesn't. Is this because the vectors are on disk? I'm pretty new to qdrant so not sure how to debug\n\nAll my vectors are in one collection тАФ is that best practice?\n\nmy code to search\n```\nfrom qdrant_client import QdrantClient, models\nimport os\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\n\nload_dotenv()\n\nqdrant_client = QdrantClient(\n    url=\"URL:6333\",\n    api_key=os.getenv('QDRANT_API_KEY')\n)\n\n\nopenai = OpenAI()\n\nembedding = openai.embeddings.create(input=[\"Drake rapper\"], model='text-embedding-3-small').data[0].embedding\n\nqdrant_client.search(\n    collection_name=\"captionEmbeddings-1536\",\n    query_vector=embedding,\n    limit=5,\n)```"
  },
  {
    "threadId": "1266958363124699197",
    "name": "Qdrant semantic search, how to make it work?",
    "messages": "Hello,\n\nI implemented a small application using Qdrant and utilized the text-ada-003 model for embeddings, which allows for selecting the embedding vector size. I created a collection with 256-sized vectors and chunked the paragraphs from two pages of a book.\n\nI watched a quick introductory video from the Qdrant team:\nhttps://www.youtube.com/watch?v=AASiqmtKo54\n\nThe process I followed is similar to what was demonstrated in the video, but it doesn't seem to achieve true \"semantic search.\" In the video, the collection of books is searched for \"alien invasion,\" and the results only include documents with the words \"alien\" and \"invasion\" in their metadata. While this method uses cosine similarity for searching, it appears more like a keyword search rather than a search based on meaning.\n\nI tried summarizing some paragraphs with GPT and searching using these summaries. This approach yielded some relevant chunks, but it doesn't seem to capture the deeper insights of a semantic search.\n\nWhen using TensorFlow Projector, the search displays a word and its neighbors, which is more aligned with what I'm aiming for. How can I achieve similar results with Qdrant?\n\nFor example, let's take page 10 of \"20,000 Leagues Under the Sea\":\nhttps://www.arvindguptatoys.com/arvindgupta/20000-leagues.pdf\n\nSuppose we chunked the page with one vector per paragraph (let's say the five large paragraphs you can see at page 10). If I search for \"Journalists talking about strange creatures,\" I would expect the following paragraph to have the highest confidence score semantically:\n\n\"For six months the war seesawed. With inexhaustible zest, the popular press took potshots at...\"\n\nThis expectation is based on the presence of words like \"press\" and the talking about newspapers and nature, creatures etc. However, the current implementation seems to work well only with keywords (and is case-sensitive) rather than concepts. Like \n\nWhat am I missing to achieve a true semantic search with Qdrant?\n\nThank you."
  },
  {
    "threadId": "1267498520085004451",
    "name": "The read operation timed out",
    "messages": "Hello, \n\nWe have configured a cluster in Qdrant Cloud that contains a couple of collections. We are observing the error `ResponseHandlingException: The read operation timed out` while performing a `search` in the collection. I have provided the collection configuration information.\n\nAny help on this would be appreciated.\n\n```{'status': <CollectionStatus.GREEN: 'green'>,\n 'optimizer_status': <OptimizersStatusOneOf.OK: 'ok'>,\n 'vectors_count': None,\n 'indexed_vectors_count': 1588646,\n 'points_count': 1589617,\n 'segments_count': 15,\n 'config': {'params': {'vectors': {'size': 384,\n    'distance': <Distance.COSINE: 'Cosine'>,\n    'hnsw_config': {'m': 32,\n     'ef_construct': 100,\n     'full_scan_threshold': 10000,\n     'max_indexing_threads': None,\n     'on_disk': True,\n     'payload_m': None},\n    'quantization_config': None,\n    'on_disk': None,\n    'datatype': None,\n    'multivector_config': None},\n   'shard_number': 2,\n   'sharding_method': None,\n   'replication_factor': 1,\n   'write_consistency_factor': 1,\n   'read_fan_out_factor': None,\n   'on_disk_payload': True,\n   'sparse_vectors': None},\n  'hnsw_config': {'m': 16,\n   'ef_construct': 100,\n   'full_scan_threshold': 10000,\n   'max_indexing_threads': 0,\n   'on_disk': True,\n   'payload_m': None},\n  'optimizer_config': {'deleted_threshold': 0.2,\n   'vacuum_min_vector_number': 1000,\n   'default_segment_number': 0,\n   'max_segment_size': None,\n   'memmap_threshold': None,\n   'indexing_threshold': 5000,\n   'flush_interval_sec': 5,\n   'max_optimization_threads': None},\n  'wal_config': {'wal_capacity_mb': 32, 'wal_segments_ahead': 0},\n  'quantization_config': None},\n 'payload_schema': {'initiativeId': {'data_type': <PayloadSchemaType.TEXT: 'text'>,\n   'params': None,\n   'points': 1589617},\n  'clientId': {'data_type': <PayloadSchemaType.TEXT: 'text'>,\n   'params': None,\n   'points': 1589617}}}```\n\nCluster configuration:\nRAM: 8GB\nvCPUs: 1.0 vCPU\nDisk space: 32 GB\nNodes: 1 Node"
  },
  {
    "threadId": "1267761097591492639",
    "name": "Search time degradation in v1.9.7",
    "messages": "Hi everyone!\nI created several Qdrant databases on v1.8.X. Each one has around 300 mln vectors, 9 segments and the search time 1-2 seconds. Now I'm creating new Qdrant databases on v1.9.7 with the same config and find that 330 mln vectors take 423 segments. Moreover, search time increased from 1-2 seconds to 60+ seconds. Why? I'm using the same config. Should I forcefully decrease segments count?\nI found this article https://qdrant.tech/documentation/guides/optimize/#latency-vs-throughput that give high-level understanding about segments. It describes two values of segments count: 2 and \"the number of cores\". So I think that 423 segments for my 32 cores is the reason of such slow search speed, isn't it? To be clear, my goal is to have min latency and I don't care about index time at all"
  },
  {
    "threadId": "1261163694704824330",
    "name": "Deadlock on upsert",
    "messages": "Hi Qdrant Team, \nWe are running into a deadlock on upsert and this seems to be agnostic to the qdrant version we are running. \nWARN collection::collection_manager::holders::segment_holder: Trying to read-lock all collection segments is taking a long time. This could be a deadlock and may block new updates.\n\nQdrant is being run locally in a docker container. I've tried upserting the same vector-set on version 1.10.0 and 1.9.4 (a version that has previously worked for us without deadlocking, but is now also deadlocking). I am finding we are hitting a deadlock at about the 200k point mark. Could this be a result of upserting too quickly and the indexing process is causing a deadlock? \n\nMachine specs:\nCPU: Intel Xeon Gold 6314U with 24 cores\nRAM: 320GB\nDisk: shared network averaging ~90k IOPS\n\nWe are batch upserting vectors in batch sizes of 100 at a time. Vector specs:\nRunning Dense and Sparse vectors with 22 different payloads, 17 of them are indexed. The index is set prior to upserting."
  },
  {
    "threadId": "1266547520444567572",
    "name": "Is there a way to move vectors from disk to RAM",
    "messages": "I have about 600k vectors that i want to move to RAM (at least the max amount)\n\nhow can i do that?"
  },
  {
    "threadId": "1267094177821491313",
    "name": "Errors with Multivector queries",
    "messages": "Currently, I am running qdrant locally on Docker (Version: 1.10.1, build: 4aac0231). \n\nI am trying to create a multimodal search engine where search results are based on image embeddings and text embeddings. I used a multivector collection:\n```py\nqdrant_client = QdrantClient(host=\"localhost\", port=6333)\nqdrant_client.recreate_collection(\n    collection_name=\"titles_collection\",\n    vectors_config=VectorParams(size=text_embeddings.shape[1],\n    distance=qdrant_models.Distance.COSINE,\n    multivector_config=models.MultiVectorConfig(\n            comparator=models.MultiVectorComparator.MAX_SIM\n        ),),\n)```\n\nFor the embeddings I used a CLIP model and stored the embeddings as such:\n```py\n# Preparing points to upload\npoints = [\n    qdrant_models.PointStruct(\n        id=row['id'],\n        vector=[image_embedding.tolist(), text_embedding.tolist()],\n        payload={'title': row['title']}\n    )\n    for row, image_embedding, text_embedding in zip(df.to_dict(orient='records'), image_embeddings, text_embeddings)\n]\n\n# Upload points to Qdrant\nqdrant_client.upsert(\n    collection_name=\"titles_collection\",\n    points=points,\n)```\n\nHowever, when I tried to query by following the documentation code for multivectors:\n```py\ntitle = [\"Person Wearing Something\"]\ninputs = tokenizer.tokenize(title).to(device)\nwith torch.no_grad():\n    query = model.encode_text(inputs).cpu().numpy()\n\nsearch_results = qdrant_client.query(\n    collection_name=\"titles_collection\",\n     query=[query[0],query[0]],\n)```\n\nI got the error `TypeError: QdrantFastembedMixin.query() missing 1 required positional argument: 'query_text'`\n\nHelp would be much appreciated! I'm unsure if this is even the correct approach for multimodal search in qdrant. I didn't have much luck with hybrid search either. \n\nFull code here:\nhttps://colab.research.google.com/drive/1-dKPc641iD9XRZWaT32nejF7jh2I-E3Z?usp=sharing"
  },
  {
    "threadId": "1267451037971648662",
    "name": "deleted_threshold???",
    "messages": "How deleted_threshold really works? Let's say I have 1M points and once a day I delete around 100-200 points. I have deafault deleted_threshold: 0.2 should I change it to lets say deleted_threshold: 0.01 or even less If I want to get rid of those points each time I do deletion job. Or there is other possible way to remove records from collection ASAP?"
  },
  {
    "threadId": "1267362925463474279",
    "name": "Migration from one server to other.",
    "messages": "Is there a way to copy/migrate collections from one server to other instead of going through each record via API"
  },
  {
    "threadId": "1267344208201519114",
    "name": "Build Apache Airflow with Qdrant Vector Database",
    "messages": "I want to know how to build a project apache airflow use qdrant like database and run daily async. I try more ways but it always has error about importing library like \"No module name 'airflow.providers.qdrant'\". I follow this link: https://qdrant.tech/documentation/frameworks/airflow/. Please help me to resolve as soon as. Thank everyone."
  },
  {
    "threadId": "1266340647217139773",
    "name": "Memory consumption keep increasing / not released when deleting collection",
    "messages": "**Summary:**\nWe have a Qdrant cluster with collection that we delete and recreate every day. We noticed that the memory consumption of the cluster keep increasing and not released after deleting the collection.\n\n**Details of the issue:**\nWe have a Qdrant cluster with collection that are setup in a blue / green manner with daily reindexing. Each day, we delete the collection that is not currently in use (flagged by an alias), recreate the collection with the new data, and switch the alias to the new collection. We noticed that the memory consumption of the cluster keep increasing and not released after deleting the collection. It reaches the point where it consumes more than the k8s memory request and I am afraid that the pod will be evicted.\n\nIf we restart the cluster, the consumption seems to be normal again. But that's not a solution.\n\nWe don't set any k8s memory limit, do you think it could improve the situation somehow?\n\nI enclose a graph of memory consumption, blue line is used memory (`kubernetes.memory.working_set`), pink link is memory request. Memory increase each day is visible (without release). Note that our data is increasing each day but not that much.\n\nCan you tell me if it's an expected behavior or if we are doing something wrong?\n\n** Cluster configuration**\nLimited by the max length of Discord messages, I put it in first response"
  },
  {
    "threadId": "1266174067858477196",
    "name": "Points custom re-raking",
    "messages": "Is there a feature to achieve something like elastic search's function score? (https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html)\n\nWhy? Say I want to achieve some custom re-ranking on server-side like this:\n\n```\nnew_score = a * qdrant_score + b * payload.some_feature + c * payload.other_feature + d* log(payload.another_feature)\n```\n\nwhere `a`, `b`, `c` and `d` are custom weights.\n\nNot calling from the client a huge sample to re-rank after.\n\nThis could be a custom recommend strategy."
  },
  {
    "threadId": "1266386695767592980",
    "name": "what is the unit of the ram_size returned by the telemetry API? It is not mentioned in the doc",
    "messages": "Could someone confirm the unit of ram_size returned by the telemetry API"
  },
  {
    "threadId": "1267021053423783987",
    "name": "help me pls. found problem when add data to qdrant cloud",
    "messages": "then run  python -m qdrant_demo.init_collection_startups\nhttps://github.com/qdrant/qdrant_demo"
  },
  {
    "threadId": "1266508882763583529",
    "name": "Updating metadata key",
    "messages": "Is it possible to update an individual key in the meta, or must the whole thing be replaced?\n\nAs you see in the image meta.title was added rather than that value being updated in the metadata.\n\nThis was using endpoint .../points/payload"
  },
  {
    "threadId": "1266333028658188360",
    "name": "How can i benchmark 2 vector databases.",
    "messages": "I want to Compare 2 vector data base .In my case i want to compare chromada and qdrant db in terms what it retrives for the particular query .Most of the frameworks suggest to pass it to the LLM and compare .But is their any way wihtout involving the llm we can check only the retrival part?"
  },
  {
    "threadId": "1266400890638696530",
    "name": "cluster deleted without warning",
    "messages": "Not sure why but I was testing and my cluster got deleted without a warning email"
  },
  {
    "threadId": "1265900958819553322",
    "name": "Extra inputs are not permitted",
    "messages": "i need help to setup Qdrant hybrid cloud can i please some help. I am very new to this and very little experience with deployment."
  },
  {
    "threadId": "1266335788912611390",
    "name": "reindexing consuming all cores",
    "messages": "Hi All,\nwhenevery reindexing of vectors is happening its consuming all cores and the machine is becoming unresponsive . Is there  a way to schedule this activty or restrict the program to use only 90% of cores .\ningenerally there are indexed and unindexed vectors in a collection, what is the difference as i am able to perform search in entire collection and get un indexed response also."
  },
  {
    "threadId": "1266262959710142535",
    "name": "what happenes to the data or record after it is written to the WAL .?",
    "messages": "Just wanted to know when the data is transferred from WAL to Mmap storage."
  },
  {
    "threadId": "1266087323570012321",
    "name": "How does qdrant choose which payload fields to use in creating additional links in the HNSW graph?",
    "messages": "Is there some way to specify this that I'm not seeing? Does it add fields for all payload fields? Or only ones that have payload indexes?\n\nThe documentation seems to suggest that payload indexes are *simply traditional indexes* and not really related to how HNSW works, so this has been confusing, except perhaps by the query planner that can choose to do an exact search after filtering with payload indexes if the cardinality is low enough?\n\nAdditionally, how should one think about setting `payload_m`? Should this be proportional to `m`? Or proportional to the number of payload fields? All I can find about `payload_m` in the docs are the comments around it in the code itself, but it isn't really enough to understand how to think about setting this.\n\nI tried digging deep in the documentation to answer these questions but so far can't find an answer. Thanks!"
  },
  {
    "threadId": "1265980060855177246",
    "name": "RAM use going down despite point count going up?",
    "messages": "One of our Qdrant instances is designed to sit and take points indefinitely. I noticed that the RAM use is not steadily increasing. On the contrary, I've seen it even go down recently. Of course, we have everything set to `on_disk`, but I still expected the RAM requirement to increase as the number of points increase.\n\nWell I'm not complaining - the DB is performant and operating as expected in all ways. No issues in the logs.\n\nIs it black magic?\n\nMaybe the question is: how does Qdrant decide RAM resource usage in scenarios like the one below?\n\nv1.9.0\n\nConfig:\n\n```json\n{\n  \"params\": {\n    \"vectors\": {\n      \"text-dense\": {\n        \"size\": 768,\n        \"distance\": \"Cosine\",\n        \"on_disk\": true\n      }\n    },\n    \"shard_number\": 1,\n    \"replication_factor\": 1,\n    \"write_consistency_factor\": 1,\n    \"on_disk_payload\": true,\n    \"sparse_vectors\": {\n      \"text-sparse\": {\n        \"index\": {\n          \"on_disk\": true\n        }\n      }\n    }\n  },\n  \"hnsw_config\": {\n    \"m\": 16,\n    \"ef_construct\": 100,\n    \"full_scan_threshold\": 10000,\n    \"max_indexing_threads\": 0,\n    \"on_disk\": true\n  },\n  \"optimizer_config\": {\n    \"deleted_threshold\": 0.2,\n    \"vacuum_min_vector_number\": 1000,\n    \"default_segment_number\": 0,\n    \"max_segment_size\": null,\n    \"memmap_threshold\": null,\n    \"indexing_threshold\": 20000,\n    \"flush_interval_sec\": 5,\n    \"max_optimization_threads\": 1\n  },\n  \"wal_config\": {\n    \"wal_capacity_mb\": 32,\n    \"wal_segments_ahead\": 0\n  },\n  \"quantization_config\": {\n    \"scalar\": {\n      \"type\": \"int8\",\n      \"always_ram\": false\n    }\n  }\n}\n```"
  },
  {
    "threadId": "1265278735406469150",
    "name": "Multitenant with mandatory tenant info in query",
    "messages": "Hi I am trying to implement multitenancy with a single collection and tenant segration using a key called \"tenant\" in the payload. But I fear there will be come of the application which could query the collection without sending the tenant identifier. Is there a way to enforce the search with a tenant information and reject the search if the tenant information is not provided?"
  },
  {
    "threadId": "1264915766650605612",
    "name": "Is there a way we can get the RAM used by a node in Qdrant through some API?",
    "messages": "Hi Guys - Is there a way we can get the memory available in a node. The Telemetry API only exposes the total memory of a node. While making a copy of a collection, this info can act as a vital check."
  },
  {
    "threadId": "1265921758242803713",
    "name": "qdrant_client.http.exceptions.UnexpectedResponse: Unexpected Response: 404 (Not Found)",
    "messages": "Hi, Is anyone help me this \n\nwhen i run the below code i am getting the below issue\n\"from langchain_community.document_loaders import TextLoader\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_qdrant import QdrantVectorStore\nfrom langchain_text_splitters import CharacterTextSplitter\nfrom langchain_qdrant import FastEmbedSparse, RetrievalMode\n\nloader = TextLoader(\"test_file.txt\")\ndocuments = loader.load()\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ndocs = text_splitter.split_documents(documents)\n\nembedding = OpenAIEmbeddings()\nsparse_embeddings = FastEmbedSparse(model_name=\"Qdrant/bm25\")\n\n# Qdrant server URL\nurl = \"http://localhost:6333\"\n\n# Create or access the Qdrant collection\nqdrant = QdrantVectorStore.from_documents(\n    docs,\n    embedding=embedding,\n    url=url,\n    sparse_embedding=sparse_embeddings,\n    collection_name=\"Hybrid_TEST\",\n    retrieval_mode=RetrievalMode.HYBRID,\n    force_recreate=True,\n)\n\n# Perform similarity search\nquery = \"what is ?\"\nsimilar_docs = qdrant.similarity_search_with_score(query)\n\nprint(similar_docs)\"\n\nQdrant is running on the url, it's valid."
  },
  {
    "threadId": "1264950318702137476",
    "name": "Question: gRPC API for Qdrant are under development and disabled by default in production builds",
    "messages": "Hi everyone,\n\nI've been going through the Qdrant documentation and noticed that the gRPC API is currently \"under development and disabled by default in production builds.\" We're interested in utilizing gRPC for our application and wanted to confirm whether the gRPC API is still under development or if itтАЩs stable enough for production purposes.\n\nCould anyone provide an update on the status of the gRPC API? Specifically, we're curious about:\n- Current stability and production readiness\n- Any known limitations or functionalities that are still incomplete\n\nThanks in advance for your assistance!"
  },
  {
    "threadId": "1265339884223205417",
    "name": "detect \"inspiration\" of generated content",
    "messages": "Hi, is there a way to detect which content was used to generate new content using vector databases?\n\nLike in the generation of a music, which songs were used, for videos and images?\n\nText I see that it can be done by ChatGPT, NotebookLM and others, displaying references.\n\nIf not, could it be done?"
  },
  {
    "threadId": "1265627787025453158",
    "name": "limit of sparse encoding",
    "messages": "RuntimeError: The size of tensor a (548) must match the size of tensor b (512) at non-singleton dimension 1\n\nthis is the error that happens when I put too long text in sparse text for encoding. My understanding is that encoders generates same dimension for whatever sentence it encode .. so why it fail ?"
  },
  {
    "threadId": "1265652746686566442",
    "name": "Need Help!",
    "messages": "I am working a project in which every user can get recommendation based on data it already provided. But how I can save users data in qdrant based on unique id?"
  },
  {
    "threadId": "1265766889401876632",
    "name": "Best way to export large collection",
    "messages": "Hi, I have a process where I'd like to export a collection of ~7M vectors. Scrolling through the points directly is taking a significant amount of time. Is it possible to extract data from a Snapshot into something more consumable or is there anything I can do to make the process faster?"
  },
  {
    "threadId": "1265765220089856061",
    "name": "Rule of thumb for points vs memory vs cpu per node in cluster",
    "messages": "Is there any rule of thumb for the amount of memory vs number of points per node in a qdrant cluster?  For example:\n100 million points per node and 32G of RAM?  24 cores?  \nAssuming storing points and payload on disk.\nThank you!"
  },
  {
    "threadId": "1265600421859295333",
    "name": "Error during running the Vector DB Benchmark on Qdrant",
    "messages": "Hi I would like to ask for a help about running the benchmark with my Qdrant DB. \n\nI am receiving an error during calling upsert function with enabled gRPC. I already tried using `grpcurl`  to list out collections and gRPC works. We are running on GKE and when I use port 6333 it works fine (two pods are communicating). I would appreciate any help. Thanks\n\n\n> File \"/usr/local/lib/python3.12/site-packages/qdrant_client/qdrant_remote.py\", line 1540, in upsert\n>     grpc_result = self.grpc_points.Upsert(\n>                   ^^^^^^^^^^^^^^^^^^^^^^^^\n> ...\n> grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n>     status = StatusCode.UNAVAILABLE\n>     details = \"failed to connect to all addresses; last error: UNAVAILABLE: ipv4:XXX:6334: Failed to connect to remote host: FD shutdown\"\n>     debug_error_string = \"UNKNOWN:Error received from peer  {created_time:\"X\", grpc_status:14, grpc_message:\"failed to connect to all addresses; last error: UNAVAILABLE: ipv4:XXX:6334: Failed to connect to remote host: FD shutdown\"}\""
  },
  {
    "threadId": "1263862030964883499",
    "name": "Is it possible `filter` for a substring?",
    "messages": "I was wondering if it is currently possible to filter an index with a substring query. \nE.g. if i have an `repository_path`field in my payload which contains the following value: `group\\subproject1\\subproject2\\project_name`\n\nIs it possible to include a filter in the search query, that would match on the start of the string, e.g. `group\\subproject1`, filtering out all entries that are not in `subproject1`.\n\nThe documentation only lists a `match` filter which will check if the strings are the same."
  },
  {
    "threadId": "1260146163256918037",
    "name": "Hybrid search group by functionality",
    "messages": "Hi all, \n\nFor hybrid search is it possible to group by id? I am using both embedding for search i.e. sparse and dense.\n\nExample (not working)\n```\nPOST /collections/chunk/points/query\n{\n  \"prefetch\": [\n    {\n      \"query\": { \n        \"indices\": [...],\n        \"values\": [...]\n      },\n      \"using\": \"text-sparse\",\n      \"limit\": 20\n    },\n    {\n      \"query\": [...],\n      \"using\": \"text-dense\",\n      \"limit\": 20\n    }\n  ],\n  \"query\": { \n    \"fusion\": \"rrf\"\n  },\n  \"limit\": 10,\n  \"group_by\": \"document_id\"\n  \"group_size\": 2\n}\n```\n\nThanks <:qdrant:1212491038581727252>,"
  },
  {
    "threadId": "1265402486743896094",
    "name": "Fastembed TextEmbedding Model causing errors",
    "messages": "I am using fastembed's TextEmbedding model to embed the word \"hello\". First time I did this, it worked fine. The second, I gota \"Could not download model from huggingface....sleeping for 3 seconds, 2 retries left\". When I did this again, this time it failed twice and it only worked after it said I had 1 retry left. Why is this happening?"
  },
  {
    "threadId": "1265154737062281278",
    "name": "Upserting not working: \"Unexpected Response: 400 (Bad Request)\"",
    "messages": "I am using upsert by record a list of models.PointStructs, but I get this error that says \"Raw response content, format error in JSON body: data did not match any variant of untagged enum VectorStruct, time 0.0, and this points directly to the upsert function in the terminal log. \n\nI checked whether or not I am successfully creating the PointStructs because I thought that's what the JSON format error was, but I indeed did it correctly. I also tried to switch it to models.Batch, but I got the exact same error. \n\nNot sure what's going on, help would be much appreciated."
  },
  {
    "threadId": "1259953642211901481",
    "name": "Is there 1) a way to use arbitrary strings as ids, and 2) query for the ids without the vectors",
    "messages": "use case: codebase embeddings, where i want to update an index based on a diff (delete old keys, add new keys), where keys are SHAs (of git tree nodes)\n\nit's possible to download an index, but i don't need the entire index, just the list of keys."
  },
  {
    "threadId": "1264886084257583205",
    "name": "is it best to have two collections for text and multimedia or a single vector space?",
    "messages": "Title is the question"
  },
  {
    "threadId": "1264854429396373585",
    "name": "bge-m3 & multimodal support",
    "messages": "I would like to use bge-m3 for its excellent search retrieval but was wondering if it's optimal for multimodal where there are images/videos or I need a separate model for that?"
  },
  {
    "threadId": "1265016167924240414",
    "name": "Is it possible to deploy a Qdrant vector database in a multi-node setup on AWS ECS service?",
    "messages": "Hi all!\n\nI couldn't find any details when I researched this question. If I missed something, I apologize, I am planning to use AWS EKS service for multi-node deployment, but I am wondering if this is possible on the AWS ECS service.\n\nThanks"
  },
  {
    "threadId": "1264355759182057632",
    "name": "Speed test",
    "messages": "I am using the free version of qdrant cloud, and haystack. \n\nI stored 3 documents, for this store:\n\ndocument_store = QdrantDocumentStore(\n    url=QDRANT_URL,\n    api_key=Secret.from_token(QDRANT_API_KEY),\n    use_sparse_embeddings=True,\n    return_embedding=True,\n    embedding_dim=768,\n)\n\nAnd this embedder:\n\nembedder = FastembedDocumentEmbedder(model=\"BAAI/bge-base-en-v1.5\")\n\n----\n\nAnd then I tried the script on the web site for haystack:\n\n# Start timing\nstart_time = time.time()\n\n# Initialize document store\ndocument_store = QdrantDocumentStore(\n    url=QDRANT_URL,\n    api_key=Secret.from_token(QDRANT_API_KEY),\n    use_sparse_embeddings=True\n)\n\n# Time after document store initialization\ndoc_store_time = time.time()\n\n# Initialize and warm up query embedder\nquery_embedder = FastembedTextEmbedder(model=\"BAAI/bge-base-en-v1.5\")\nquery_embedder.warm_up()\n\n# Time after query embedder warm-up\nembedder_warmup_time = time.time()\n\n# Initialize retriever\nretriever = QdrantEmbeddingRetriever(\n    document_store=document_store,  # The same document store as the one used for indexing\n    top_k=3,  # Number of documents to return\n)\n\n# Initialize pipeline\nsearch_pipeline = Pipeline()\nsearch_pipeline.add_component(\"query_embedder\", query_embedder)\nsearch_pipeline.add_component(\"retriever\", retriever)\nsearch_pipeline.connect(\"query_embedder.embedding\", \"retriever.query_embedding\")\n\n# Time after pipeline setup\npipeline_setup_time = time.time()\n\n# Print initialization times\nprint(f\"Document store initialization time: {doc_store_time - start_time:.2f} seconds\")\nprint(f\"Query embedder warm-up time: {embedder_warmup_time - doc_store_time:.2f} seconds\")\nprint(f\"Pipeline setup time: {pipeline_setup_time - embedder_warmup_time:.2f} seconds\")\nprint(f\"Total initialization time: {pipeline_setup_time - start_time:.2f} seconds\")\n\n\n---\n\nMy store only containts points for 3 documents. The search time takes 1 second+. Is this to be expected, or did I miss somethign crucial?"
  },
  {
    "threadId": "1265209795976237197",
    "name": "How many collections i can crearte ?",
    "messages": "Hey guys, I have a problem where I need to create many instances and each instance will be a collection, so I want to know whether qdrant allows creating multiple collections or not, or is it only fixed to a certain level? \nThanks for sp!"
  },
  {
    "threadId": "1264929191556812903",
    "name": "RAM Usage in HNSW",
    "messages": "Hey guys,\n\nI am running some benchmarks using different vector dbs and one of those is qdrant. So I am monitoring CPU, ram and disk usage. When I insert 1 million vectors in my db the search function is using 3 mb of ram, while for 95000 vectors in db it's using 700mb. What happens there? Any ideas?ЁЯдУ"
  },
  {
    "threadId": "1262479051474468974",
    "name": "Complex architecture",
    "messages": "Hello,\n\nI am trying to create a collection in Qdrant to store articles (both brief and detailed) and information about their authors. \nHere are the details of my problem:\n\n**Desired Structure:**\n\n- Each article should have `text_vector `and `image_vector `vectors, as well as payload fields like `article_title`, `article_length`, and `author_id`.\n- Each author should have payload fields like `follower_count`, `article_count`, and a `text_vector `to represent the author's embedding.\n\n**Specific Needs:**\n\n- I do not want each article to repeat the information about `follower_count `and `article_count`. \n- I want to be able to filter articles based on both article_length and follower_count.\n\n**Question:**\n\nWhat would be the best way to set up the collection and perform such a query?\n\nThank you in advance for your help!"
  },
  {
    "threadId": "1265212377155113040",
    "name": "While using QdrantVectorStore, I am getting this error.",
    "messages": "Please help me. Why am I geeting this error?"
  },
  {
    "threadId": "1263992754069831764",
    "name": "Ingestion Losing Points",
    "messages": "I'm trying to run an ingestion in rust using upsert_points and have a PointStruct vector of length 2847, but when I run the ingestion into an empty collection, only 2843 points are ingested. I tried chunking and I'm trying to get more visibility into the points vector I'm inserting, but wasn't sure if there was some behavior in qdrant I was missing or where I might be able to look to get additional visibility since upserting points seemed to be abstracted through the upsert_points function."
  },
  {
    "threadId": "1264969134580695151",
    "name": "Error while use .invoke in loaded from local langchain Qdrant vectorstore",
    "messages": "Hi guys, do you know what's wrong on my code? I loaded my persisted Qdrant vectorstore from local directory, but when I try to ask something on that, the `[Errno -3] Temporary failure in name resolution` appeared. Do I need a server to load it?\n\nThis is my code:\n```\nfrom qdrant_client import QdrantClient\nfrom langchain.vectorstores import Qdrant\n\nclient = QdrantClient(\"tmp/path\") \n\nqdrant = Qdrant(\n    client=client,\n    collection_name=\"my_documents\",\n    embeddings=OpenAIEmbeddings(),\n    metadata_payload_key=\"payload\"\n)\n\nqdrant.as_retriever().invoke(\"what do you know about anything?\")\n```"
  },
  {
    "threadId": "1264702706518589533",
    "name": "Error storing vectors with uuid",
    "messages": "I have this code here to fastembed add to the Qdrant collection\n```\n        client.add(\n            collection_name=\"my_documents\", \n            documents=docs,\n            metadata=metadata,\n            ids=[uuid.uuid5(uuid.NAMESPACE_URL, metadata[i]['url']) for i in range(len(metadata))],\n        )\n```\n\nand I'm having this error: \n\n```\n  Input should be a valid integer [type=int_type, input_value=UUID('d92ca0f1-a83d-58f5-a097-5341f7996024'), input_type=UUID]\n    For further information visit https://errors.pydantic.dev/2.8/v/int_type\nid.str\n  Input should be a valid string [type=string_type, input_value=UUID('d92ca0f1-a83d-58f5-a097-5341f7996024'), input_type=UUID]\n    For further information visit https://errors.pydantic.dev/2.8/v/string_type\n```\n\nI tried casting the uuid to a string as well and I get this error:\n```\nUnexpected Response: 400 (Bad Request)\nRaw response content:\nb'{\"status\":{\"error\":\"Format error in JSON body: value <generator object Tool._run.<locals>.<genexpr> at 0x332fdf790> is not a valid point ID, valid values are either an unsigned integer or a UU ...'\n```\n\nAppreciate any help anyone can provide!"
  },
  {
    "threadId": "1264809177251840082",
    "name": "\"Did you mean\" part",
    "messages": "Hello everyone\n\nI wanted to ask what is the best approach to implement the \"did you mean\" part ?\nWhich Qdrant API to use ?"
  },
  {
    "threadId": "1275546963948863593",
    "name": "Configuring hnsw_config on the vector vs on the collection",
    "messages": "Hi, whats the difference between configuring the hnsw_config for each vector and configuring it for the whole collection? \n\nAs in, would these two requests have the same effect on the single vector?\n\nPATCH /collections/my_collection\n{\n  \"vectors\": {\n    \"body_vector\": {\n      \"hnsw_config\": {\n        \"m\": 32,\n        \"ef_construct\": 200\n      }\n    }\n  }\n}\n\nVS\n\nPATCH /collections/my_collection\n{\n  \"hnsw_config\": {\n    \"m\": 32,\n    \"ef_construct\": 200\n  }\n}"
  },
  {
    "threadId": "1264128253673869312",
    "name": "Update a key in payload with a modified version of the existing values of that key",
    "messages": "Hello everyone, \nI am using qdrant in a langchain application, and each document stored in my qdrant vectordb has a `page_content` and a `metadata` field. The metadata field has all the information about the data in the `page_content` field ('source' and 'row_number' of the data).\n\nThe 'row_number' field values are stored as '3/2/0', for example. I want to make it 320. \nOn the documentation, I could find a method to update keys in the payload with a fixed string value, but what I want to do is, just remove the '/' in the existing value, which I could not make out how to do (change 4/5 to 45, etc).\n\nI had considered rendering the whole data from a specific 'source' value of the data, but that's a bit too expensive since there's a lot of data.\n\nAny solutions would be appreciated. \nThanks in advance!"
  },
  {
    "threadId": "1261327256312086650",
    "name": "Error: No space left on device",
    "messages": "I have deployed qdrant to Azure. All was working fine until  the number of (I guess) transactions has increased inside one collection. Now we are getting the following error: \"Service internal error: No space left on device: WAL buffer size exceeds available disk space\". The service is running on the VM inside a pod on Azure with 128GB disc space and is currently at 17% of usage. The problem also occurs even when trying to query points. It seems that some log files might be full.\nHow to figure out the  issue here and how to fix it?"
  },
  {
    "threadId": "1243090681778671720",
    "name": "How can I know the status of indexing?",
    "messages": "Hi, I am following the instructions at https://qdrant.tech/documentation/tutorials/bulk-upload/ for bulk uploading 110M points. Just adding the points took only 5-6 hours, but after setting  \"indexing_threshold\": 20000 afterwards, the status has now been yellow for over 12 hours. I also can't see the indexed_vectors_count go up at all. It's still stuck at 0.\n\nSo my question is is my index actually building or is something stuck? How can I know that it is progressing as it should, or if something is broken and I need to fix it? Thanks!"
  },
  {
    "threadId": "1264213291547234364",
    "name": "Retrieval/Metadata filtering too slow",
    "messages": "Cloud: https://e3058b00-9694-41a8-abcb-158c7b1c9879.us-east-1-0.aws.cloud.qdrant.io\n Config: 128GB DISK (58gb used). 32GB RAM (22gb used + 5gb cache). 4 vCPU (0.3 used)\nNodes: 4 million. Vectors: 3072 dense + SPLADE sparse.\nIssue: single item retrieval takes 70-80 seconds. (metadata alone takes 20+ seconds)"
  },
  {
    "threadId": "1264188326550966272",
    "name": "No filter match double?",
    "messages": "I'm wondering why there isn't a payload filter option for \"Match a Double\"? Do I have to use range object instead?\nHere (also visible in the attached screenshot) I see that only String or Integer is supported:\nhttps://qdrant.github.io/qdrant/redoc/index.html#tag/points/operation/search_points\n\nIf I try 'match' I get:\nFormat error in JSON body: data did not match any variant of untagged enum Condition at line 1 column 147\n\nIs it planned to implement it?\nI'm using currently image qdrant/qdrant  1.9.5"
  },
  {
    "threadId": "1186368479478157463",
    "name": "Service internal error: Can't reach one of the workers: channel closed",
    "messages": "I took a snapshot from one machine which has qdrant 1.5.0.\n\nI proceeded to restore the snapshot in another desktop with same qdrant version 1.5.0\n\nThe restore succeeds, however, adding, removing from the collection leads to the error:\n```\n2023-12-18T18:03:37.763487Z ERROR qdrant::tonic::logging: gRPC /qdrant.Points/Upsert unexpectedly failed with Internal error \"Service internal error: Can't reach one of the workers: channel closed\" 0.000177    \n2023-12-18T18:03:37.768017Z ERROR qdrant::tonic::logging: gRPC /qdrant.Points/Delete unexpectedly failed with Internal error \"Service internal error: Can't reach one of the workers: channel closed\" 0.000110\n2023-12-18T18:03:41.479684Z ERROR qdrant::tonic::logging: gRPC /qdrant.Points/Upsert unexpectedly failed with Internal error \"Service internal error: Can't reach one of the workers: channel closed\" 0.000180    \n2023-12-18T18:03:41.483466Z ERROR qdrant::tonic::logging: gRPC /qdrant.Points/Delete unexpectedly failed with Internal error \"Service internal error: Can't reach one of the workers: channel closed\" 0.000168 \n```"
  },
  {
    "threadId": "1263871151990374410",
    "name": "Quickstart is deprecated",
    "messages": "https://python-client.qdrant.tech/quickstart in step 6 uses .recreate_collection which sends \nDeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead."
  },
  {
    "threadId": "1263388507708067841",
    "name": "Helm chart for 1.10.1",
    "messages": "Hello, should I expect a new helm chart for 1.10.1 (it is currently for 1.10.0), or I need to manually update the `appVersion` value in `charts/qdrant/Chart.yaml`? I want to try out some bug fixes in this release."
  },
  {
    "threadId": "1263724940457279498",
    "name": "How can I resolve the following error when trying to upsert data to Qdrant in Flowise?",
    "messages": "I'm new to Flowise and Qdrant. Can anyone help me resolve the following error when trying to upsert data to Qdrant in Flowise?\nError: vectorsService.upsertVector - Error: Error: Error in `inputs`: value is not a valid dict"
  },
  {
    "threadId": "1263446036370948096",
    "name": "Access control for qdrant db",
    "messages": "How should I perform access control for my collection both programtically and through the dashboard also. Like for developers it should be read, and for admin all the access from create to delete ."
  },
  {
    "threadId": "1262442471892324474",
    "name": "Feature that supports search history",
    "messages": "In Qdrant is there a feature that allows developers to check the recently searched points?"
  },
  {
    "threadId": "1262653292945604688",
    "name": "How do I add new model",
    "messages": "I want to use another model instead of the sentence-transformers/all-MiniLM-L6-v model. How do I add a new model?"
  },
  {
    "threadId": "1263120863927209984",
    "name": "Quantization not reducing RAM Usage",
    "messages": "Hi, I am experiencing difficulties in tuning RAM usage. Currently we have 30M points with size 512 and QDrant consume about 128GB RAM\n\nWhile on https://cloud.qdrant.io/calculator it seems that RAM required about 86GB. Moreover, with scalar quantization it should be around 22GB\n\nI have turned on scalar quantization, but RAM usage still high\n\nAm I missing something?"
  },
  {
    "threadId": "1263392859130953770",
    "name": "filter exception or prioritized filters",
    "messages": "does qdrant have any way to handle if the \"must\" filter I am using resulted in  0 hits, something like changing the filter at this point ? \nWhat I want to do is if I did not find \"A\" in data, then and at this case only try to find \"B\" which is not equal to find \"A\" or \"B\""
  },
  {
    "threadId": "1263151034533019829",
    "name": "How to Efficiently Use Qdrant Vector Search for User-Specific Queries?",
    "messages": "Hello everyone,\n\nIтАЩm looking to implement vector search and could use some advice on the best approach for user-specific searches.\n\nFor instance, if IтАЩm developing a chatbot system, I want to search for relevant information within each userтАЩs conversation history to provide context. This seems manageable with a few hundred users, but what if there are hundreds of thousands of users? Using conventional methods might consume an excessive amount of memory and resources. Separating the data into different databases or tables also seems impractical due to the sheer number of tables it would generate.\n\nSo, is there any convenient way with Qdrant? Any suggestions or insights would be greatly appreciated!"
  },
  {
    "threadId": "1263070101603749908",
    "name": "Bulk upsert like Elasticsearch",
    "messages": "Hi there, is there a way to bulk-upload points to Qdrant? Currently we are chunking the points on our own. But I guess it would be more convenient if Qdrant offers and bulk upsert endpoint (like https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html) that better manages the load."
  },
  {
    "threadId": "1273955505009594410",
    "name": "Using datatype float16",
    "messages": "If we create a collection with a datatype of float16 for a vector, does qdrant automatically cast the upserted vectors to float16, even though they are upserted as float32?\n\nI was testing this on a new collection, and I saw this output when I tried to run `get_collection()`"
  },
  {
    "threadId": "1262299559703351296",
    "name": "Fuzzy payload search",
    "messages": "Greetings everyone\n\nIs there a way that I can search using only payload with fuzziness ?"
  },
  {
    "threadId": "1262515066666750086",
    "name": "Multivector comparators",
    "messages": "Hey all,\n\nI'm super excited to start using the new multivector support for animation and audio search where I have time-series of embeddings.\nWith that, I'm curious if there are plans to support new comparison functions. The problem I'm anticipating I'll face with `max_sim` is I'll lose the overall maximum similarity due to the sum across vectors in the query. For example I won't know if I got similarity scores of `[0.1, 0.1, 0.1, 0.9]` or `[0.3, 0.3, 0.3, 0.3]`, the first of which is a much better match than the second.\nCurrently I can solve this by querying each vector in my query matrix individually, but would I be missing out on some performance optimizations behind the scenes?\nI imagine an overall maximum comparator could be pretty useful in this scenario"
  },
  {
    "threadId": "1262511163728461864",
    "name": "Qdrant multivec migration",
    "messages": "With the 1.10.0 release we want to add support for bm25/bm42 sparse vectors using multivec. Is there a way to migrate our currently existing collections to add an extra `sparse_vector` in the config. Or will we need to scroll all previous points and add them to a new colllection manually?"
  },
  {
    "threadId": "1261195722041200650",
    "name": "multi tenancy with llamaindex",
    "messages": "How shall I handle multi tenancy in llamaindex application\n\nSetup\n\nI have qdrant setup as a persistent storage in my backend in my llamaindex app. I want to isolate users docs across my qdrant vector db, docstore and cache (in sync) to achieve user isolation and better retrieval performance.\n\nQuestions\n\n1. The isolation is within qdrant db but can we extend it to docstore, is there an intergration for it?\n2.Does payload filtering create a isolated partition in the  same collection, and how many such partitions can it support before we need to branch into seperate collection?\n3. \n```\nclient.create_payload_index(\n    collection_name=\"{tenant_data}\",\n    field_name=\"group_id\",\n    field_schema=models.PayloadSchemaType.KEYWORD,\n)\n```\nIs the field_name unique to each user?"
  },
  {
    "threadId": "1260992109104730227",
    "name": "Can't access dashboard",
    "messages": "Hello.\nI just updated my Hybrid cloud cluster to 1.10.1 and now I can't access the dashboard.\nI tried to restart"
  },
  {
    "threadId": "1262660422620942356",
    "name": "Qdrant cloud cpu throttling",
    "messages": "We are using the qdrant cluster with 2 nodes - 64GB and 16 vCore provided by aws marketplace. We used to get 6-7 rps load usually and we'd use about 2 vCore per node. We started getting around 24 rps and our cpu started throttling. It has become completely unusable. Unable to check the logs as well. Can anyone please help on this?"
  },
  {
    "threadId": "1261378368696352802",
    "name": "Creating an Qdrant vector db using my parquet files",
    "messages": "I have a parquet file and the file looks like this\n\nURL | Text | Embeddings\n\nI am making the embeddings using HF model but I'm pretty new how a databaes can be made from the parquet file I have on Qdrant. If someone could provide me with some demo code or template that'll be really helpful. Also I am trying to do cosine similarity for now. so calculate query embeddings with then calculate the cosine. \n\nand later I want to create an advanced fusion RAG (https://docs.llamaindex.ai/en/stable/examples/low_level/fusion_retriever/)"
  },
  {
    "threadId": "1262308508695789640",
    "name": "Async query failed: 'NoneType' object has no attribute 'search'",
    "messages": "Hi all,\nI am evaluating a RAG using Ragas.  I am using llamindex & Qdrant DB. \nfrom ragas.integrations.llama_index import evaluate\nfrom ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n    Settings.chunk_size = chunk_size\n    Settings.chunk_overlap = chunk_overlap\n    Settings.embed_model = embed_model\n    Settings.llm = llm\n\n    query_engine = vector_index.as_query_engine()\n\n    # Prepare the dataset\n    dataset = Dataset.from_dict(ds_dict)\n\n    # Define metrics\n    metrics = [\n        faithfulness,\n        answer_relevancy,\n        context_precision,\n        context_recall,\n    ]\n\n    # Evaluate using Ragas\n    start_time = time.time()\n    result = evaluate(\n        query_engine=query_engine,\n        metrics=metrics,\n        dataset=dataset,\n        llm=llm,\n        embeddings=embed_model,\n        raise_exceptions=False,\n    )\n\nThrowing the same exception with LangChain also."
  },
  {
    "threadId": "1260016068244668527",
    "name": "Point Deletion Memory",
    "messages": "I'd just like to confirm, should deleting points free up all the memory from a collection, or would I also need to delete the vectors as well? I still see vectors in the collection even after deleting all the points and wanted to make sure that I'm properly cleaning up the collection"
  },
  {
    "threadId": "1260275156815970449",
    "name": "Searching for many vectors",
    "messages": "I need a help in my use case. Simply, I get a sample from user of 3 text features [A,B,C], and each data point has three vector features as well. I want the most similar data point over the three features so I encode A,B and C and match each one separately on its corresponding feature of my data points, so A is matched upon all my data points by cos similarity in my collection which is 20000 point, and same is done with B and C. Now, for each data point I merge the three matching results and have a combination of similarities with the three features of user, so I get the most similar over the three features to the user.\n\nThis solution consumes very high time as I need to make search three times, each one with limit = 20000, then merging results of similarities for all the features. It takes about 12 secs for such process and no secret it will take much more if my data is much larger. \n\nSo my question is, Is there a way I can make match such that I get the limit of 50 most matching data points over the three features in more efficient way without need to retrieve all the data points for the three features and merge them on my own.\n\nPlease if some thing is not clear ask me to clarify it, thanks"
  },
  {
    "threadId": "1261332685981487175",
    "name": "How can I pass custom distance metrics in Qdrant, while index building ?",
    "messages": "I want to use angular distance as distance metrics for indexing  instead of cosine, as our other systems are  use angular metrics for similarity. Is there any way, I can pass my custom formula for similarity calculation ?"
  },
  {
    "threadId": "1261445547437588491",
    "name": "is possible to use qdrant langchain and openAi and maintain history?",
    "messages": "Hello, I'm trying to create an chat to talk about uploaded documents but I found no articles about that... But is really necessary for my final project to add history or context to the chats. If someone can help me, thanks"
  },
  {
    "threadId": "1261299927724855347",
    "name": "My qdrant cloud cluster is not starting",
    "messages": "My cloud cluster status is unkown for a while.    I recreated (deleted and made new) my cluster because i couldn't start my old one and it was stuck.  \nThe new one has  been spinning in unkown status for more than 20 minutes\nI can't see any logs as well (it says logs temporarily unavailable)."
  },
  {
    "threadId": "1247852564146556948",
    "name": "slow points upload using llama-index python",
    "messages": "hello, \n\nwe are trying to upload too many points to qdrant cloud ( url =  https://f193ad04-38ca-4340-b58a-d4cef7ffc6ad.us-east4-0.gcp.cloud.qdrant.io:6333) (number of points is around 200K point) but the process is very slow especially with hybrid search configuration. \n\nhere is the snippet of code i am trying : \n\nclient = QdrantClient(\n    url=QDRANT_URL,\n    api_key=QDRANT_API_KEY,\n    prefer_grpc=True,\n    https=True,\n    # timeout = 999999,\n)\n\nif not client.collection_exists(collection_name=COLLECTION_NAME):\n    client.create_collection(\n        collection_name=COLLECTION_NAME,\n        optimizers_config=models.OptimizersConfigDiff(indexing_threshold=0,),\n        hnsw_config=models.HnswConfigDiff(on_disk=True),\n        vectors_config={\n            \"text-dense\": models.VectorParams(\n                size=3072, \n                distance=models.Distance.COSINE,\n            )\n        },\n        sparse_vectors_config={\n            \"text-sparse\": models.SparseVectorParams(\n                index=models.SparseIndexParams()\n            )\n        },\n    )\n\nvector_store = QdrantVectorStore(\n    collection_name=COLLECTION_NAME,\n    client=client,\n    enable_hybrid=True,\n    batch_size=64,\n)\n\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\nindex =  VectorStoreIndex.from_documents(\n    documents=documents,\n    storage_context=storage_context,\n    vector_store=vector_store,\n    show_progress=True,\n)\n\nclient.update_collection(\n    collection_name=COLLECTION_NAME,\n    optimizer_config=models.OptimizersConfigDiff(indexing_threshold=20000),\n)"
  },
  {
    "threadId": "1261038666437103757",
    "name": "Restore Snapshot Upload Connection Closed",
    "messages": "Hello,\n\nWhen trying to restore a backup snapshot, I have this error in the browser console:\nThe upload process stop randomly between 60 and 75% \nI use paid hosting from cloud.qdrant.io\n\nCould you help me figure out what happens ?"
  },
  {
    "threadId": "1261245576465485896",
    "name": "Clarification on Qdrant Vector Store Behavior When Adding New Documents With Llamaindex",
    "messages": "I have a question regarding the behavior of the Qdrant vector store in Llamaindex when adding new documents to an existing vector store.\n\nScenario:\nI initially created a Qdrant vector store using 3 documents, and their embeddings were stored in the Qdrant cloud. Later, I added 2 more documents to the same directory. When I create the vector store again, I want to understand how it handles the existing and new documents.\n\nQuestion:\nWhen I recreate the vector store after adding the new documents, does it:\n- Recreate the vector store by reprocessing the original 3 documents along with the embeddings of the 2 newly added documents, and then update the cloud?\n- Only create embeddings for the 2 new documents and add them to the existing vector store in the cloud without reprocessing the original 3 documents?\n\nUnderstanding this behavior is crucial for optimizing our workflow and avoiding redundant processing."
  },
  {
    "threadId": "1261198236408877158",
    "name": "CRUD Operations on qdrant vector db in llamaindex",
    "messages": "What is the best practice to do crud operations to maintain consistency between docstore and qdrant vector db in llamaindex?"
  },
  {
    "threadId": "1260833527247933440",
    "name": "Hybrid Cloud cluster stuck in 'creating'",
    "messages": "Hi,\n\nI've been working to setup a Hybrid Cloud instance of Qdrant. I created the environment which is using EKS. I did all the steps to get this running:\n- Creating the EKS cluster\n- Setting up a bastion host to connect to the EKS cluster\n- Running all the kubectl and helm CLI commands\n\nMy environment says it's in a ready state.\n\nI went to then create the cluster from the Qdrant Hybrid Cloud interface and selected the bare minimum configuration.\nThis seemed to all work and I can see my cluster within the Hybrid Cloud environment however it's been a while and it's still 'creating'.\nNot really sure how long I need to wait for it to complete and I've been looking around the documentation to see if theres anything I can find. Would really appreciate some guidance.\n\nHappy to provide you with any info from kubectl if needed.\n\nThanks"
  },
  {
    "threadId": "1261043928686395498",
    "name": "HNSW & Group By",
    "messages": "Hey is there more information on how search with the \"group by\" endpoint works under the hood? I do understand HNSW, i'm trying to understand e.g. what happens when a vector in a keyword is really close but other instances in the same keyword are not. Is there some avg of the scores done within the keyword group to rank the top x groups etc. Are multiple indexes built for each keyword? \n\nThanks in advance!"
  },
  {
    "threadId": "1261020595404144781",
    "name": "MultiCollection Search",
    "messages": "Hi, I am using a local Qdrant instance with Docker.\n\nI have multiple collections, each containing many vectors. I want to perform cosine similarity searches with the  query across different collections simultaneously. I mean, I want to find the similarity of my query with the following vectors:\n\nIDs: id-1, id-2, id-3, id-4 in collection A\nIDs: id-5, id-6, id-7, id-8 in collection B\nIDs: id-x, id-y, id-n in collection Z\nAnd so on.\n\nI noticed the batch search feature, but I have to send `top_k` to all of them and sort again on my side it is acceptable for me but  Is there any other way to do it?\n\n\n\nAlternatively, would it be more efficient to merge all collections into a single collection (containing up to 1 billion vectors or more) and use an ID filter like the one below? Does this approach introduce any extra costs or are there any limits on the maximum number of vectors per collection?  Or anything I need to consider about this approach?\n\n```\n    models.Filter(\n        must=[\n            models.HasIdCondition(has_id=ids),\n        ],\n\n```"
  },
  {
    "threadId": "1212471333900914739",
    "name": "VacuumOptimizer threshold to rebuild index: Is this per shard or per segment?",
    "messages": "Hi, I have a question about VacuumOptimizer and how Qdrant rebuild the index. So the VacuumOptimizer will rebuild the index when the number of deleted records have exceeded certain threshold, i.e: 0.2. Is this 0.2 threshold number per shard or per segment?\n\nFor context, I want to implement a daily cleanup job to clean up some of the old index and don't want to trigger index rebuilding too many times."
  },
  {
    "threadId": "1260871669048803349",
    "name": "Anyway to batch update payload",
    "messages": "Hi there,\nIs there any way to batch update the payload only? We have millions of pairs of IDs and payloads (without vectors) and would like to update the payloads only.\nI've seen this documentation (https://qdrant.tech/documentation/concepts/points/#upload-points), but it mentions that updates have to include vectors."
  },
  {
    "threadId": "1260568969044164668",
    "name": "point count not increasing",
    "messages": "Point count is not increasing upon point insert. Point count is stagnant at 466"
  },
  {
    "threadId": "1259948459981668393",
    "name": "Databricks driver crashes using Qdrant connector 2.3",
    "messages": "Hello <@791281231987343370> , I hope you are doing well. Following our POCs early this year, we tried using Qdrant for additional POC and this time we tried to install the latest driver. We are using Databricks 14.3 LTS runtime.\nRegards from <@1192591674233339996>"
  },
  {
    "threadId": "1257766736367980544",
    "name": "Is Qdrant's filtering like an SQL WHERE clause or is it only filtering after semantic search",
    "messages": "I am currently using qdrant for vector storage in semantic search but I need to add capabilities to my system for users to be able to query only against certain fields, for example `Geography='California'` with no semantic searching involved. Does qdrant's filtering capability support this type of linear scanning of every record's metadata to see if the condition matches or is it only applying this condition after a semantic search? I check the docs here, https://qdrant.tech/documentation/concepts/filtering/, but they did not say. \nClarification is greatly appreciated!"
  },
  {
    "threadId": "1260666160354033815",
    "name": "How to effectively broadcast a fastembed model to a pandas_udf in spark?",
    "messages": "As far as I understand, fastembed internally tokenizes the input before running ort pipeline and then does some post-processing as well. Given all the files in the `cache_dir`, I came to realise that itтАЩs not possible to use `sparkContext` `add_files` to broadcast. The model initialisation it seems to me, has to happen inside the udf. But I am trying to find a way to share the cache_dir to all executor nodes as well with the optimistic expectation that then the python workers wonтАЩt crash and there wonтАЩt be multiple attempts to download the models either. Would appreciate some insight into how this could be achievable, I am considering cloud object storage. Any other ideas are truly welcome."
  },
  {
    "threadId": "1260640087100625008",
    "name": "Searching again and again on the same query yields different results.",
    "messages": "I have created a vector database with texts from 1500 documents. After searching a query multiple times, sometimes I got different results. How can I get rid of this?"
  },
  {
    "threadId": "1260648178269687889",
    "name": "Pass stopwords to BM25",
    "messages": "Hello, I want to use BM25 as my hybrid search method. My data are all in Bahasa Indonesia. Hence, I'd like to use Bahasa Indonesia stopwords. I can't seem to pass stopwords to BM25. Do I have to create my own HF repo and pass the stopwords there as `stopwords.txt` just like the [Qdrant/bm25](https://huggingface.co/Qdrant/bm25/blob/main/stopwords.txt) one?\n\nFor stemmer, I saw that there's `IndonesianStemmer` too but how do I use it? I am using `qdrant==1.10.0` btw."
  },
  {
    "threadId": "1260481665642270760",
    "name": "Upgrading from v1.9.4 to v1.9.5 in steps, Hit failure on upgrade from version 1.9.4 -> 1.9.5",
    "messages": "We're trying to upgrade our qdrant version from v1.8.4 to v1.10.0\n\nWe were able to upgrde from 1.8.4 to 1.9.0 to 1.9.1 to 1.9.3 to 1.9.4 pretty smoothly.  When we tried to upgrade from 1.9.4 to 1.9.5 we got the following error. \n\nFull logs linked below. Any help? Would this operation mean data is corrupted and we can't revert the node back to 1.9.4? We currently have 2x replication so this error is not pressing, just want to know what next steps would be"
  },
  {
    "threadId": "1260576918881828864",
    "name": "Get distinct Values of Payload",
    "messages": "I am using the payload feature to save meta information about my documents. Now I would like to obtain all distinct values for a categorical field. For example, given these documents, I would like to obtain a distinct list of cities in my dataset. Is there a way to do that without loading all documents?\n```\n[\n  { \"id\": 1, \"city\": \"London\", \"color\": \"green\" },\n  { \"id\": 2, \"city\": \"London\", \"color\": \"red\" },\n  { \"id\": 3, \"city\": \"London\", \"color\": \"blue\" },\n  { \"id\": 4, \"city\": \"Berlin\", \"color\": \"red\" },\n  { \"id\": 5, \"city\": \"Moscow\", \"color\": \"green\" },\n  { \"id\": 6, \"city\": \"Moscow\", \"color\": \"blue\" }\n]\n```"
  },
  {
    "threadId": "1260208727047405690",
    "name": "Caching",
    "messages": "I wanted to understand how Qdrant caches the indices in RAM? My usecase is we are having thousands of bots with embeddings similarity search"
  },
  {
    "threadId": "1260252617657946223",
    "name": "vector UNIT8 didn't work",
    "messages": "Using python async client, I've created new collection:\n        await client.create_collection(\n            collection_name=\"test_collection\",\n            vectors_config=models.VectorParams(size=50, \n                                               distance=models.Distance.COSINE, \n                                               datatype=models.Datatype.UINT8),\n        )\n\nI have float vector:\nvector = [2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 3.0, 3.0, 2.0, 0.0, 0.0, 2.0, 4.0, 0.0, 1.0, 1.0, 2.0, 0.0, 3.0, 0.0, 0.0, 2.0, 0.0, 3.0, 1.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n\nChanging to int8:\nvector_int8 = np.array(vector,dtype=np.int8).tolist()\n\nvector_int8= [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 3, 2, 0, 0, 2, 4, 0, 1, 1, 2, 0, 3, 0, 0, 2, 0, 3, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\nadding to qdrant and I see it in the collection, but when I try to search I does not mach, why? I get empty response: []\n\n    res = await client.search(\n            collection_name=\"test_collection\",\n            query_vector=vector_int8,\n            score_threshold=0.8,\n            limit=1\n        )\n\nIf I don't use \"datatype=models.Datatype.UINT8\" works perfect."
  },
  {
    "threadId": "1260330468726018119",
    "name": "huge empty collection size",
    "messages": "There is something wrong with my qdrant instance...\nA collection with 804 points weight 133mb, one with only one points and the same vector size weight 104mb, how is that possible?\n\nI'm using the snapshot size to get the size of it"
  },
  {
    "threadId": "1260204661668057118",
    "name": "Accidentally Inserted Zeros Vector",
    "messages": "Due some errors in embedding processing, some of my data are inserted with vector with all zero values. Those values make HNSW index accuracy worse. Is there any way to query those points when I use cosine index? \n\nUtilizing QDrant search API with a zero vector query yields no results."
  },
  {
    "threadId": "1260295071387881512",
    "name": "Speed up the Retrieving",
    "messages": "When I make search, I need to speed up retrieving speed as I put limit = 30,000. Is there any way for that ?"
  },
  {
    "threadId": "1260258788037099621",
    "name": "Does Qdrant now support bge-m3 in all dimensions?",
    "messages": "title was the question thanks"
  },
  {
    "threadId": "1252554767981346866",
    "name": "is qdrant support BM25?",
    "messages": "Hi, there , I have developed the Hybrid search using the langchain.So may i know whether the i can store the embeddings of the BM25 in the qdrant or not?"
  },
  {
    "threadId": "1259930793006989332",
    "name": "BM42 article",
    "messages": "I am trying to implement the code at the end of this article:  https://qdrant.tech/articles/bm42/, but it is not working.\n\nI am trying to use openai for the dense embeddings, so I changed the code like so:\n\n```\nmodel_bm42 = SparseTextEmbedding(\n        model_name=\"Qdrant/bm42-all-minilm-l6-v2-attentions\"\n    )\n    model_openai = OpenAIEmbeddings(api_key=settings.openai_api_key)\n\n    sparse_embedding = list(model_bm42.query_embed(input.question))[0]\n    dense_embedding = list(model_openai.embed_query(input.question))[0]\n\n    points = client.query_points(\n        collection_name=settings.qdrant_collection_name,\n        prefetch=[\n            models.Prefetch(query=sparse_embedding.as_object(), using=\"bm42\", limit=10),\n            models.Prefetch(query=dense_embedding, using=\"openai\", limit=10),\n        ],\n        query=models.FusionQuery(fusion=models.Fusion.RRF),  # <--- Combine the scores\n        limit=10,\n    )\n```\n\nBut I am getting the following exception (I include only the relevant part because I hit discord word limit):\n\n```\n2024-07-08 13:45:09     |   File \"/app/rag_server/app/chains/retriever.py\", line 85, in _split_input\n2024-07-08 13:45:09     |     models.Prefetch(query=sparse_embedding.as_object(), using=\"bm42\", limit=10),\n2024-07-08 13:45:09     |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2024-07-08 13:45:09     |   File \"pydantic/main.py\", line 341, in pydantic.main.BaseModel.__init__\n2024-07-08 13:45:09     | pydantic.error_wrappers.ValidationError: 24 validation errors for Prefetch\n```\n\nI am using the latest version of qdrant-client (1.10.0) and also the latest version of fastembed: 0.3.1.  What I am doing wrong?"
  },
  {
    "threadId": "1259897503483039906",
    "name": "Qdrant/bm42-all-minilm-l6-v2-attentions no more available on HF",
    "messages": "hello I cannot see the BM42 model on HF,  is there anything planned since the \"twitter drama\" ?"
  },
  {
    "threadId": "1258549050660225136",
    "name": "Filter based on lookup payloads",
    "messages": "Hi all, \n\nIf I have 2 collections, one saving vectors for every chunk of a document and the other saving the payloads for every document. \n\nHow can I group search with lookup the chunk collection and filter based on the payloads from the lookup collection i.e. the second collection? \n\nPlease provide simple request to test in the console or link to relevant documentation. Also, if possible the go client example.\n\nThanks <:qdrant:1212491038581727252>"
  },
  {
    "threadId": "1259747754192666675",
    "name": "Segment max size recommendation",
    "messages": "Hi\nI followed the instructions in:\nhttps://qdrant.tech/documentation/guides/optimize/\nto increase throughput, and set \"default_segment_number=2\".\nHowever it did not change the number of segments I had, so I figured I have to also increase \"max_segment_size\".\n\nIt worked, and now each shard has 2 segments, one of them is 27G and the other 156K.\nMy questions:\n1. Is 27G too big for a single segment? Is there any recommended limit?\n2. Is it on purpose that the segments differ by that much in size"
  },
  {
    "threadId": "1259672756220596305",
    "name": "How To Increase Replication Factor in Self-hosted Qdrant Cluster?",
    "messages": "I am currently operating a Qdrant cluster on EKS with four shards and no set replication (effectively replication factor = 1). I aim to enhance the system's availability by increasing the replication factor and would appreciate guidance on how to achieve this.\n\nCould you please confirm if the following approach would be appropriate?\n\n1. Scale up the physical nodes to double the current specifications.\n2. Increase the `replicaCount: 4` in `values.yml` from `4` to 8.\n3. Expect that pods(shards?peers?) without data will be added to the cluster.\n4. Use the new peer_id to copy existing shards to the newly added peers by `POST /collections/{collection_name}/cluster`\nI'm unsure about steps 2-4, so any corrections to these assumptions or additional steps would be extremely helpful."
  },
  {
    "threadId": "1258379428065575033",
    "name": "Use Lookup in Recommendation API",
    "messages": "Hello everything,\n\nI have to collections and I want to connect them using \"with_lookup\"\nbut it seems that is doesn't work with \"Recommend\" api. It doesn't bring the payload from the second collections. I tried the same Request in \"Search\" API and it worked\n\nBelow is my test request\n\nPOST collections/collection1/points/recommend \n{\n  \"limit\": 10,\n  \"positive\": [ \"here arrays of vectors\" vec1, vec2 ],\n  \"group_by\": \"document_id\",\n  \"limit\": 2,\n  \"group_size\": 2,\n  \"with_lookup\": {\n      \"collection\": \"collections2\"\n  }\n}\n\n\nthis is the result of above request\n{\n  \"result\": [\n    {\n      \"id\": \"0000-0000-0000-0000-000\",\n      \"version\": 6,\n      \"score\": 0.5859822,\n      \"payload\": null,\n      \"vector\": null\n    },\n    {\n      \"id\": \"0000-0000-0000-0000-000\",\n      \"version\": 7,\n      \"score\": 0.5772977,\n      \"payload\": null,\n      \"vector\": null\n    }\n  ],\n  \"status\": \"ok\",\n  \"time\": 0.001171059\n}"
  },
  {
    "threadId": "1258698024805339136",
    "name": "Hybrid search + recommend.",
    "messages": "I have hybrid collection - \"text-dense\", \"text-sparse\", \"image-dense\".\nI am want to create a hybrid query - text-dense + text-sparse --> get results\nfrom the results --> chose image vectors - e.g 1 positive 1 negetive --> create another search with the same text queries(dense+sparse) and pos/neg images vectors.\nI can find an option to do that with the recommend api with non existing vector in a collection unless I calculate the pos/neg vector by hand. <#1244504645519540304>"
  },
  {
    "threadId": "1269255535253917768",
    "name": "how to retrieve all points in a collection",
    "messages": "I am trying to retrieve all points in a collection, i get error when the next page offset is not None . I trued to write a function that takes that into account but still give error \" tuple object has no attribute points.\n\nThe function\n\n# Function to scroll through all points in the collection\ndef retrieve_all_points(client: QdrantClient,collection_name):\n    scroll_result = client.scroll(\n        collection_name=collection_name,\n        with_payload=True,\n        with_vectors=True,\n        scroll_filter=None,  )\n    \n    all_points = scroll_result[0]\n    next_page = scroll_result[1]\n    print(next_page)\n    while next_page is not None:\n        scroll_result = client.scroll(\n            collection_name=collection_name,\n            with_payload=True,\n            with_vectors=True,\n            scroll_filter=None, \n            offset=next_page\n        )\n        all_points.extend(scroll_result.points)\n        next_page = scroll_result[1]\n    return all_points"
  },
  {
    "threadId": "1259571545034985535",
    "name": "Regard qdrant Upsertion of sparse/dense",
    "messages": "\"optimizer_status\": {\n      \"error\": \"optimizations pending, awaiting update operation\"\n    }\nI am getting the above error for qdrant and the upsertion of the records are done in the following way\n\nI have inserted sparse and dence vectos like this\npoint = models.PointStruct(\n                    id=point_id,\n                    vector={\"content_vector\": embeddings,\n                            \"text\": models.SparseVector(indices = indice, \n                                                        values  = weights,\n                                                                    )},\n                    payload=payload_dict,\n                )"
  },
  {
    "threadId": "1257601658523877449",
    "name": "Question on BM42 performance on Quora dataset",
    "messages": "https://github.com/castorini/anserini/blob/5eb46b9f9bd563c34deca85a5c7417c068348972/docs/regressions/regressions-beir-v1.0.0-quora.flat.md\n\nAccording to anserini's experiment, using BM25 can get NDCG@10 at 78.8%, which is much better than the reported number Precision@10 at 45% in https://qdrant.tech/articles/bm42/. Why the BM25 performance in much worse than anserini using Elasticsearch?"
  },
  {
    "threadId": "1258903191886757969",
    "name": "Storage deadlock when trying to recover collection",
    "messages": "We are using 2x replication on a 30m collection with qdrant version 1.9.4 (From the logs Collection 1024_vectors) is the 30m sized collection. One of the nodes died during ingest at the 30 million mark and we've been waiting all day for it to come back online. After 30 minutes of loading we get the following message \n\n```\ncollection::collection_manager::holders::segment_holder: Trying to read-lock all collection segments is taking a long time. This could be a deadlock and may block new updates.\n```\nWe are not adding any new data to qdrant while its trying to recover so  i'm not sure why/how it would be locked this badly. Can we restart all the nodes and free the lock?"
  },
  {
    "threadId": "1258825319629066322",
    "name": "go authentication help",
    "messages": "hi everyone, could i get some guidance with a quickstart connect to my cluster in the managed qdrant service from my go web service? unfortunately though the managed solution docs reference a way to do this from the go docs, the code snippets aren't there. does anyone have any information on how i could use my cluster URL + API key to connect to my client? thanks!"
  },
  {
    "threadId": "1184207887615131728",
    "name": "Content type error with Qdrant Cloud, grpc and go",
    "messages": "Hi, Im following the authentication example for the go lib but I keep getting this error\nThe example Im following\n\n\"rpc error: code = PermissionDenied desc = unexpected HTTP status code received from server: 403 (Forbidden); transport: received unexpected content-type \"application/json; charset=utf-8\"\n\nis 'application/json; charset=utf-8' really not correct or is something else going on here?\n\nThis is the example im following:\nhttps://github.com/qdrant/go-client/blob/master/examples/authentication/main.go\n\nTo replicate, just insert your address and api key"
  },
  {
    "threadId": "1258427402938355882",
    "name": "How to troubleshoot indexed_vectors_count 0 after hours of high cpu usage?",
    "messages": "Basically title. After a few hours of high cpu usage by qdrant (over 2000%, no queries performed in this timeframe, only some inserts), indexed_vectors_count is still 0 while points_count is over 200m and status is yellow"
  },
  {
    "threadId": "1249563677582037002",
    "name": "The pod is not running or error",
    "messages": "I install qdrant cluster using helm in local cluster with 4 woker nodes. When I set replicas = 4 and there is only 1 pod is running, others are error. Sometimes 2 pods running, 2 pods error or 3 pods running, 1 pod error."
  },
  {
    "threadId": "1252845034315845714",
    "name": "Filtering with HNSW",
    "messages": "Hello, I'm interested in how HNSW indexing works and I found Qdrant. In one of posts from Qdrant, I checked that Qdrant uses different filtering which is not pre or post filtering by using payload index. I am not sure whether is it possible ore not, but is there any code in the open source that I can take a look to understand how filtering is working?"
  },
  {
    "threadId": "1256142847590273024",
    "name": "hybrid search",
    "messages": "I have qdrant collection which have millions of records I want to perform hybrid search can anyone please guide me how to do that?"
  },
  {
    "threadId": "1258536749865500712",
    "name": "Issue with Memory Usage During Qdrant Helm Version Upgrade on EKS Using Terraform",
    "messages": "I am currently working on upgrading our Qdrant Helm deployment on EKS from version 0.9.4 to 0.10.0 using Terraform. Before the upgrade, the memory usage was as follows.\n```ts\nNAME                CPU(cores)   MEMORY(bytes)\nqdrant-mlab-prd-0   4m           13029Mi\nqdrant-mlab-prd-1   4m           13220Mi\nqdrant-mlab-prd-2   7m           16831Mi\nqdrant-mlab-prd-3   4m           13259Mi\n```\n\nHowever, while monitoring memory usage during the upgrade process, I noticed a significant increase in memory consumption, reaching 100%. In a hurry, I attempted to revert to the previous version using Terraform, but the memory usage again escalated to 100%.\n\n```ts\nNAME                CPU(cores)   MEMORY(bytes)\nqdrant-mlab-prd-0   4m           13034Mi\nqdrant-mlab-prd-1   5m           13223Mi\nqdrant-mlab-prd-2   7m           16834Mi\nqdrant-mlab-prd-3   288m         28261Mi\n```\n\nAside from vertically scaling the cluster, are there any other solutions to this issue? Also, could this be a bug?\n\nThank you for your assistance."
  },
  {
    "threadId": "1258425092891742229",
    "name": "Does anyone know a public API platform supporting sparse embeddings?",
    "messages": "Something like the existing pay-as-you-go dense embeddings, ready to use."
  },
  {
    "threadId": "1258340030062002218",
    "name": "How to integrate the BM25 in the qdrant",
    "messages": "while implementing i am getting the below issues may i know why it's getting .\n\n\"ValueError: Model Qdrant/all_miniLM_L6_v2_with_attentions is not supported in SparseTextEmbedding.\""
  },
  {
    "threadId": "1177615834059903016",
    "name": "how to sort the response of my query according to another fileds",
    "messages": "i have built a semantic search engine to search about a database which contain data about freelancers \ni search about the \"about\" section in this dataset and return the most similiar freelancers but i want to add the following conditions\n1. freelancer who has a rating average appears first according to their values in range between 0-5\n2. last seen come first\n3. sold services,customers count, sort them according to one whic has the max value"
  },
  {
    "threadId": "1257985106560417862",
    "name": "How to use lookup in go",
    "messages": "Hi all,\n\nI have 2 collections, one is files which has the vectors of a document chunked accordingly, and the second is metadata which saves the metadata for each document. \n\nI have this code in go, but the lookup part is not working for some reason.\n```\n    results, err := pc.SearchGroups(ctx, &pb.SearchPointGroups{\n        CollectionName: \"files\",\n        GroupBy:        \"path\",\n        Vector:         vector,\n        Limit:          1000,\n        ScoreThreshold: &score,\n        GroupSize:      1,\n        WithPayload: &pb.WithPayloadSelector{\n            SelectorOptions: &pb.WithPayloadSelector_Enable{\n                Enable: true,\n            },\n        },\n        WithLookup: &pb.WithLookup{\n            Collection: \"metadata\",\n            WithPayload: &pb.WithPayloadSelector{\n                SelectorOptions: &pb.WithPayloadSelector_Include{\n                    Include: &pb.PayloadIncludeSelector{\n                        Fields: []string{\"*\"},\n                    },\n                },\n            },\n            WithVectors: nil,\n        },\n    })\n```\nThanks ЁЯМ╗"
  },
  {
    "threadId": "1257250870157115402",
    "name": "QPS not improved after horizontal scale",
    "messages": "Hi,\nI'm new with Qdrant and just started doing some benchmarks.\n\nI have a 40M 512dim collection, and with 3 nodes (shards=6, replication=2) I reached around 600QPS (M=64, ef_construction=256, ef=16)\n\nI tried to improve it by adding 3 more nodes and changing to shards=12 (and rebuilding the collection), But the results did not change, still around 600QPS.\n\nAny suggestions? Maybe I'm doing something wrong?\nThanks!"
  },
  {
    "threadId": "1258008059079557194",
    "name": "How to filter and delete points which doesnt have payload filed",
    "messages": "I have database in qdrant which has some points which doesnt have particular payload, and hence it is giving me error at search time, how can I delete the points now?\n\n[ScoredPoint(id=7933, version=371, score=0.5498272, payload={'issuenumber': 313981, 'itproduct': 'NOT_USED', 'itqueue': 'Central Systems', 'service': 'Servers', 'subservice': 'Servers'}, vector=None, shard_key=None), ScoredPoint(id=681006, version=1178, score=0.5422493, payload={'issuenumber': 681006, 'itqueue': 'Central Systems', 'service': 'File Services (ITALL-403193)', 'subservice': 'Network Shared Folders (ITALL-421963)', 'summary': 'test summary'}, vector=None, shard_key=None), ScoredPoint(id=24484, version=1148, score=0.49772912, payload={'issuenumber': 332404, 'itproduct': 'NOT_USED', 'itqueue': 'Service Desk', 'service': 'Standard Hardware and Software', 'subservice': 'Standard Hardware and Software'}, vector=None, shard_key=None), ScoredPoint(id=369, version=17, score=0.40357256, payload={'issuenumber': 305866, 'itproduct': 'NOT_USED', 'itqueue': 'Service Desk', 'service': 'Standard Hardware and Software', 'subservice': 'Standard Hardware and Software'}, vector=None, shard_key=None), ScoredPoint(id=10421, version=488, score=0.05678095, payload={'issuenumber': 316679, 'itproduct': 'NOT_USED', 'itqueue': 'Service Desk', 'service': 'Standard Hardware and Software', 'subservice': 'Standard Hardware and Software'}, vector=None, shard_key=None)]\n\nfor referance here some ponitns doesnt have itproduct, i want to delete those points, or add itproduct filed there?"
  },
  {
    "threadId": "1257938672112767056",
    "name": "Qdrant SAAS OFFERING",
    "messages": "Hi,\n\nWe planning to use qdrant as a SAAS service provided by AWS ( https://aws.amazon.com/marketplace/pp/prodview-rtphb42tydtzg ). There are some quetions that we would like to be answered before we move forward.\n  a) Is Qdrant single or multi-tenant\n  b) What about dedicated versus shared instances?\n  c) What does Qdrant do to protect customer information and ensure confidentiality. Do you have a privacy policy documented that we can reference\n\nIf someone knows the answers to these questions and help us out with this."
  },
  {
    "threadId": "1257922336087216221",
    "name": "indexed_vectors_count and num points",
    "messages": "Hi, when i restored cluster by upload snapshot, i got this.\nHow to get the vector indexed, this is the old cluster without restored from snapshot.\nOne more, the number of vectors in restored cluster and the old one is not the same.\nCan you help me with this?\nThanks"
  },
  {
    "threadId": "1257890619922186383",
    "name": "K8s deployment",
    "messages": "Hi,\nRight now i have 3-nodes cluster running without load balancer. For solving this, i want to setup k8s environment on that cluster.\nCan i move data from old qdrant cluster to new one built with k8s? Are there any inconvenient things will happen?\ntks a lot"
  },
  {
    "threadId": "1199657579631104010",
    "name": "Is it possible to add a named vector in an existing collection containing some named vectors already",
    "messages": "I have defined a collection with 2 named vectors and the collection is all set up with the some points. I want to add a new named vector to store embeddings of new model keeping the previous named vectors and thier values intact. Is there any way to add named vector and upsert points with id and new named vectors embeddings?"
  },
  {
    "threadId": "1257730349535072306",
    "name": "Metadata Separation in Single Document Chunks",
    "messages": "hey all! \n\nIm building a document search engine system using qdrant. The idea is that I am going through many documents, chunking the document content, embedding them and then inserting them into Qdrant. In am also saving the metadata for the documents as a point payload in Qdrant. \n\nSince I have many chunks (and consequently vectors) per document, I am currently repeatedly saving the same metadata fields into all my Qdrant points related to the same document. I can already see that as my project scales up I will be running into performance and usage issues since there will be a lot of repeated metadata. \n\nTo avoid repeating metadata, these are the approaches that I have thought of so far:\n1) Create a separate Qdrant collection to store metadata only and then reference the vectors collection with some ID\n\n2) Store metadata externally (e.g. in MongoDB)\n\n3) Keep doing what I assume is the naive approach of saving all metadata into all my Qdrant points (which is what im currently doing)\n\nIf anyone could provide some direction on this or has some insight please do let me know!!\n\nThanks <:hugging_fire:1212506142580936766>"
  },
  {
    "threadId": "1225339095929196589",
    "name": "Error: Service internal error: Waiting for consensus operation commit failed. Timeout set at: 10 sec",
    "messages": "Hello, I am having this issue when creating new collections. I only have the single pod for my EKS replicaset for qdrant.\n\nIt happened when I request the collection creation at once; if it sleeps 10 seconds sequentially then no error happens."
  },
  {
    "threadId": "1257653762470707220",
    "name": "Is multi vector a viable alternative to embedding averaging?",
    "messages": "So excited about 1.10! I noticed multi vector support as documented here. https://qdrant.tech/documentation/concepts/vectors/#multivectors\n\nWe have some long documents that are bigger than our max context on the embedding models that we use. Today we are chunking, generating multiple embeddings for chunks larger than context, then averaging those vectors.\n\nThe documentation didn't call out this use case but would it be appropriate to use it for those chunks that happen to have multiple vectors instead of doing the averaging? \n\nThe reason a chunk might be bigger than the embedding size is the chunking strategy we use is not tied to token count it's tied to the structure of the document."
  },
  {
    "threadId": "1257426273286750299",
    "name": "HNSW on_disk without mmap?",
    "messages": "Is it possible to persist HNSW index to disk wihtout mmaping it? \n\nBaiscally my issue is that if the HNSW index is mmaped, it will get flushed out of the OS page cache by other process' I/O if qdrant is not constantly getting queries that use it.\n\nHowever, if I set on_disk=false then server startup takes more than 1h, I assume because it is rebuilding the hnsw from scratch every time.\n\nMy dataset is ~200 million vectors of 70 dimensions each, footprint on disk is 85GB."
  },
  {
    "threadId": "1245670369306607720",
    "name": "Backup for Azure instance",
    "messages": "Hello,\n\nFrom what I've seen you cannot set automatic backup on Azure instance. Do you guys have any code snippet to save the all Qdrant DB I have and save the snapshot / content into a bucket or something to have backup files on my side incase something happen ? \n\nUsing Python. \n\nTy  !"
  },
  {
    "threadId": "1257432680459534407",
    "name": "Commercial friendly sparse vectors",
    "messages": "We are looking at using some of the newer sparse vector support in qdrant and noticed that SPLADE licensed to not allow commercial usage. Are there other friendly models out there that can plug in?\n\nIve also heard you can convert bm25 results into a sparse vector but not finding good examples of that."
  },
  {
    "threadId": "1257591042551578674",
    "name": "select a random id from a collection",
    "messages": "I want to implement the scroll api, but each time, with a different `offset`. So, I need to be able to select a random id, and put it as the offset. Is there any way to do this?\nI only need to get a random id, and give it to the `offset` api."
  },
  {
    "threadId": "1257380189718052995",
    "name": "exact phrase match in fulltext indexed column",
    "messages": "Hello,\nIs it possible to match a phrase in it's entirety by using a fulltext index?\n\nRight now I have the following setup ```\"text\": {\n        \"data_type\": \"text\",\n        \"params\": {\n          \"type\": \"text\",\n          \"tokenizer\": \"word\",\n          \"min_token_len\": 2,\n          \"max_token_len\": 20,\n          \"lowercase\": true\n        },\n        \"points\": 38915641\n      },```\n\nThe documents are rather large (thousands of words). I want to force a search for \"foo bar baz\" where the entire phrase is matched inside the text and not [\"foo\",\"bar\",\"baz\"]. \n\nThanks!"
  },
  {
    "threadId": "1222733547207655497",
    "name": "search_batch & group_by incompatible?",
    "messages": "I'd like to confirm if the search_batch method is incompatible with search groups. From what I can tell in the python client, it only accepts SearchRequest, not SearchGroupsRequest. Documentation says it will take any type of search request, but this seems to only be true of SearchRequest type?"
  },
  {
    "threadId": "1256243106668351508",
    "name": "Slow metadata modification after running payload index",
    "messages": "Hello, \nI was having issue with slow filter so I ran this code :\n\nqdrant_client.create_payload_index(\n    collection_name=\"Collection\",\n    field_name=\"RULES\",\n    field_schema=\"keyword\",\n)\n\nNow the filter is fast, however whenever I modify \"RULES\",  like adding or removing elements from them it takes about 9/10 secondes each time. I've got about 250.000 vectors.\n\nIs there a way to make this a bit faster ? \n\nThanks in advance !"
  },
  {
    "threadId": "1257062208488083567",
    "name": "Difference in benefits of creating new collection vs providing filter on payload",
    "messages": "I'm trying to understand (maybe I missed this in the documentation), but what could be the justification for creating new collection vs adding payload which will be used to filter.\nLet say I have big (over 500k records) DB of books. Those books can be either in purchase or rent category. Other than that each book has similar set of fields - genre, author, title etc.\n**What would be better in such case** - creating two collections: `books_rent` and `books_sale` and populate each collection with proper subset of books, or create one combined collection `books` and use payload to store info whether book is for rent or sale?\n\n**User will never be able to search in both subsets simultaneously.** What I mean by that is if user is performing search they are required to specify whether they are doing search for book to rent or buy."
  },
  {
    "threadId": "1256360052969963581",
    "name": "support for aggregations on metadata?",
    "messages": "Given a collection with many points, where each point has metadata that indicates where it came from, is it possible to return a list of unique values for that metadata?\n\nExample: 3 books with 100s of points each, each point with the book ID as metadata. I can use must or should to restrict to points with specific field values on searches.\n\nWhat I would like to do is be able to query the collection to return all unique values on that metadata, so I can know if some book ID is indexed or not. And to get a list of books indexed.\n\nIf I were using Elastic, I would use aggs / Aggregations. https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-terms-aggregation.html"
  },
  {
    "threadId": "1255890100752224286",
    "name": "High Load and Slow Response Issues with Qdrant on Docker Desktop for Mac",
    "messages": "Hello,\n\nI am facing two significant issues with Qdrant, and I could really use some help.\n\nAfter restarting the Qdrant container on Docker Desktop for Mac, there is an extremely high load for about an hour, during which it seems like the Collections are being re-indexed. During this time, Qdrant becomes unresponsive to requests, severely impacting our development process. This happens every time we restart the container.\nFollowing the high-load period, the response time for nearest neighbor search requests becomes impractically slow, sometimes taking up to 30 seconds per request.\nOur hardware setup is a MacBook Pro 2019 (Core i9 / 8-core 16 Threads), with 32GB of RAM and a 1TB SSD. We have allocated 24GB of RAM to Docker Desktop for Mac.\n\nWe are using Qdrant version 1.9.7, and we have 20 collections registered. Each collection has about 1.5 million points, and the snapshot size is approximately 35-37GB per collection.\nHere is the result of running collection info:\n\n```\nnext post.\n```\n\nAnd our ~/config/config.yaml file is as follows:\n\n```\nnext post.\n```\n  \n  Interestingly, on another MacBook Pro with M2 Max and 32GB of RAM, using the same Qdrant version and only 8 collections, the nearest neighbor search response is very fast, even after thousands of consecutive requests. (Due to storage constraints, we cannot register more collections on this machine.)\n\nCan these two issues be resolved by adjusting any parameters in the config.yaml file?\nDuring the high-load period, there are no outputs in the console. Is there a way to find out what Qdrant is doing during this time?\nGiven the large volume of collections and points, could this be a hardware limitation? (We plan to upgrade to higher-spec hardware in a few months, but we need to address the current issues in the meantime.)\nAre there any other actions we can take to potentially resolve these issues?\nThank you for your assistance."
  },
  {
    "threadId": "1254089411893788675",
    "name": "Any optimizations for two vectors per point type collection? (text+image)",
    "messages": "Hello,\n\nI've been using Qdrant Cloud for a few months now, but I'm starting to hit the limits of my current configuration.\n\nBefore considering scaling up the architecture, are there any optimizations I can make to my collections in terms of RAM usage and execution speed?\n\nIn these collections, each point has two vectors:\n> one for text, using OpenAI's text-embedding-3-small with a dimension of 1536\n> one for image with a dimension of 512\n\nWhat would you recommend?\nThank you for your help"
  },
  {
    "threadId": "1256023080615018497",
    "name": "Recreating collections stuck",
    "messages": "hello, I am trying to recreate collections but it stucks forever. The meaning `stuck` is not only recreating collection doesn't work, but also other api calls such as loading collections gets timeout. I am using v1.9.0. If there's someone can help me, I will comment on the details."
  },
  {
    "threadId": "1178905971712606258",
    "name": "About init_from",
    "messages": "Can we use init_from to collect data from a populated collection into an existing populated collection??"
  },
  {
    "threadId": "1162292753779134505",
    "name": "How to enable CORS on Qdrant Cloud",
    "messages": "I tried  to use qdrant-js and fetch api  call but returning 403 forbidden error in React js but the same URL working in postman. I found that because the API doesn't support CORS, and how to enable it in Qdrant Cloud. Thank you."
  },
  {
    "threadId": "1255925567207768174",
    "name": "German langauge comes while in querying in english",
    "messages": "I have stored some of the german and french language documents along with english documents. Surprisingly, even if the query is in english I am getting some responses from the german and french. Any suggestion?\n\n<#1149327864936808529>"
  },
  {
    "threadId": "1252881845008334939",
    "name": "Get documents uploaded to collection",
    "messages": "We are building a RAG app where a user can add/remove documents. How can we get the documents added to a collection so we can delete via a UI.\n\nThere should just be a way to do something like:\n\nrag.listDocuments(colletionName)\nand get the names and Ids of the documents, that way we can set up a delete function to go with it."
  },
  {
    "threadId": "1255667971511287883",
    "name": "Insert timeouts",
    "messages": "Hi ЁЯСЛ \n\n- We started getting timeout errors yesterday though nothing has changed in our implementation and the load looks ok, and the logs are clean. See the screenshots attached.\n- We have a cloud instance on qdrant cloud. \n- We have two collections with approximately 650k vectors\n- The timeouts happen solely on insert.\n- timeout is set to 300s which should be plenty"
  },
  {
    "threadId": "1255551529423863808",
    "name": "how to cite Qdrant ?",
    "messages": "<#1149327864936808529>  Hi, I'm using Qdrant for one of our projects, and I'm currently wrinting about it. I wonder to know how to cite Qdrant, especially who are the authors of this great database ?"
  },
  {
    "threadId": "1252861702479941682",
    "name": "Does quantization help in saving RAM.?",
    "messages": "Can quantization be used to save RAM.?"
  },
  {
    "threadId": "1255028192524242976",
    "name": "Iam facing this. Can someone help me as soon as possible",
    "messages": "While creating index using llama index, I am facing this issue. Please some help me as soon as possible"
  },
  {
    "threadId": "1253974837484064870",
    "name": "Highlight answer in pageContent (Keyphrase Highlighting)",
    "messages": "Any suggestion how to highlight the related answer in the pageContent metadata?"
  },
  {
    "threadId": "1254935000974098442",
    "name": "Upgrade and snapshot restore from 1.7.2 to 1.9.6",
    "messages": "Hi, we have a 3-node cluster running at version 1.7.2.  Is it possible to use a snapshot file from the cluster and restore to a new 1.9.6  3-node cluster?\n\nAnother question, is there a process I can follow to do a live migration from 1.7.2 to 1.9.6"
  },
  {
    "threadId": "1254778334437707786",
    "name": "shuttle-qdrant-semantic-caching: This model does not support specifying dimensions",
    "messages": "When following the tutorial https://www.shuttle.rs/blog/2024/05/30/semantic-caching-qdrant-rust, I get an error in `RagSystem::embed_and_upsert_csv_file`\n```\nError: Some(\"invalid_request_error\"): This model does not support specifying dimensions.\n```\nIt's probably easy to fix, and I'd like you to be aware of it."
  },
  {
    "threadId": "1254823493103783936",
    "name": "collection in yellow after creating index",
    "messages": "We have a collection over over  1M points, we created an index over on int field and the collection have been in yellow for over few hours now with no visibility into what is happening. The CPU usage is at requested limit though. is there are way we can look at the progress or estimate when this would be done?"
  },
  {
    "threadId": "1254367983761883177",
    "name": "Can someone help me for this",
    "messages": "I am facing the issue while running query method."
  },
  {
    "threadId": "1252706490251481279",
    "name": "What is additional Pricing for Hybrid Cloud for qdrant and scalability features?",
    "messages": "I'm interested in knowing the price for Hybrid Cloud in Google Cloud. \n\nThis is my setup: \nCPUs: (8 vCPUs x 5 pods)\nRAM: (64 GB x 5 pods)\nStorage: (800 GB x 5 pods)\n\nThe open source product does not include automatic backups, does the hybrid cloud has support for that similarly to how the managed cloud works? Do horizontal & vertical scaling behave differently as to how a self-managed instance work?"
  },
  {
    "threadId": "1254044378914099210",
    "name": "Reconfigure/recreate quantized vectors",
    "messages": "What does Qdrant do when I use quantize vectors and I insert new vectors that have scalar values outside the existing max quantile? Does it have to recompute all existing quantized vectors?\n\nI suppose it uses the statistics from the existing vectors. But I define the quanization on on the collection before any vectors are inserted. It's not clear what's happening.\n\nHow do I reconfigure/recreate the quantization statistics for a collection after I've inserted vectors?\n\nI can't find anything about this in the docs. I've checked https://qdrant.tech/documentation/guides/quantization/"
  },
  {
    "threadId": "1254040383831281795",
    "name": "Qdrant recommendation, for multiple products that are unrelated to each other",
    "messages": "I am using Qdrant rec api, but for the positive ids, i am sending a list of 10 or maybe more ids of the products that the user have been interacted recently. these products may be completely different to each other, one being for example a watch, while the other one is some t-shirt. So, regarding Qdrant docs, in average_vector or best_score strategy, in both of them, qdrant gives a recommendation regarding all the points. for example, if i understant it right, it tries to give a product that is both similar to the watch and the t-shirt at the same time. No, I do not want this. I want to get recommendations that are similar to the points individually. Is this possible directly? if not, how do you suggest to achieve this?"
  },
  {
    "threadId": "1250135613177331833",
    "name": "question about must_not filters and full text index",
    "messages": "Hi! \nI have a couple of payload fields that I want to set positive and negative filters on (must and must_not in a combo). \nThe performance of must_not is OK, but I'm seeing an UNINDEXED_FIELD warning when running the issues call (despite the fact the fields are indexed) :\n```\"text\": {\n        \"data_type\": \"text\",\n        \"params\": {\n          \"type\": \"text\",\n          \"tokenizer\": \"word\",\n          \"min_token_len\": 2,\n          \"max_token_len\": 20,\n          \"lowercase\": true\n        },\n        \"points\": 27831895\n      },``` \n```\"filter_keywords\": {\n        \"data_type\": \"text\",\n        \"params\": {\n          \"type\": \"text\",\n          \"tokenizer\": \"word\",\n          \"min_token_len\": 2,\n          \"max_token_len\": 20,\n          \"lowercase\": true\n        },\n        \"points\": 27830145\n      },```\n\nIs there a support for must_not on a full text index?\nThanks!"
  },
  {
    "threadId": "1253707515007602739",
    "name": "Can someone help me to search data from qdrant using qdrant api key.",
    "messages": "While searching data from qdrant usinf qdrant api key."
  },
  {
    "threadId": "1253759141013618799",
    "name": "Qdrant client search error: Can anyone help me with this error",
    "messages": "While executing a client.search using the Qdrant API key, I encountered this error."
  },
  {
    "threadId": "1253698241770422426",
    "name": "Flush collection",
    "messages": "Is there a way to flush all points from a collection without deleting the collection itself?\nI'm looking for a way to reset all collection data so that the collection configuration remains the same."
  },
  {
    "threadId": "1252964174427062326",
    "name": "Restoring Snapshot: File IO error: failed to unpack",
    "messages": "Hello, \nI've created a snapshot on the interface and wanted to share it to one of my engineer.\nWhen he tried to restore the snapshot on his own computer he gets the following error:\n```rust\n2024-06-19T09:26:00.613825Z INFO actix_web::middleware::logger: 172.18.0.1 \"GET /collections HTTP/1.1\" 200 90 \"http://localhost:6333/dashboard\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 Edg/126.0.0.0\" 0.001094\n2024-06-19T09:26:57.868421Z WARN qdrant::actix::helpers: error processing request: File IO error: failed to unpack `/qdrant/storage/tmp/col-pythia-bm25-recovery-eXzhWJ/0/segments/5ca6442e-686b-4cb2-9f4e-4563d3908eb0.tar`\n2024-06-19T09:26:57.883186Z INFO actix_web::middleware::logger: 172.18.0.1 \"POST /collections/pythia-bm25/snapshots/upload HTTP/1.1\" 500 167 \"http://localhost:6333/dashboard\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 Edg/126.0.0.0\" 32.982850\n```\nWe use the exact same Qdrant docker image. What could be the cause ? Ram/Storage ? The Snapshot is only 6gb, so it's unlikely a storage issue"
  },
  {
    "threadId": "1252503438336458875",
    "name": "Degradation of performance under highload",
    "messages": "We use Qdrant in production. But when rps are increase enough high we see qdrant degradation. (E.g. from 30 to 150 rps)\n\nSettings\nWe use Qdrant in k8s via official helm chart\nQdrant version: 1.9.1\nWe connect to Qdrant via grcp from python client.\n\nOs version: Debian GNU/Linux 12 (bookworm)\nCPU: requests: 400cpu\nRAM: requests: 2Gi\nSize of storage: ssd disks 200gb\n\nWe have 5 nodes for qdrant.\n```json\n{\n    \"status\": \"green\",\n    \"optimizer_status\": \"ok\",\n    \"indexed_vectors_count\": 15007,\n    \"points_count\": 26360,\n    \"segments_count\": 8,\n    \"config\": {\n        \"params\": {\n            \"vectors\": {\n                \"size\": 512,\n                \"distance\": \"Cosine\"\n            },\n            \"shard_number\": 5,\n            \"replication_factor\": 3,\n            \"write_consistency_factor\": 2,\n            \"on_disk_payload\": false\n        },\n        \"hnsw_config\": {\n            \"m\": 16,\n            \"ef_construct\": 100,\n            \"full_scan_threshold\": 10000,\n            \"max_indexing_threads\": 0,\n            \"on_disk\": false\n        },\n        \"optimizer_config\": {\n            \"deleted_threshold\": 0.2,\n            \"vacuum_min_vector_number\": 1000,\n            \"default_segment_number\": 1,\n            \"max_segment_size\": null,\n            \"memmap_threshold\": 200000,\n            \"indexing_threshold\": 10000,\n            \"flush_interval_sec\": 5,\n            \"max_optimization_threads\": null\n        },\n        \"wal_config\": {\n            \"wal_capacity_mb\": 32,\n            \"wal_segments_ahead\": 0\n        },\n        \"quantization_config\": {\n            \"scalar\": {\n                \"type\": \"int8\",\n                \"always_ram\": true\n            }\n        }\n    },\n    \"payload_schema\": {\n        \"loaded_at\": {\n            \"data_type\": \"integer\",\n            \"points\": 26360\n        }\n    }\n}\n```\n\nStrangely, sometimes after quadrant crashes under load, it recovers for a while, but then it still crashes\n \nAlso unclear why some of points are indexed. Seems they shouldn't be indexed at all"
  },
  {
    "threadId": "1252942984652001290",
    "name": "Only find info if using Qdrant via langchain",
    "messages": "Not sure why, but my RAG doesn't give me the answer if I am querying using the code below:\n\n        const queryResponse = await this.client.search('necps', {\n            limit: k,\n            vector: query,\n            with_payload: true\n        });\n\n\"The provided text does not include specific information about the ...\" ... \n\nBut works when using langchain like below\n\n        const vectorStore = await QdrantVectorStore.fromExistingCollection(\n            new OpenAIEmbeddings({ apiKey: OPENAI_API_KEY }),\n            {\n                url: VITE_QDRANT_CLUSTER_URL,\n                apiKey: VITE_QDRANT_API_KEY,\n                collectionName: collection,\n                contentPayloadKey: 'page_content'\n            }\n        );"
  },
  {
    "threadId": "1252940020931629137",
    "name": "Hi! Having problems with distributed deployment using helm qdrant.",
    "messages": "We have a 5 nodes qdrant cluster setup in Production with each node having 512GB of ram and 1.5TB disk of gp3 type (16k IOPS). Overall there are 2B vectors of dimension 768.\n\nIt was working fine. But then starting yesterday it started giving out timeouts on searches. checking the logs we found the following errors/warnings:\n\nlogs from qdrant node: `WARN storage::content_manager::consensus_manager: Failed to apply collection meta operation entry with user error: Wrong input: Cannot deactivate the last active replica 493202810298905 of shard 6`\n\nerror msg from search api call: `{\\\"status\\\":{\\\"error\\\":\\\"Service internal error: The replica set for shard 5 on peer 578373308907840 does not have enough active replicas\\\"},\\\"time\\\":0.000365886}`\n\nCould someone help us here?"
  },
  {
    "threadId": "1250703371980111913",
    "name": "Multivector/Multimodal Vector Search in Individual Queries",
    "messages": "Consider a Product Search Platform where each \"product\" (or point, in Qdrant terms) includes both image and text data. For example, a product may have multiple images (e.g., 5 images) and a text attribute (e.g., title).\n\nI aim to store embeddings for all 5 images (multivector) and the title (multimodal embedding) within the same point. I also want to query these multiple vectors simultaneously. For instance, if I query using one image embedding and one text embedding at the same time, the goal would be to find the point with the closest matching image and text embeddings.\n\nI am still working out the exact logic for this. If you have any suggestions or insights on how to implement this proof of concept (or if it's a viable approach), please share your thoughts!<:thonk:1212491184732377189>"
  },
  {
    "threadId": "1252371722867441766",
    "name": "query vector embedding",
    "messages": "Does fastembed in QD have a different embedding for queries vs. regular vectors? From all documentation I've found online it seems to use the same embedding function, though from my understanding these SHOULD be different. I could be wrong though."
  },
  {
    "threadId": "1252208988280455209",
    "name": "Frequent Connection Reset by peer",
    "messages": "Hi, We've been using qdrant for around 6 months now. Server is hosted in aws ( us-east-1 and bought through aws marketplace). I'm trying to access from a different region (aws me-central-1) and frequently getting `Connection reset by peer` . Although this goes away after retrying, I just wanted to confirm if there is anything I could do to avoid this in the first place. Client version 1.7.0 and server version 1.7.3\n\nAttaching the complete logs."
  },
  {
    "threadId": "1252550368810766336",
    "name": "Out of memory cluster restarting for 1 hour without stopping",
    "messages": "Hello, \n\nI have a free cluster that arrived out of memory. I restarted it but it has been restarting for 1 hour without stopping.  I don't know how to get out of this loop. The only option I have is to delete it. All the other buttons are disabled.\n\nWhat should I do?"
  },
  {
    "threadId": "1252310901735030794",
    "name": "add method, switching from in memory to docker client",
    "messages": "When I use the client.add function on an in memory databse, everything works fine. However, as soon as I switch it to a an actual port, all of a sudden I get a a ResponseHandlingException: Input should be a valid integer. Does the .add method only work for inmemory vector stores? What should I be using instead?"
  },
  {
    "threadId": "1243855655601967144",
    "name": "What is additional Pricing for Hybrid Cloud for qdrant?",
    "messages": "I am using aws cloud so what is additional cost from qdrant side for using hybrid cloud"
  },
  {
    "threadId": "1262379736709988523",
    "name": "Inconsistent vector",
    "messages": "Hello,\n\nI'm writing an qdrant upserter, and for unit test I use in-memory Qdrant client.\nBut I can't retreive my vectors, they're different.\n\n**Vector inserted** : `[-0.22470705211162567, 0.3938416540622711, -0.7628212571144104, -0.6186320781707764, 0.04166025295853615]`\n**Retrieved vector** : ` [-0.20756955444812775, 0.3638049364089966, -0.704643964767456,  -0.5714514851570129, 0.038482993841171265]`\n\nTest code :\n```python\nret_values_from_qdrant = self.client.scroll(\n    collection_name=f\"collection_corpus_en_embmodel_v2\",\n    limit=100,\n    with_vectors=True,\n)\n\nself.assertEqual(2, len(ret_values_from_qdrant[0]))\nfor s in ret_values_from_qdrant[0]:\n    self.assertIn(uuid.UUID(s.id), [self.slice_id0, self.slice_id1])\n    e0 = self.emb0.tolist()\n    e1 = self.emb1.tolist()\n    self.assertIn(s.vector, [e0, e1])\n    self.assertEqual(s.payload[\"document_id\"], self.docid)\n    self.assertListEqual(s.payload[\"document_sdg\"], [1, 2])\n    if s.id == self.slice_0.id:\n        self.assertEqual(s.payload[\"slice_sdg\"], 1)\n    elif s.id == self.slice_1.id:\n        self.assertEqual(s.payload[\"slice_sdg\"], 2)\n```\n*Note : * `self.emb0` and `self.emb1` were created with numpy : `numpy.random.uniform(low=-1, high=1, size=(5,)).astype(numpy.float32)`"
  },
  {
    "threadId": "1251298280696971337",
    "name": "in-memory storage",
    "messages": "I'm trying to use the in-memory storage, I upserted the vectors into the in memory collection in a jupyter notebook, and then I tried to do a vector search through it from a different file within the same directory, also using just client = QdrantClient(\":memory:\"), but for some reason it thinks that the collection doesn't exist. What's going on?"
  },
  {
    "threadId": "1251173767007174677",
    "name": "Do filters boost threshold?",
    "messages": "I wanted to know if I apply a filter on a query. Will this boost the scores  based on that filter? Or it just filter based on condition and do not imact the scores?"
  },
  {
    "threadId": "1250919019972788264",
    "name": "How to retrieve points based on metadata filtering and extract embeddings from those points?",
    "messages": "If you could tell me how to achieve it with LlamaIndex nodes, that would be even better. If not, using qdrant native library is also fine. I can see that the embedding arrays are persisted in the vector store and I can copy each individual embedding in the qdrant dashboard. But how can I retrieve them from the API?"
  },
  {
    "threadId": "1244627629320175657",
    "name": "Determining collection size when using Librosa.feature.mfcc for embedding",
    "messages": "I have s script that embeds sound waves using librosa.feature.mfcc() function as \"mfcc=librosa.feature.mfcc(S=log_S, n_mfcc=13), which returns a two dimensional (13,1206) numpy array as embeddings.\n\nMy question is that what I should do when creating a collection? What value should I use for size attribute?  Also if the vector of a point is always an one dimensional list, should I flatten the two Dimensional numpy array?\n\nThank you in advance."
  },
  {
    "threadId": "1250762818911862814",
    "name": "How to apply filter to search an element  of query array in payload array ?",
    "messages": "I have an array key in my payload, now i want to apply filter to search if the any of the element in the query array matches any of the value in payload array, for ex: i have a query array [\"blue\", \"black\"], i want to apply filter to check if any of these colors is present in the array named colors , ex: colors: [\"green\", \"brown\"...etc], Is there any way to achieve this with curl request ?"
  },
  {
    "threadId": "1247546418110005258",
    "name": "Hybrid search & rerank",
    "messages": "My aim is to build a search engine benefiting from Qdrant + AWS Neptune, where i do keyword search on AWS Neptune and vector search on Qdrant.\n\nI was able to combine their results and run it in parallel. However, the way i combine was linear weight for each of them.\n\nI came accross this article [https://qdrant.tech/articles/hybrid-search/] mentioning that its much better to use a cross-encoder instead of weights on results.\n\nCould not found any example, nor implementation of such idea. \nAny hint of implementation? or what should be done at this stage?\n\nShould i use a different lang model to do distance measuring between returned results and query? so we have some variation in scoring?"
  },
  {
    "threadId": "1249992532662685769",
    "name": "Hybrid Filtering",
    "messages": "Hi Team,\nI have a cluster with around 10 payload fields.\nI will require 3 of the fields to be indexed.\nNeed to understand if I need to index rest 7 as well.\n\n```\n{\n  \"field_1\": \"integer\", -> Indexed\n  \"field_2\": \"integer\", -> Indexed\n  \"field_3\": \"integer\", -> Indexed\n  \"field_4\": \"keyword\", -> Not-Indexed\n  \"field_5\": \"integer\", -> Not-Indexed\n  \"field_6\": \"keyword\", -> Not-Indexed\n  \"field_7\": \"keyword\", -> Not-Indexed\n  \"field_8\": \"integer\", -> Not-Indexed\n  \"field_9\": \"bool\",    -> Not-Indexed\n  \"field_10\": \"integer\", -> Not-Indexed\n}\n```\nScenario: \nI will always run a search with filters \"field_1\", \"field_2\" and \"field_3\" applied. There will be additional \"field_4\" and \"field_5\" which will always be available in points which satisfy the \"field_1\", \"field_2\" and \"field_3\"  criteria.\n\nWanted to understand does Qdarnt indexed fields first and then non-indexed to optimise performance / or is the filter selection random and it can filter first based on a non-indexed field as well?"
  },
  {
    "threadId": "1250156268837011536",
    "name": "How do i calculate the server cost for 1000 concurant requests.",
    "messages": "I don't have enough experience with cloud computing, so can someone give any guidence to manage this discussion, i have to build a RAG chat bot for a website for which i want to use qdrant for production. I am not sure of the size of data textual data lets say approximately 1 GB and have to ensure good performance for 500 concurant users. what specifications should i use while deploying qdrant database ?"
  },
  {
    "threadId": "1250487751019597944",
    "name": "Searching for Qdrant freelancer for long-term collaboration",
    "messages": "Hi, we are currently building a software to automate and fully personalize newsletters. One part of it is creating labels aka categories for existing subpages/page titles pools and scoring their matching. For the initial setup of Qdrant VDB, the ongoing maintenance process and future development we are searching for a Qdrant-skilled freelancer with 5h of capacity weekly. Also, recommendations would be much appreciated."
  },
  {
    "threadId": "1248625738349936670",
    "name": "Question about memory consumption",
    "messages": "Hello,\nOur qdrant cluster is made up of 9 collections, which are quite different in size but all have more than 100k points.\n\nYesterday I did a big clean up thanks to some product side changes and removed almost 50% of our points in the cluster.\n\nBefore the clean-up we had a RAM request of 10Gi, after the deletion I was expecting consumption to be around 5GB. But the RAM behaved strangely.\n\nFirst it dropped as expected to around 7GB. I then restarted the cluster with a 7GB request. After peaking at almost 10GB, Qdrant went down to 2.8GB for about 30 minutes, before going up to 3.86GB, then 6.1GB, holding steady for a while until dropping back to 5.6 almost 15 hours later."
  },
  {
    "threadId": "1250400040644509827",
    "name": "Pagination in during live upserts",
    "messages": "Hey, how is the similarity search pagination affected by active upserts occuring in the DB during the pagination calls?\n\nFor example, I make a similarity_search, I get back the offset, I give the offset back to qdrant to get the next \"page.\" How will these pages be affected if I am simultaneously upserting into Qdrant, and potentially affecting the index that is being used for the similarity_search call?"
  },
  {
    "threadId": "1250036183778459678",
    "name": "retrieval results are less accurate ranging between 30-40% #help",
    "messages": "Recently all the results from Qdrant from all of my projects are showing accuracy between 30-40%. How to solve this #help"
  },
  {
    "threadId": "1245570878415044671",
    "name": "Pydantic Model Error",
    "messages": "I updated my cluster and now when I run my query code I'm getting a Pydantic Model error that says extra inputs are not permitted. I have no idea where this stems from. Any help would be awesome."
  },
  {
    "threadId": "1249790788238442578",
    "name": "Can I deploy qdrant to a VPS?",
    "messages": "Is it possible to deploy to a VPS with docker? is it recommended? What are the cons?"
  },
  {
    "threadId": "1250008091161002045",
    "name": "Can't update collections settings",
    "messages": "Hello,\n\nI try to update settings from some collections, by setting \"on_disck\" on true. But this query return an error :\n```json\nPATCH collections/collection_wikipedia_fr_sentence-camembert-base_v4\n{\n  \"vectors\": {\n    \"on_disk\": true\n  }\n}\n```\n\nThe error :\n```json\n{\n  \"error\": \"Format error in JSON body: invalid type: boolean `true`, expected struct VectorParamsDiff at line 1 column 26\"\n}\n```\n\nIf I follow the API references I can make :\n```json\n{\n  \"property1\": {\n    \"vectors\": {\n      \"on_disk\": true\n    }\n  }\n}\n```\nIt pass but doesnt change collections settings at all"
  },
  {
    "threadId": "1226777655328636928",
    "name": "Access qdrant docker with in other docker application",
    "messages": "How to access my qdrant docker runinng with a mounted path access it within other docker ?"
  },
  {
    "threadId": "1249974363441074208",
    "name": "set_payload with filter, but instead of filtering points, filter nested payload",
    "messages": "I am trying to use the set_payload api, and as demonstrated in the docs, there are two ways to use this api, either with a list of point_ids, or using `filters` keyword. But in my case, neither can help me. I have the point ids of the products that I want to modify their payload, but the specific nested property that I intend to modify, is an item of a list, that has a specific `property_id`; so, at first I have to filter the property with the id, and then modify it. Something like this:\n```\nPOST /collections/lang5_products/points/payload?wait=true\n{\n  \"filter\": {\n    \"must\": [{\n      \"key\":\"attributes[].id\",\n      \"match\": 2283\n    }]\n  },\n  \"points\": [14104159]\n  \"payload\": {\n    \"title\": \"test new\"\n  },\n  \"key\": \"attributes\"\n}\n```\nSo, in my main payload, I have a field named `attributes`, which is a list of attribute objects, each having an id, a title, and a value. I want to first find the one with the specific id (here 2283), and then modify its title (only the title, I DO NOT have the value at this point, but there is a value in db).\nI know this structure is not correct, but I wrote it this way to deliver my point."
  },
  {
    "threadId": "1247251707214430260",
    "name": "Langchain and Sparse Vectors in Qdrant",
    "messages": "Hi, \nI've been working with Langchain and Qdrant for the past year in a couple of projects.\nIn a new project I need to perform Hybrid Search to improve the retrieval of documents. The problem is there's no documentation of how to do this with Langchain.\n\nFrom this previous Post-Question (https://discord.com/channels/907569970500743200/1235840144821583892/1237422353663656006), there's an explanation of how to do it with Qdrant, but not how to implement this Hybrid search capability within a project that mainly uses Langchain to interact with Qdrant. https://qdrant.tech/articles/sparse-vectors/\n\nIs there any explanation, code example,  code repo, or documentation around this?\n\nThanks ЁЯЩВ"
  },
  {
    "threadId": "1249841595625377843",
    "name": "Optimizing Payload Retrieval in Qdrant: Accessing Specific Fields Only",
    "messages": "I'm currently performing a db migration, and I need to go through all the points from a very large collection.\n\nI'd like to retrieve only a specific field from the payload of each point, rather than the entire payload.\n\nIs there a good way of doing this?\n\nThanks in advance!"
  },
  {
    "threadId": "1249715141222858813",
    "name": "Scores with Discover",
    "messages": "Hi all, \n\nCurrently working with multiple scoring techniques that QDrant allows. Basically, the normal \"recommend\" with positive/negatives + best_score strategy, as well as the Discover functionality with the paired list + a target vector. I see this is in the documentation:\n\n\"Context Search: Without a target, .....the maximum score a point can achieve is 0.0, meaning many points may have a score of 0.0.\n\nTarget Search: When a target is provided......the integer part represents the rank with respect to the context, and the decimal part relates to the distance to the target. The context score for each pair is +1 if the point is closer to a positive example than to a negative example, and -1 otherwise.\"\n\nWhen I run the search with a target, a lot of my scores are very negative (like -16) or very positive (+11).  Is it a proper interpretation that the \"11\" and the \"-16\" part then refer to the \"rank with respect to the context\", and the decimal after it is the distance to the target. Aka, something VERY positive in this case is probably a very relevant item?\n\nLikewise, when I run this search without a target vector and just the paired list, my scores end up ranging from -.79 to -2.7, but I've also seen it be a mix of positive and negative. and from my understanding, the perfect score in this case is \"0\".  Is doing some sort of absolute value on this (aka sorting by distance to 0) the right idea?"
  },
  {
    "threadId": "1248701893287608551",
    "name": "Search vs Count inconsistent results",
    "messages": "I am encountering an issue with the search functionality in my system. When I use the search feature with specific filters, the results returned are fewer than expected. However, when I use the count feature with the same filters, the count value is significantly higher and appears to be correct.\n\nHere is the payload for both the search and count operations. Interestingly, when I use only one location keyword instead of two, the search results are accurate. This leads me to suspect there might be an issue with the indexing of the location.keywords field.\n\nThis problem has surfaced recently; it was working correctly before. Could anyone help diagnose why the search functionality is no longer yielding correct results?"
  },
  {
    "threadId": "1249579624636219462",
    "name": "Can't upload a collection due to \"Unable to close http connection\" error",
    "messages": "I'm trying to upload a collection but keep receiving this error, could anyone help please?:\n\n```qdrant_client.http.exceptions.UnexpectedResponse: Unexpected Response: 403 (Forbidden)\nRaw response content:\nb'{\"error\":\"forbidden\"}'\nWARNING:root:Unable to close http connection. Connection was interrupted on the server side```\n\nI'll share the full code I'm using if necessary."
  },
  {
    "threadId": "1249283001569902602",
    "name": "Blog-Reading Chatbot with GPT-4o",
    "messages": "I'm quite neew to Qdrant. I've been following the Blog-Reading Chatbot with GPT-4o tutorial and just want to confirm that the first part is about setting up a Hybrid Client on Scalway, and then proceeds to run Qdrant locally...?!\n\nhttps://qdrant.tech/documentation/examples/rag-chatbot-scaleway/"
  },
  {
    "threadId": "1163988877262999604",
    "name": "geo distance in geo filter with radius",
    "messages": "Which distance is used (for example, earth distance) in geo filter with radius?\nIs it possible to return distance as output?\nThank you."
  },
  {
    "threadId": "1249295679805853809",
    "name": "Langchian using Qdrant.from_existing_collection()",
    "messages": "Hi all I've created an instance of Qdrant using:\n\ndocker run -p 6333:6333 -p 6334:6334 \\\n    -v $(pwd)/qdrant_storage:/qdrant/storage:z \\\n    qdrant/qdrant\n\nI create the collection an load some vectors\nthen in order to use Qdrant as retriever I use the suggested function:\n\nqdrant = Qdrant.from_existing_collection(\n    embedding=embeddings,\n    collection_name=collection,\n    url=qdrant_url,\n)\n\nbut when I run the script the error is:\nTypeError: Qdrant.from_existing_collection() missing 1 required positional argument: 'path'\n\nif I use the path to qdrant_storage intead of url:\n\nqdrant = Qdrant.from_existing_collection(\n    embedding=embeddings,\n    collection_name=collection,\n    path=qdrant_path,\n)\n\nthe message is:\nRuntimeError: Storage folder ./qdrant_data/collections/ is already accessed by another instance of Qdrant client. If you require concurrent access, use Qdrant server instead.\n\nCan you help me understand what is the issue, thanks"
  },
  {
    "threadId": "1248843881827532851",
    "name": "UUID string format for point IDs",
    "messages": "Hi Team,\n\nIf we use want to use UUID for point IDs, do they have to be a standard type of UUID format such as 550e8400-e29b-41d4-a716-446655440000?\n\nOr can we make it any arbitrary custom string as long as it is unique? E.g. \"project_1234_000001\".\n\nThank you"
  },
  {
    "threadId": "1248424253968224349",
    "name": "Highly variable query speeds",
    "messages": "I've created a small collection with the following details:\n\n`{'status': 'green',\n 'optimizer_status': 'ok',\n 'vectors_count': None,\n 'indexed_vectors_count': 292436,\n 'points_count': 146218,\n 'segments_count': 7,\n 'config': {'params': {'vectors': {'dense': {'size': 1024,\n     'distance': 'Cosine',\n     'hnsw_config': None,\n     'quantization_config': None,\n     'on_disk': True,\n     'datatype': None}},\n   'shard_number': 1,\n   'sharding_method': None,\n   'replication_factor': 1,\n   'write_consistency_factor': 1,\n   'read_fan_out_factor': None,\n   'on_disk_payload': True,\n   },\n  'hnsw_config': {'m': 16,\n   'ef_construct': 100,\n   'full_scan_threshold': 10000,\n   'max_indexing_threads': 0,\n   'on_disk': False,\n   'payload_m': None},\n  'optimizer_config': {'deleted_threshold': 0.2,\n   'vacuum_min_vector_number': 1000,\n   'default_segment_number': 0,\n   'max_segment_size': None,\n   'memmap_threshold': 10000,\n   'indexing_threshold': 20000,\n   'flush_interval_sec': 5,\n   'max_optimization_threads': 1},\n  'wal_config': {'wal_capacity_mb': 32, 'wal_segments_ahead': 4},\n  'quantization_config': None},`\n\nThe collection currently has 23 payload fields. All of which are currently indexed (just for filter testing) Note we plan to reduce to 16 (15 indexed) when we scale. \n\nI am using the search_groups function with the limit set to 500 results. I 've run a test consisting of 10 queries repeated five times to see the impact of the on_disk storage for vectors and our large number of payload fields:\n\nTest 1: Dense and payload fields on disk - query speeds sit at ~3-8 seconds per query\n\nTest 2: Dense and payload fields both in RAM - query speeds sit at ~0.5 seconds\n\nTest 3: Dense in RAM, payload on disk - query speed back to original 3-8 seconds.\n\nWe are currently using a Network-Mounted SATA SSD Disc with measured IOPS of 45k and want to scale to 50 million vectors. Can you advise on our config and how we can speed up search while using on_disk storage for our payload?"
  },
  {
    "threadId": "1248666593177632934",
    "name": "Qdrant with vdf-io",
    "messages": "!import_vdf qdrant  -u 'https://node-0-sdfasdfasdfasdfasdfasdffasdfasdfsdfp.cloud.qdrant.io'\nbelow is the error when i am running above code\nError: <_InactiveRpcError of RPC that terminated with:\n    status = StatusCode.UNIMPLEMENTED\n    details = \"Received http2 header with status: 404\"\n    debug_error_string = \"UNKNOWN:Error received from peer  {created_time:\"2024-06-07T15:48:10.21411441+00:00\", grpc_status:12, grpc_message:\"Received http2 header with status: 404\"}\"\n>"
  },
  {
    "threadId": "1248330503262830702",
    "name": "Optimizing vector retrieval for short texts",
    "messages": "Hello, \n\nI'm currently using text-embedding-3-small with Qdrant.\n\nMy db is composed of vectors which are like that : embedding(\"Very short sentence of max 5,6 words\")\n\nMy inputs are usually single word or maybe 2/3 words max like \"sword\" or \"agency\", or things like that.\n\nFrom the documentation https://qdrant.tech/documentation/embeddings/openai/, I see how I can use the embedding model to retrieve points that answer a specific question: \"What is the best to use for vector search scaling?\"\n\nHowever, I rather want to use it to recover the points which are closest to my input in the way of writing, and the very meaning of the sentence. Not as a question answer.\n\nWhat would be the best way to do that?"
  },
  {
    "threadId": "1248392259733295176",
    "name": "Indexed Payload Advice:",
    "messages": "We are create a collection with a large amount of different file types and want to filter on a few consistent fields held within the string. From what we can tell, there are two options for payload filtering:\n1. Set the filepath to a payload index of type full-text (https://qdrant.tech/documentation/concepts/indexing/#full-text-index) and use the MatchText filter (e.g D:\\\\Working\\Documents\\Document123.docx)\n\n2. Break up the filepath items into three separate indexed payloads of type 'keyword' (e.g. Drive='D:\\\\', Directory='Working', File extn='.docx')\n\nAre you able to advise if there are any adverse performance issues with the first option (e.g. slower filter/search speeds)? My take is the second option will be a larger RAM overhead due to three additional indexed payloads (even though they will be limited to a much smaller subset of unique values). \n\nWe plan to store the payloads on_disk and expect in the order of 50 million vectors (and 16 indexed payload fields). Our current disk drive is a network-shared drive with a measured IOPS of 50k (during peak traffic periods)."
  },
  {
    "threadId": "1233084359863435356",
    "name": "Qdrant memory utilization monitoring",
    "messages": "We have a Grafana dashboard monitoring Qdrant memory utilization. It alerts when the container available memory is < 10%. Specifically, we are looking at the `container_memory_rss` metric. \nIn the past, we noticed Qdrant would perform poorly when it ran out of memory, so we added this alert as a leading indicator. However, the past few times this alert has gone off, Qdrant was performing normally even though it was using ~95% of available memory. Our workaround at this time is to restart our Qdrant service.\nI'm looking for advice on the following:  \n1) I'm assuming Qdrant is using all of its available memory as a cache. Is there a way to cap this cache so that it stops growing after some limit?\n2) It looks like this is all working as designed/intended. Is there a better metric to observe to alert on qdrant performance?"
  },
  {
    "threadId": "1248508592131407893",
    "name": "Train deep learning model with Qdrant",
    "messages": "Can I use PyTorch to train a model with Qdrant? If so, how can I do it? If not, since all the embeddings are stored in Qdrant, how can I download and extract all the points and vectors? Thanks in advance!"
  },
  {
    "threadId": "1248097529812222057",
    "name": "Nested payload indexes",
    "messages": "Hi, is it possible to create nested payload indexes? I am using langchain which if using its default qdrant integration does nest all payloads under metadata\n\n```\n{\n  payload:{\n    metadata:{\n      user_id: userId,\n      id: dataId\n    }\n  }\n}\n```\n\ni so far thought its not possible to create indexes on nested payload keys, but in this github issue https://github.com/qdrant/qdrant/issues/2336 it is said that indexes can be created at any level?\nIf that's really the case how would one create an index for the user_id or id in my example? \nMy guess would be using dot notation but there is no mention of any of this in the docs\n\n```\nclient.createPayloadIndex(\"{collection_name}\", {\n  field_name: \"metadata.user_id\",\n  field_schema: \"keyword\",\n});\n\n```\n\nThanks for the support, greetings"
  },
  {
    "threadId": "1247112224208715817",
    "name": "Trying to read-lock all collection segments is taking a long time. This could be a deadlock and may",
    "messages": "Hello,\n\nFor some time now I've had this error message : `Trying to read-lock all collection segments is taking a long time. This could be a deadlock and may` appear in my qdrant logs and it seems to be causing timeouts on my ingestions scripts.\n\nHow can I avoid this problem?"
  },
  {
    "threadId": "1247615284186189915",
    "name": "Pre Filtering and Updating records",
    "messages": "We have a dataset that is compromised of 70mil records where each record has 2 vector fields, 5 string fields and 4 integer fields.  We want to create an application that basically sends in a vector to qdrant database of 70 mil records and retrieve the closest records. \n\nQuestions:\n\n1.) Can we pre filter on the string and integer fields such as:\n\"dog\" in string field A\n\"price\" > 50.19\n\"toy\" not in string field B\n\n2.) Once the prefilters happen, since we have 2 vector fields, can we assign a weight of 50/50 to each and retreive the results in batches of 40, we want our application to retrieve the first 40, then the next most similar 40 kind of in a limit and offset mechanism, is that possible? \n\n3.)We have metadata fields such as the string and integer fields above that we want to update on a daily basis as these fields are constantly changing, is that possible and what is the indexing time look like, our use case is we want to update every record once every 24 hours which is over 70 million. \n\n4.) Can we setup a call as our company wants an enterprise plan and we have many customers so a meeting to discuss use cases would greatly help, is that possible?"
  },
  {
    "threadId": "1248202675032555551",
    "name": "Is_null issue",
    "messages": "Hi, \nwhen I queried :\n```json\nPOST collections/collection_name/points/count \n{\n  \"filter\": {\n    \"must\": [\n      {\n        \"is_null\":{\n          \"key\": \"slice_sdg\"\n        }\n      }\n    ]\n  }\n}\n```\n\nIt run and retrieve points without care for the condition. I know I need a must clause but shouldn't this raise an error? It's very confusing."
  },
  {
    "threadId": "1248200599011590154",
    "name": "Updating collection",
    "messages": "Hi guys, \n\nCan you help/guide me?\n\nI am trying to add new points into existing collection. What are steps that needs to be done to handle adding new points to existing colletion correctly? \n\nI am using POST collections/{collection_name}/points/count to get \"last id\" of point in collection\nI am adding points with id like this \"last_id\" + index in new points array\nbatch insert\n\nshould I add some more additional steps? Somehow to force qdrant to reindex or ?\n\nThanks!"
  },
  {
    "threadId": "1247469389494616106",
    "name": "Recommend API: ValueError: unknown enum label \"best_score\"",
    "messages": "Code Snippet:\n```\nq_client.recommend(\n    collection_name=\"example_collection\",\n    query_filter=models.Filter(must=qdrant_filters),\n    with_payload=True,\n    with_vectors=False,\n    limit=query_limit,\n    score_threshold=0.75,\n    using=\"abc\",\n    strategy='best_score',\n    positive=positive,\n    negative=negative,\n)\n```\nGet the error:\n```\nFile \"/usr/local/lib/python3.8/site-packages/qdrant_client/async_qdrant_client.py\", line 568, in recommend\n    return await self._client.recommend(\n  File \"/usr/local/lib/python3.8/site-packages/qdrant_client/async_qdrant_remote.py\", line 621, in recommend\n    grpc.RecommendPoints(\nValueError: unknown enum label \"best_score\"\n```\n\nDoes anyone know the reason?"
  },
  {
    "threadId": "1239553410060124220",
    "name": "Is there a way to stop qdrant caching disk data in RAM and preloading disk data in ram.?",
    "messages": "Hi, I'm trying to run qdrant on an aarch64 machine with limited RAM. Even after setting the on_disk attribute to true I observed qdrant taking more ram than required, I went through the documentation titled \"Database Optimization Strategies\" and I found that qdrant caches and preloads disk data onto ram, I just want to know if there is a way to stop it from doing so.!\nThank you"
  },
  {
    "threadId": "1247861837962416159",
    "name": "Scaling options for qdrant deployments in Kubernetes/OpenShift",
    "messages": "We have a Qdrant deployment running a single replica in OpenShift. Is it possible to run more than one replica to ensure 100% uptime? For example, can we use a PVC with RWX mounted to multiple pods?"
  },
  {
    "threadId": "1247500230589419571",
    "name": "Kindly explain `always_ram` option in quantisation config",
    "messages": "My understanding was that when `always_ram = False`, then the quantised vectors are stored in SSD disk.\nI created a collection with 6M vectors, with all vectors and payload are on disk. I have indexed 2 payload keys and HNSW index is on RAM.\nWhen I used scalar quantised vectors with `always_ram = False`, then also the collection is using around 4GB of RAM. I was expecting it to be much lower, since only payload index and vector index is on RAM. Is my understanding correct?"
  },
  {
    "threadId": "1247736703607767071",
    "name": "Should I index the column that's being used for Multitenancy?",
    "messages": "Hey, we are using Qdrant Cloud. Recently, we implement multitenancy in our Qdrant. To differentiate between project, we create a payload named \"project\". For now, there are 4 projects on the same vector database but the payload for each project is very different.\n\nNow our nodes is getting timeout everytime we searching on our vector database even though the nodes is green (I actually from the same affiliation as https://discord.com/channels/907569970500743200/1047555268499755152/1247731205122691103). My suspicion is that it's because of the multitenancy that we implement. We haven't index the \"project\" payload. Do you think this is the cause of the problem? Thank you in advance!"
  },
  {
    "threadId": "1247595263539810304",
    "name": "Retrieve payloads in similarity search results",
    "messages": "Is there a way to retrieve the payload from the similarity search I run on the vector store?\n\nRight now I first get the _id and then have to make another call with the _id to fetch the payload."
  },
  {
    "threadId": "1244546064091189279",
    "name": "Tips on retrieving large number of candidates",
    "messages": "I am using qdrant to retrieve similar candidates in a collection (6 million entries). Our target is to achieve retrieve large number of candidates (approx 10k).\n\nCurrently, I am getting timeout error for limit=10k. Can you provide some insights on what can be possible reasons and tips on how to achieve this?"
  },
  {
    "threadId": "1247046324587593820",
    "name": "How similar results are sorted in search and can it be modified?",
    "messages": "Doing basic search using vector embedding I've spotted following thing happening:\nLet's say I have 3 records ( with ids A, B and C)  with identical embeddings. When I do search using embedding of B\n```client.search(\n            collection_name=COLLECTION_NAME,\n            query_vector=B.embedding.vector,\n            limit=10\n        )\n```\nI can end up having following result:\n```\n[Point(score=0.9999999999999991, payload={...,my_id='A'}),\n Point(score=0.9999999999999991, payload={...,my_id='B'}),\n Point(score=0.9999999999999991, payload={...,my_id='C'})]\n```\nIn other words in case of the same score ordering appears to be happening based on payload. Or at least this is my guess.\n\nI'd like to have `Point(score=0.9999999999999991, payload={...,my_id='B'})` as a **first** element in the list, since that's the one I've used as an origin element.\n\nIs there a way to control this output sorting?"
  },
  {
    "threadId": "1247529307845169152",
    "name": "Make.com Error",
    "messages": "Hello, I'm trying to retrieve data from the collection via make.com. I'm creating embeddings with openai and then using the Search Points module. I get always the error: \"The operation failed with an error. [400] Wrong input: Vector dimension error: expected dim: 1536, got 1\".\n\nI have attached few screenshots."
  },
  {
    "threadId": "1246143329628979362",
    "name": "Getting the data from a collection on Qdrant Cloud and running similarity search on it.",
    "messages": "I am trying to get my collection from Qdrant cloud and then run a similarity search for a query against the collection but I can't get it right. Can I please get some assistance?\n`vector_store = Qdrant.from_existing_collection(embedding=OpenAIEmbeddings(),collection_name=\"os.getenv(\"QDRANT_COLLECTION_NAME\")\",url=os.getenv(\"QDRANT_URL\"),api_key=os.getenv(\"QDRANT_API_KEY\"))`\nIt keeps asking me to include a path, what does that refer to?"
  },
  {
    "threadId": "1247321834462908588",
    "name": "Payload Indexing Relative Size Overhead",
    "messages": "Hi there, \nI am trying to decide between different field types for my payload indices. I want to know if the RAM overhead will be larger if I have strings of text (e.g. keyword) or if I can store the same strings as integers and have a key:value pair that sits outside of qdrant that can be used as a look up to perform integer filtering. \n\nThe payloads I am looking at are Project Managers and I could use the name of the Project Managers as a string (keyword) or the employee ID (integer). \n\nAre you able to advise which field type will have a larger RAM overhead or is the RAM overhead on payload indices based on the number of unique values (so the overhead will be the same)?"
  },
  {
    "threadId": "1247231536768614571",
    "name": "Creating a new collection from part of existing one",
    "messages": "Hi!\n\nI am trying to create a new collection from only filtered points from my current collection. \n\nI tried this but it was not working as the output of scroll are Record objects and input to upsert should be PointStruct objects. \n\n`documents = qdrant_client.scroll(\n            collection_name=\"current-collection\",\n            scroll_filter=models.Filter(\n                should=[\n                    models.FieldCondition(\n                        key=\"doc_id\",\n                        match=models.MatchValue(value=\"id_here\"),\n                    ),\n                ]\n            ),\n        )\n\nqdrant_client.create_collection(\n    collection_name=\"new-collection\",\n    vectors_config=models.VectorParams(size=384, distance=models.Distance.COSINE),\n)\n\nqdrant_client.upsert(\n    collection_name=\"new-collection\",\n    points=[\n        models.PointStruct(\n            documents[0][0])\n    ],\n)`\n\nDoes anyone have any advice on how to resolve this issue?"
  },
  {
    "threadId": "1247219876465082389",
    "name": "Error calling GET collection",
    "messages": "I'm seeing a weird error in logs when making a call to a distributed Qdrant cluster. The error is `Service internal error: 0 of 0 read operations failed` (with no further detail in the error message) when calling GET collection (https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/get_collection). It's always for the same collection, other collections are retrieved without issue. If I shell into a pod and hit the qdrant API that way I don't get the same error. This call is made from a scheduled job which has been consistently successful in the past. I have just started working with qdrant and didn't see anything in the docs or discussions here, would anyone have any ideas?"
  },
  {
    "threadId": "1247132321404944404",
    "name": "Help regarding the tutorial posted at https://qdrant.tech/documentation/quick-start/#run-a-query",
    "messages": "Hello Team,\nI'm learning vector databases and came across qdrant. \nI was going through data insertion and retrieval example posted here:\nhttps://qdrant.tech/documentation/quick-start/#run-a-query\n\nI'm just wondering how do we generate vector values from user query inputs so that I can use it as shown in the example.\nAlso, while storing the data records, how do we find respective vectors ?\nIt isnt mentioned in the article as well."
  },
  {
    "threadId": "1246111820402655273",
    "name": "Similar search but via point ID",
    "messages": "Hello, \n\nI'd like to do a similar search but instead of giving the api the vector I'd like to give the ID of the element already in the database for which I'd like to retrieve the closest points. \n\nWould that be possible?\n\nThanks,\nRiad"
  },
  {
    "threadId": "1247111741997846650",
    "name": "Binary Quantization Slow Search",
    "messages": "Hi, I'm trying binary quantization on QDrant for the first time and my batched search times are huuuge compared to search without quantization (compared to hnsw search on float32 vectors). I think only one thread is used. How am I supposed to reach the 40x faster search as in https://qdrant.tech/articles/binary-quantization/ ?"
  },
  {
    "threadId": "1245730485385039882",
    "name": "Croatia and Slovenia Languages",
    "messages": "Hello all, do you have any suggestions for embeddings which perform well for Croatia and Slovenia Languages?"
  },
  {
    "threadId": "1246393164923666442",
    "name": "Help on qdrant local storage",
    "messages": "Hey so I am new to rags so I have build a rag using langchain and qdrant locally using docker and I want to know whether is there any limit on the amount of embeddings I can store locally and on my docker container I see container size 300/3.5gb. does this mean I have only 3.5gb of space to store embeddings? I am kind of confused over so many things any help is appreciated please bz I want to scale this thing up \n<#1244504266496806994>"
  },
  {
    "threadId": "1246874962745364542",
    "name": "Getting the date of Collection Creation",
    "messages": "Hello there,\n\nI have a use case where I'm taking advantage of aliases (since we are not allowed to rename collections) in order to switch to updated embeddings live with zero downtime. \n\nI basically will have collectionA and collectionB that will ping pong between being the \"production\" database every night. I'd like to be able to simply check the timestamp of a collection being created to know what is the \"newest\" one to assign the production alias to when the time comes for my recommendation service to restart and bring in the freshest data, however I'm not sure if there is a way to obtain this information with the current API. Is there anywhere to figure out what collection is \"newest\"?"
  },
  {
    "threadId": "1246601258786230313",
    "name": "Connection delays",
    "messages": "Hello,\n\nI'm running Qdrant in a local Docker container (Debian 11, Docker 26.1.3, Qdrant 1.9.4). At the moment, the instance only contains a single collection with around 10k points. There is a web application that is calling the Qdrant API by connecting to http://127.0.0.1:6333/... \n\nFor quite some time, everything seemed to be working fine, but recently I've noticed that the app is acting slower than usual - with connections to Qdrant taking over a second. However, the database itself does not show any performance issues, and according to the logs, all queries are taking less than 0.02 seconds. \n\nWhat could be the problem? Does it have anything to do with Qdrant or Docker itself? What settings should I check to identify the root cause?\n\nThanks!"
  },
  {
    "threadId": "1245837285434327041",
    "name": "What is the best practice of using timeouts for qdrant?",
    "messages": "Python qdrant client initializes via QdrantClient(), and it has a timeout parameter. What value should I set it to? I measured that on average qdrant takes less than 1 ms to respond, but I still set the timeout to 0.5 seconds just to be sure.\n\nIs timeout 0.5 seconds too low? As far as I understand, 0.5 might be too low because of qdrant's reindexing. For an example, during reindexing of the HNSW the response times might be higher than usual."
  },
  {
    "threadId": "1246186472374145096",
    "name": "Scrolling with `order_by` doesn't permit the use of offset?",
    "messages": "Hello, I am trying to scroll through all points in my DB. When I activate `order_by` on a timestamp, the returned `offset` is `None`. \n\nWhen I remove the `order_by` argument, the `offset` is correctly returned (the point id).\n\nIs this by design? If so, how would I scroll on the entire DB with `order_by`?"
  },
  {
    "threadId": "1245765451187748905",
    "name": "High Payload Update Impacting Search Engine Performance",
    "messages": "Hi everyone,\n\nI have a collection containing 2 million vectors in my search engine, which relies on daily crawling of products. Each day, I need to update the payloads associated with these vectors. However, when I begin updating the payloads (which have payload indexes on it too), my Qdrant instance becomes unresponsive to user search requests.\n\nDo you have any recommendations on how I can resolve this issue?"
  },
  {
    "threadId": "1245719225729945640",
    "name": "Payload limit for PDF",
    "messages": "Hey folks,\n\nI want to upload the pdf content to qdrant db, is there any limit for uploading the pdf content in one go? if yes then how much and how can I increase it?\nand if no, then how it does it work?\n\nThanks."
  },
  {
    "threadId": "1245707666005819502",
    "name": "Upsert api returning \"status\": \"ok\" even though it is not ok",
    "messages": "I was trying to modify a point, both the payload and the vector. When I was testing the api in the console, I tried with a vector that had different dimensions than the config (my vector config is 1024 dimensions and the test vector had only 3), the result said \"ok\" in the status, while when I checked, it did not apply the update. \n```\nPUT /collections/lang5_products/points\n{\n  \"points\": [\n    {\n      \"id\": 14104091,\n      \"payload\": {\n        \"color\": \"test color\"\n      },\n      \"vector\": [\n        0.1,\n        0.2,\n        0.3\n      ]\n    }\n  ]\n}\n```\nAnd the result:\n```\n{\n  \"result\": {\n    \"operation_id\": 468230,\n    \"status\": \"acknowledged\"\n  },\n  \"status\": \"ok\",\n  \"time\": 0.000102529\n}\n```\n\nAlso, when I used a vector with the same dimension, this time, it worked and also the result was the same as before."
  },
  {
    "threadId": "1245372141340459069",
    "name": "High time for search api",
    "messages": "I am wondering why should this search take 500-600 ms?\n```\nasync def search_query(\n    lang_int: int,\n    search_query: str,\n    limit: int,\n    qd_client,\n):\n\n    if not lang_int:\n        return 0\n    embedding_time = time.time()\n    query_vector = await generate_embeddings(search_query)\n    print(\"Embeddings took:\", time.time() - embedding_time)\n    normalized_vector = query_vector / np.linalg.norm(query_vector)\n    search = time.time()\n    search_results = await qd_client.search(\n        collection_name=f\"lang{lang_int}_products\",\n        query_vector=normalized_vector,\n        query_filter=models.Filter(\n            should=constants.SHOULD_FILTERS,\n        ),\n        search_params=models.SearchParams(hnsw_ef=128, exact=False),\n        limit=limit,\n        with_payload=constants.PRODUCT_LIST_PAYLOAD,\n    )\n    print(\"Qdrant search took:\", time.time() - search)\n    return search_results\n```\nAdditional notes:\n- `limit` is set to 700 results\n- I checked it with and without the `query_filter`, the result is the same (I have indexed the filtered fields, the difference between filtered and not-filtered search is less than 50 ms)\n- I have 500.000 items in the collection. Each one have a size of almost 100 kbs.\n- The time in question is the `print(\"Qdrant search took:\", time.time() - search)` line."
  },
  {
    "threadId": "1244983806709076141",
    "name": "Qdrant search is slower when adding filter condition",
    "messages": "filter_condition = models.Filter(\n            must=[\n                    models.FieldCondition(\n                        key=\"AuthorLoginId\",\n                        match=models.MatchValue(value=file_data['AuthorLoginId']),\n                    )\n                ]\n            )\n\n            # Perform the search query with the filter condition\n            top_predictions = client.search(\n                collection_name=collection_name,\n                query_vector=(\"summary\",  file_data['summary']),\n                limit=20,    \n                query_filter=filter_condition,\n                timeout=1000\n            )\n\nAfter adding filter condition: 3.28s/it\nBefore adding filter condition: 10.46it/s\n\nis there any solution for this search to be faster?"
  },
  {
    "threadId": "1245308828598140928",
    "name": "Snapshot Recover Status",
    "messages": "Hi there!\n\nI am using the snapshot api to recover my collection. Because of large collections I am using the wait = False parameter. But I am not getting what the status of my snapshot restoration is in the response. Is there any way to track what the restoration status is or when my restoration operation is completed?"
  },
  {
    "threadId": "1245136435862831104",
    "name": "Hi,",
    "messages": "Does qdrant supports partial indexing? If updates are made to an already indexed collection, does qdrant re-index the entire collection or only the newlyadded points? How does qdrant handle updates?"
  },
  {
    "threadId": "1245093015622651904",
    "name": "Getting Format error in JSON body error",
    "messages": "Been getting an error with a Recommend search with QDrant. The error specifically is:\n\n```Format error in JSON body: data did not match any variant of untagged enum RecommendExample at line 1 column 13386.```\n\nI've identified what is causing the error, which is the specificiation of just a single ID in the \"negative\" in the following code (note I'm using R + Retciulate to access the Python client)\n\n```\n    recs <- qclient$recommend(collection_name = \"products\",\n                              positive = kept_db_ids,\n                              negative = dec_db_ids,\n                              strategy = models$RecommendStrategy$BEST_SCORE,\n                              query_filter = models$Filter(must = list(models$HasIdCondition(has_id = as.character(elig_vector_points)))),\n                              limit = length(elig_vector_points))\n```\ndec_db_ids is a string \"78c3aad0-b059-491a-a3ca-086520e92b2a\". I can confirm that this point exists through the QDrant UI, but if I delete this from the above call (aka remove \"negative\"), the call works fine, so it's something clearly with this point in particular not working.\n\nI can confirm that for most of my calls, this has worked just fine, but every once in a while I run into this issue"
  },
  {
    "threadId": "1243556231600275516",
    "name": "Qdrant violates docker memory limit and gets killed",
    "messages": "Hi! I have the following setup:\n- Qdrant 1.8.4 deployed via docker swarm (26.0.1), single instance, compute node with ubuntu 22.04, currently with 16CPUs (AMD) and 64G RAM\n- One collection with 44M vectors of size 768, 3 payload indexes\n- I have on_disk: true for vectors and hnsw_config, and on_disk_payload: true\n- Starting without any memory limit imposed, RAM usage, as reported by docker, stops at 17G\n- Starting with a limit of 14G consistently leads to OOM kill\n- Starting with a limit of 18G leads to OOM kill after starting a query (RAM usage slowly creeps up to 18)\n- There is nothing else running on the node\n- Performance is not of great concern at the moment. I'd like to find a way to safely operate the instance with limits applied.\n- Collection config is attached.\n\nAny ideas? Did i misconfigure anything?"
  },
  {
    "threadId": "1244582905032736788",
    "name": "[Solved] Weird vectors amount in database accross time",
    "messages": "Hi,\nOver the last few weeks I've noticed a strange fluctuation in the quantity of vectors retrieved via the \"collections_vector_total\" exposed variable.\n\nIt's worth noting that I'm continuously adding documents to the Qdrant database and deleting very few (adding several hundred documents every hour VS deleting 1 to 3 documents at the same time).\n\nCan this fluctuation be explained by the duplication of points on multiple segments?"
  },
  {
    "threadId": "1244032182650015924",
    "name": "Llama index VectorStoreIndex from Qdrant",
    "messages": "Hi! I am trying to create a VectorStoreIndex from Qdrant collection but not from the entire collection but just filtered out points (filtered according to doc_ids). \n\nHere is how I used to (successfully) create the VectorStoreIndex from the entire collection:\n`vector_store = QdrantVectorStore(client=self.qdrant_client, collection_name=self.qdrant_collection)\nself.index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n`\n\nI found online VectorStoreQuery, and _build_query_filter options and but they have very little documentation so I am not sure how to implement them. Here is what I tried but it is still using the entire collection not just given doc_ids:\n`vector_store = QdrantVectorStore(client=self.qdrant_client, collection_name=self.qdrant_collection)\n            doc_filter = VectorStoreQuery(doc_ids = self.context_files) # Filter collection for only chosen docs (context_files)\n            filter_docs = vector_store._build_query_filter(doc_filter)\n            self.index = VectorStoreIndex.from_vector_store(vector_store=vector_store, filter = filter_docs)`\n\nHas anyone done anything similar before or has any advice how to make this work? \n\nThank you! \n\nTo reiterate, here is my goal (not sure if it's an appropriate approach; would appreciate any advice):\nI want to have 1 collection with multiple documents saved. Then I want to create VectorStoreIndex from just some (not all) documents (according to given ids) at a time for further use."
  },
  {
    "threadId": "1244021114171162658",
    "name": "How to connect qdrant with javascript in react vite app",
    "messages": "I am tring to connect to the qdrant instance app but get the  following \n```\nAccess to fetch at 'https://9ba9ef57-705b-47c9-...etc' from origin 'http://localhost:5173' has been blocked by CORS policy: Response to preflight request doesn't pass access control check: No 'Access-Control-Allow-Origin' header is present on the requested resource. If an opaque response serves your needs, set the request's mode to 'no-cors' to fetch the resource with CORS disabled.\n```"
  },
  {
    "threadId": "1243809108835176539",
    "name": "Why does PointID not support arbitrary strings, only support uuid?",
    "messages": "What considerations is this design based on?"
  },
  {
    "threadId": "1243646380010897418",
    "name": "RAM usage far exceeds expected RAM usage",
    "messages": "I'm using hybrid cloud and trying to create a dataset of 40 Million 512 dimension vectors and Splade Vectors. Following the calculator, it said I needed  122GB of RAM for all the vectors. I created a Qdrant cluster of 4 128 GB ram boxes, and am using Binary Quantization. \n\nI had it run overnight but qdrant stopped at around the 10 million vectors. Looking at the UI all of the clusters seem to be completely full. The optimizer status says\n```json\n{\n\"error\":\"optimizations pending, awaiting update operation\"\n}\n```"
  },
  {
    "threadId": "1243614949046227024",
    "name": "Embedding are modified when send in qdrant",
    "messages": "Hello all!\n\nWhen I create an embedding with FastEmbed the first vector begin like this : \n\n```\n[-4.36788192e-04, -2.08982788e-02,  3.51007446e-04,  1.80549175e-02,\n        3.83424200e-03,...\n```\n\nbut when I print from qdrant I get this :\n```\n[-0.0004367882,-0.020898279,0.00035100745,0.018054917,0.003834242,-....\n```\n\nWeird ins't it? \nHere is the qdrant function :\n```py\ndef create_embeddings(data, filename):\n    embedding = TextEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n    embeddings = list(embedding.passage_embed(data['lemmatize_words'], axis=1))\n    client = QdrantClient(host=\"localhost\", port=6333)\n\n    try: \n        test_collection = client.create_collection(\n            collection_name = filename,\n            vectors_config = VectorParams(size=768, distance=Distance.COSINE)\n        )\n    except Exception as e :\n        print(e)\n\n    payload = []\n    # Upload des embeddings dans la collection\n    point_ids = list(range(len(embeddings)))\n    upload_result = client.upsert(\n        collection_name = filename,\n        points=[\n            {\n                'id': point_id,\n                'vector': embedding.tolist(),\n                'payload': {} \n            }\n            for point_id, embedding in zip(point_ids, embeddings)\n        ]\n    )\n```\n\nto create my training embedding I only use the first 2 lines (and take data from the same database to avoid any mistake):\n```\nembedding = TextEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n    embeddings = list(embedding.passage_embed(data['lemmatize_words'], axis=1))\n```\nI don't knwo what to to to change it an I trained my model with the first embedding so performances are worse with my new embeddings."
  },
  {
    "threadId": "1243580297119797361",
    "name": "Are there any recommended tools or services for simplifying the data ingestion process when setting",
    "messages": "I'm looking to streamline the process of loading data into Qdrant for a RAG (Retrieval-Augmented Generation) setup. The steps of data loading, chunking, and choosing the right embedding model can be time-consuming and complex. Are there any online services or products that could help automate or simplify these steps? My goal is to get started with Qdrant and RAG quickly and efficiently. Any suggestions or recommendations from the community would be greatly appreciated!\n\nOnce the setup works well enough, I'd like to convert it into a data pipeline with airbyte or airflow so that I can make sure the data stays updated"
  },
  {
    "threadId": "1217709490506305566",
    "name": "I'm trying to understand the memory footprint of my collections.",
    "messages": "I'd would love to be able to hit an endpoint and get the collection size in term of ram and disk usage.We are trying to tune our settings to tradeoff speed for cost (memory), We currently run out the cloud offering so our visibility is limited to what you expose.\n\nMy goal is to test different configurations and find out their impact on our resource utilization. Also I'll be running some latency tests to ensure that we don't pay too big of a tax.\n\nWe have vectors and payload indexes and will be testing different ways to move data between disk and memory as well as quantization."
  },
  {
    "threadId": "1240641344150568970",
    "name": "ColBERTv2  Support",
    "messages": "When will Qdrant have a release with full support for multi vector Colbertv2? What's the timeline for this? https://github.com/qdrant/qdrant/issues/3684 and what is possible in the meantime?"
  },
  {
    "threadId": "1242737579401678891",
    "name": "how to use query_filter",
    "messages": "Hello, I like to make use of query filters to add a keyword based approach on top of the vector search, I know its not the best idea of all but in my use case it does make some sense....\n\nI did create the data in qdrant using llama_index the data is indexed as documents. So now when I add any query_filters I do get non results when using the must filter or I get all the data when using the must_not filter. This shows me I do something very wrong. The filter has not the effect I would expect. When using must_not it brings back data where that words are still in.\n\nWould you mind to share an example of how to do a must/must_not filter on data that has been indexed using llama_index?\n\nMy guess is I'm pointing the filter to the wrong field to be filtered on, but llama_index does not show any examples of how to do so except when using the query_engine. I'm doing a direct search from the qdrant client as this also allows for paged results, which the query_engine does not support.\n\nit looks like this:\n        vector = embed_model.get_text_embedding(query)\n        result = qdrant_client.search(collection_name='someCollection',\n                                            query_vector=vector,\n                                            limit=limit,\n                                            offset=(offset+1)*limit,\n                                            query_filter= filters\n                                            )\n\nThe filter would look like this:\n\n        for word in must_words.split(','):\n            must_filter.append(FieldCondition(\n                key=\"text\",\n                match=MatchText(\n                    text=f\"{word}\"\n                )\n            ))\n\nI use the key text as I think that's where the text gets stored in by llama_index. but I got a strong feeling that's not how it will work as it just does not work ЁЯЩВ\n\nAny advice is highly welcome ЁЯЩВ"
  },
  {
    "threadId": "1243203098697273446",
    "name": "docker vs docker compose - docummentation storage mis-match",
    "messages": "Hello,\nI am looking at the documentation for docker/docker compose deployments in https://qdrant.tech/documentation/guides/installation/#docker . I want to create a volume for the container inside docker compose. But I have noticed a mismatch between the volume data storage location in the documentation mentioned above.\n\nIn docker it shows to create a volume to the containers \"/qdrant/storage\" directory.\nBut in the docker compose example it shows to create a volume to the \"/qdrant_data\".\n\nIs it the one or the other? Or maybe it is docker compose dependent?"
  },
  {
    "threadId": "1243184866233942083",
    "name": "Keyword vs Integer filtering performance",
    "messages": "Hello,\ncoming from ElasticSearch, I was wondering whether creating payload index with keywords would be faster for filtering than integer index (lookup=true, range=false). My main goal is to filter data using \"match any\".\n\nThe filtered field will contain an array of integers = [1, 2, 3, 4 ...]. On this field I want to use match any clause with array of integers as well. So it is a lookup, and I do not need range queries. But coming from ElasticSearch background, where keywords are much faster than integers (thanks to lucene). I was wondering whether should I convert the integers to string keywords, in case it is faster here as well."
  },
  {
    "threadId": "1242370031304572969",
    "name": "Memory is increased for every search and not released",
    "messages": "Hi team,\nIтАЩm using the AsyncQdrantClient which connects to remote instance and I face memory increase of around 0.2MB after each time search method was executed in a machine that is sending request (not in a machine that holds Qdrant node). When significant amount of requests are executed it results in a huge increase in memory consumption which is not released automatically neither using garbage collector. I believe the most probable reason could be some caching of requests. Do you know how to avoid such memory consumption or how to release this reserved memory?\n\nPython 3.11, qdrant-client 1.9.1"
  },
  {
    "threadId": "1243137575435374652",
    "name": "performance of qdrant on reindexing when vector is added.",
    "messages": "Hi all, I am trying to build a image retrival system(face's) using qdrant, i went through documentation, i couldnt find  any benchmarking on how fast can qdrant index newly added vectors, i know its. performance on retrival even for million vectors is veryfast and accurate but  lets say i want to keep adding vectors, will that slowdown the system ?"
  },
  {
    "threadId": "1243127168855183441",
    "name": "UnexpectedResponse: Unexpected Response: 403 (Forbidden)",
    "messages": "i obtained API key and cluster URL from your cloud and tried running this code:\nclient = qdrant_client.QdrantClient(api_key=qdrant_api_key, url=\"https://9cb22f65-fdbf-4713-914e-7091b3346815.us-east4-0.gcp.cloud.qdrant.io\")\n\nvector_store = QdrantVectorStore(client=client, collection_name='qdrant_rag')\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\nindex = VectorStoreIndex.from_documents(documents=llama_parse_documents, storage_context=storage_context, show_progress=True)\n\nthe code failed when reaching the last line (index= ...) with this error returned: \nUnexpectedResponse: Unexpected Response: 403 (Forbidden)\n\nwhen pasting the cluster URL in my browser i also get forbidden:\nhttps://9cb22f65-fdbf-4713-914e-7091b3346815.us-east4-0.gcp.cloud.qdrant.io\ni also tried: \nhttps://9cb22f65-fdbf-4713-914e-7091b3346815.us-east4-0.gcp.cloud.qdrant.io:6333\ni keep getting the fobidden error, whats wrong? thanks"
  },
  {
    "threadId": "1242772435657560135",
    "name": "How to use create_snapshot with wait=false?",
    "messages": "Hello, I would like to backup collections using create_snapshot. Because some collections are very large, I set wait to False. My question is: how do I know, when the snapshot is created and can be downloaded? Is there an example/tutorial on how to use create-snapshot with wait being set to False?"
  },
  {
    "threadId": "1242526032192344155",
    "name": "Support for RBAC JWT in Qdrant Cloud",
    "messages": "How do I find the config file, when I'm launching a cluster (free-tier) on the cloud, so as to enable RBAC JWT? Moreover is there a tool in the cloud UI to generate JWT tokens like there is in the local UI?"
  },
  {
    "threadId": "1242490408227836095",
    "name": "Collection Deletion",
    "messages": "Hi,\n\nDoes Qdrant delete all the points in collection if I delete collection itself ?  In my scenario, I added 785 points to the collection, later I deleted the collection and re-creating with the same name & configuration. I observed that the newly created collection(with same old name) still has the old records. Is it expected behavior? If so, how to remove all the previous points/records when re-recreating the collection ?\n\n**Qdrant Config**\n- Cluster mode with 3 nodes\n- V1.9.0\n- shard_number : 2\n- replication_factor : 2"
  },
  {
    "threadId": "1241888312718004297",
    "name": "Caching of Queries",
    "messages": "Hi team. Apart from page cache that's there for disk-based storage, is there any other caching strategy in place that optimises qdrant search performance. Does it support caching in memory based storage? Or does the qdrant search engine has any in-built cache storage?"
  },
  {
    "threadId": "1242376742434242560",
    "name": "How can i find the index for VectorIndexRetriever using the get_collection method",
    "messages": "I am using VectorIndexRetriever class import from llama_index.core.retrievers import VectorIndexRetriever"
  },
  {
    "threadId": "1242341855539105825",
    "name": "Corruption when upsert",
    "messages": "Hi everyone. When I upsert 100M vectors into Qdrant, it has 2 errors in each time when I upsert.\nThe first error is:\n\"Server internal error: Failed to flush_id tracker versions: Service runtime error: RocksDB flushed_cf error: Corruption: Corrupted Key: Internal Key too small. Size=4. \"\n\nAnd the second error is:\n\"Service internal error: RocksDB error: RocksDB put_cf error: Corruption: Corrupted Key: '<redacted> seq: 38004619851137024, type:127\"\n\nI upsert 1000 vectors each time. Every I upsert 10M successfully, I stopped the process and waited for indexing done. However, after several time of upsert, it has error above.\n\nMy configs are:\nvector size 512\nvector distance: Dot\n[\"vectors\"][\"hnsw_config\"][\"m\"]: 32\n[\"vectors\"][\"hnsw_config\"][\"ef_construct\"]: 100\n[\"vectors\"][\"hnsw_config\"][\"full_scan_threshold\"]: 10\n[\"vectors\"][\"hnsw_config\"][\"on_disk\"]: true\n[\"vectors\"][\"on_disk\"]: true\non_disk_payload: true\n[\"hnsw_config\"][\"m\"]: 32\n[\"hnsw_config\"][\"ef_construct\"]: 100\n[\"hnsw_config\"][\"full_scan_threshold\"]: 10\nperformance max_search_threads 6\nhnsw_index max_indexing_threads 6\nAnd other configs are default.\n\nHow to upsert 100M vectoers (large dataset) successful ?"
  },
  {
    "threadId": "1230856406077407332",
    "name": "Count API with Vector Search",
    "messages": "I am currently implementing vector-search. For that I am using the `search` API  to return all points that exceed a certain `score_threshold`. I do not want to display all datapoints at once but implement a server-side pagination. I am aware of the `limit` and `offset` parameters, however, I do not know how many datapoints match my criteria. Therefore, I cannot compute the number of pages in advance. \nI am currently exploring the `count` API which I would like to call before the `search` API in order to know how many pages I need. However, I saw that the `count` API does not support a vector-based search. \nDo you have any ideas how I could build a vector-based search with server-side pagination?"
  },
  {
    "threadId": "1242062685202546718",
    "name": "Automatic disconnection from qdrant client",
    "messages": "Hi, I am using qdrant: 1.9.2 on my local host and with qdrant-client: 1.9.1\n\nAs soon as I try to insert data in to my database, the process gets killed and the connection is closed. Any idea why this is happening?"
  },
  {
    "threadId": "1241116307613679678",
    "name": "normalize the score for better sorting",
    "messages": "Hi,\nI have 2 collections, one collection with clip model verctors and one collection with text vectors.\nwhen I run semantic search i run it on both collections. I get the top 5 results from each collection.\n\nThe texts score are like:\n[\n    {\n        \"score\": 1.0000001\n    },\n    {\n        \"score\": 1.0000001\n    },\n    {\n        \"score\": 0.8582796\n    },\n    {\n        \"score\": 0.39315316\n    },\n    {\n        \"score\": 0.38655293\n    },\n    {\n        \"score\": 0.38118294\n    }\n]\n\nand the clip image result score are like:\n\n[\n    {\n        \"score\": 0.28405023\n    },\n    {\n        \"score\": 0.2815319\n    },\n    {\n        \"score\": 0.28059542\n    },\n    {\n        \"score\": 0.2805196\n    },\n    {\n        \"score\": 0.28016663\n    },\n    {\n        \"score\": 0.2787255\n    }\n]\n\nI want to join the arrays, but i can't really sort by score, can the database return the same range of scores so I can combine the array so it would be meaningful?\nThanks"
  },
  {
    "threadId": "1240358001077129246",
    "name": "How to specify efSearch parameter in query",
    "messages": "I have created a collection with M=32 and efConstruct=64 \n\ndef query_db(embedding, n_results=10):\n    return qdrant_client.search(\n        collection_name=collection_name,\n        query_vector=embedding,\n        limit=n_results\n    )\n\nI do not see any efSearch paramter in search api ? \nHow do I change efSearch while querying ? \nWhat is the default value ?"
  },
  {
    "threadId": "1241005039594508339",
    "name": "payload addition",
    "messages": "Hey all, I am trying to update my collection and I am facing an issue with payload. I have a dictionary eg:\n{'page_content': ' some text',\n  'metadata': {,\n   'page': 1,\n   }}\n\nI am trying to add this using the update_collection method but it throws an error :\nInput should be a valid dictionary [type=dict_type, input_value='page_content', input_type=str]\n\nwhen I convert the dictionary to json using json.dumps, it throws an error:\nInput should be a valid dictionary [type=dict_type, input_value='{', input_type=str]\n\nAny help on how should I add the payload?"
  },
  {
    "threadId": "1240906813050982462",
    "name": "update/delete points to corresponding embeddings",
    "messages": "I'm working on chunking the Confluence page into smaller segments, converting them into embeddings, and storing them in a Qdrant collection. \nif the Confluence page gets updated, I have identified the added and removed lines and I need assistance in subsequently removing the corresponding vectors or embeddings from the database."
  },
  {
    "threadId": "1240662522709938218",
    "name": "Help me with an example using BatchVectorStruct for upsert points using java client",
    "messages": "https://qdrant.github.io/qdrant/redoc/index.html#tag/points/operation/upsert_points I need an example of using java driver to leverage `BatchVectorStruct` for creating \"**named dense vectors**\". I tried to search the forums and discord, but couldn't find one and hence the reach out here. Thank you in advance!"
  },
  {
    "threadId": "1239891791214284800",
    "name": "distributed Qdrant using docker compose",
    "messages": "hello everyone, I have a problem with distribution using docker compose. it seems like the nodes can't communicate properly. what's the solution? I have attached the error below and my docker compose.\n\ni'm following docker compose from this repo https://github.com/qdrant/demo-distributed-deployment-docker and i create collection like the image i attach. did i do it wrong ?"
  },
  {
    "threadId": "1230517323496951889",
    "name": "Max supported throughput on upsert",
    "messages": "I have been experimenting with the max throughput with which I can upload points to the qdrant cluster.\nFor the sake of experiment:\nI have a 15 node cluster - 64G nodes on qdrant cloud\n\nCluster config: \n'{\n   \"name\":\"temp\",\n   \"vectors\":{\n      \"vector\":{\n         \"size\":768,\n         \"distance\":\"Cosine\"\n      }\n   },\n   \"on_disk_payload\":true,\n   \"hnsw_config\":{\n      \"on_disk\":true,\n      \"payload_m\": 16,\n      \"m\": 0\n   },\n   \"quantization_config\":{\n      \"product\": {\n            \"compression\": \"x16\",\n            \"always_ram\": true\n        }\n   },\n    \"optimizers_config\": {\n        \"memmap_threshold\": 20000,\n        \"indexing_threshold\": 20000\n   },\n   \"shard_number\": 30\n}'\n\nI am upserting a batch of 1500 points with payload (5 fields) at each request. (using REST API for upserts)\n\nI am able to hit 20-26 rps with latency close to 1000ms. Any suggestions on the cluster config or things i could try out to increase this throughput? \n\nWhile i understand upsert seems a computationally expensive operation at qdrant, Aim is to be able to hit close to 50rps for upserts."
  },
  {
    "threadId": "1240361921266450554",
    "name": "Sparse vectors similarity normalization",
    "messages": "when I use sparse vectors to for semantic similarity between texts using the search api of qdrant, the similarity between my points and the query ranges beyond the range from 0 to 1, the similarity score examples are 22.3 , 11.3 , 8.6. My application needs those score to range from 0 to 1 and I should normalize them upon the output scores it self. I have checked what metric is used in calculating similarity and I found that it is just dot product with no normalization. Is their a solution ?"
  },
  {
    "threadId": "1240287563105964123",
    "name": "\"error\":\" Wrong input: Vector params for  are not specified in config\"",
    "messages": "I get an error with this QA Rag Pipeline. Anyone see the bug?\n\n\nclient = QdrantClient(\n    url=userdata.get('QDRANT_URL'),\n    api_key=userdata.get('QDRANT_API_KEY')\n)\n\nclient.create_collection(\n    collection_name=\"attention\",\n    vectors_config={\n        \"content\": VectorParams(size=1536, distance=Distance.COSINE),\n    }\n    )\n\nembedding = OpenAIEmbeddings(openai_api_key=userdata.get('OPENAI_API_KEY'))\n\ndef add_data_to_collection(data, client=client, collection_name=\"attention\"):\n\n    chunked_metadata = []\n\n    for item in data:\n\n\n      id = str(uuid4())\n      content = item.page_content\n      source = item.metadata[\"source\"]\n      page = item.metadata[\"page\"]\n\n      content_vector = embedding.embed_documents([content])[0]\n      vector_dict = {\"content\": content_vector}\n\n      payload = {\n            \"id\": id,\n            \"content\": content,     \n            \"source\": source, \n            \"page\": page,\n            }\n\n      metadata = PointStruct(id=id, vector=vector_dict, payload=payload)\n      chunked_metadata.append(metadata)\n\n\n    client.upsert(\n                  collection_name=collection_name,\n                  wait=True,\n                  points=chunked_metadata)\n\nvectorstore = Qdrant(client=client, \n                     collection_name=\"attention\",\n                     embeddings=embedding,\n                     vector_name=\"content\")\n\nllm = ChatOpenAI(temperature=0.0, \n                 model=\"gpt-4o\", \n                 max_tokens=512, \n                 openai_api_key=userdata.get('OPENAI_API_KEY'))\n\n# Retrieval qa chain\nqa_db = RetrievalQA.from_chain_type(\n                                    llm=llm,\n                                    chain_type=\"stuff\",\n                                    callbacks=[handler],\n                                    retriever=retriever)\n\nqa_db.invoke({\"query\": \"What is the attention mechanism?\"})"
  },
  {
    "threadId": "1240057602482765824",
    "name": "How to turn off query parallelism",
    "messages": "qudrant is using parallelism in query. I think the parallelism is dpendent on number of cpu ? Is there anyway we can set the parallelism to 1 . I tried by chagning max_search_threads=1  in config.yml . \n\nHowever the query is still using all 8 cpus\n\n\nservice:\n  http_port: 6333\n  grpc_port: 6334\nstorage:\n  path: \"./storage\"\n  wal_path: \"./storage/wal\"\nlog_level: \"warn\"\nservice:\n  max_search_threads: 1\ntelemetry:\n  enabled: true"
  },
  {
    "threadId": "1239881511851917342",
    "name": "Points lost after trying to configure multitenancy",
    "messages": "Hello,\n\nI am currently trying to configure multitenancy on qdrant with docker image and python client on v1.9.1. I am trying to transfer the data from my 100+ collections to a single collection (\"collection_main\"). In order to do so, I am using the python client's `scroll` to get Records, added in `group-id` to payload, and converting the Records received to Points using the `PointStruct` model. Next, I use `upload_records` to upload to \"collection_main\", with ` parallel=4, max_retries=3` . Throughout the process, I keep track of the number of points at the following stages: 1) Initial collection count (using `count` method with `exact=true`), 2) After converting to `PointStruct`, 3) Checking total number of points from specific `group_id` after upload, all of which tally. However, when I run a separate script (using `count` with `exact=true`) to count the points immediately after the initial script which does the data transfer, I notice data loss within some collections, with some collections having 0 points. \n\nI have ran this script multiple times and noticed that the same few collections face the issue with the same number of points lost. \n\nFor further context, I am running qdrant on a local docker container and using a local directory as the persistent store. Additionally, these collections were initially created in version 1.6.1 (which might explain the need to convert from `Record` to `PointStruct` despite the deprecation of `Record`. And before running the data transfer script, I have progressively mounted the local directory to the intermitent docker container versions (1.6.1 -> 1.7.2 -> 1.8.0 -> 1.9.0 -> 1.9.1).\n\nIs there any reason why this may be happening? Do let me know if any more information is required.\n\nThanks in advanced for any advice!"
  },
  {
    "threadId": "1239872105181478983",
    "name": "Flowise and qdrant",
    "messages": "hello, i use qdrant with flowise and ollama (llama3). i have an error when i want save vector data.\nError: vectorsService.upsertVector - Error: Error: 400 Bad Request: Wrong input: Vector inserting error: expected dim: 1536, got 4096\n[00:40]\ndo you have an idea of the problem ?"
  },
  {
    "threadId": "1239984893002514615",
    "name": "Geting Unexpected Response: 502",
    "messages": "All, \n   I am exploring qdrant and I am trying to see how it performs. My plan is to load 1M sift vectors. \n\nBuilt qdrant on my host.  Run the server on port 80. \n\n/qdrant --config-path config/config.yaml\n\nVersion: 1.9.2, build: 34f7f8ec\nAccess web UI at http://localhost/dashboard\n\nTrying following  client \n\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import Distance, VectorParams\nfrom qdrant_client.http.models import PointStruct\nimport time\n\nqdrant_client = QdrantClient(url=\"http://localhost:80\")\n\ntry:\n    # Get and print the list of collections\n    collections =  qdrant_client.get_collections()\n    #print(\"Collections:\", collections)\nexcept Exception as e:\n    print(\"error:\", e)\n\nError:\n\nerror: Unexpected Response: 502 (cannotconnect)\nRaw response content:\nb'<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">/n<html>\\n<!-- FileName: index.html\\n     Language: [en]\\n-->\\n<!--Head-->\\n<head> ...'"
  },
  {
    "threadId": "1239881102647099463",
    "name": "Fetching files (loading vectors each time by upload_collection)",
    "messages": "Hello folks!\n\nI have a question. Even though I give the \"cache_dir\" parameter to qdrant client's \"set_model\", it does \"fetching files\" every time qdrant's \"upload_collection\" is called (loading data into a collection piece by piece) and the vector is extracted. It was fixed by turning off the \"download_model\" function  and give a \"model_dir\" with PosixPath while debugging. It didn't do fetching every time.\n\npython: v3.11.7\nqdrant-client: v1.9.1\nfastembed: v0.2.7\n\n        EMBEDDING_MODEL_NAME = os.environ.get(\"EMBEDDING_MODEL_NAME\", \"BAAI/bge-small-en-v1.5\")\n        EMBEDDING_CACHE_DIR = os.environ.get(\"EMBEDDING_CACHE_DIR\", \".../.caches/\") (like that)\n\n        model_description = self._get_model_description(model_name)\n        cache_dir = define_cache_dir(cache_dir)\n        model_dir = self.download_model(\n            model_description,\n            cache_dir,  \n            local_files_only=self._local_files_only\n        )\n        self.load_onnx_model(\n            model_dir=model_dir,\n            model_file=model_description[\"model_file\"],\n            threads=threads,\n            providers=providers,\n        )\n\nWhen each piece is loaded, it prints them on the screen and this seems like a waste of time if I'm not wrong (I shortened it): \n\nFetching 5 files: 100%|\nтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ5/5[00:00<00:00,63743.22it/s]\n\nFetching 5 files: 100%|\nтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ5/5[00:00<00:00, 70611.18it/s]|\n\nFetching 5 files: 100%|\nтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ5/5[00:00<00:00, 26990.37it/s]\n\nFetching 5 files: 100%|\nтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ5/5[00:00<00:00, 12694.62it/s]| \n\nWhat am I missing? Is everything normal? Otherwise how can i fix this problem? If you need detailed information, please a me. Thank you very much ЁЯЩВ <@354387036666462210> fyi"
  },
  {
    "threadId": "1239958285835309157",
    "name": "Qdrant Fastembed",
    "messages": "Is Qdrant fastembed library compatible with llama 3 or Phi 3 and openai llms? or the embed vectors supports all?"
  },
  {
    "threadId": "1239943152035692617",
    "name": "Hierarchical Search",
    "messages": "Is there a way to search the data using hierarchy?\nFor example, I have documents that have relationships so they could be structured in a tree-like format.\n\nWhen I divided the chunks, I mapped the parent chunks with child ones. I also have versions of the documents, so I want to map it as well. When the user sends a query, I want to be able to search a subset of nodes (chunks) using vector search. And this subset is reached using metadata filters. Is there a way to do it using Qdrant?"
  },
  {
    "threadId": "1239905032737001532",
    "name": "Help needed configuring Hybrid Cloud (ingress/service with NodePort))",
    "messages": "Hi,\n\nI have run through the default base setup for Hybrid Cloud in my Kubernetes cluster.\n\nI have run the generated setup scripts for kubectl and helm\n\nI want to enable external ingress, trypically we do this using a service with NodePort and and Ingress (I am used to doing this via helm chart values for other things)\n\nFor the operator with Hybrid cloud I see we can override some values as shown in https://qdrant.tech/documentation/hybrid-cloud/operator-configuration/\n\nI am confused what the exact syntax is for enabling the NodePort on the service and the Ingress via that set of values in the operator config.\n\n1. Where can I find the operator charts source code- is it on github somewhere\n2. Am I able to configure what I need via the operator values modified in the console - or do I need to deploy my own manifest files to get this working?"
  },
  {
    "threadId": "1239900245383708684",
    "name": "How do I see my quantized vectors alongside the original Vectors?",
    "messages": "I used this code to create a collection with scalar quantization --\n\n```from qdrant_client import QdrantClient, models\n\nclient = QdrantClient(url=\"http://localhost:6333\")\n\nclient.create_collection(\n    collection_name=\"Joe_Biden\",\n    vectors_config=models.VectorParams(size=1024, distance=models.Distance.COSINE),\n    quantization_config=models.ScalarQuantization(\n        scalar=models.ScalarQuantizationConfig(\n            type=models.ScalarType.INT8,\n            quantile=0.99,\n            always_ram=True,\n        ),\n    ),\n)```\n\nThen I upserted some vectors into it\n\n```client.upsert(collection_name=\"Joe_Biden\",\npoints= models.Batch(ids= [1,2], vectors = doc_emb.tolist()))```\n\nBut when I do \n\n```client.scroll(collection_name=\"Joe_Biden\", with_vectors=True)```\n\nI only see the original vectors.  How do I see the quantized one as well?"
  },
  {
    "threadId": "1239506996663681054",
    "name": "Langchain with Qdrant Cloud",
    "messages": "New to RAG here. I have created a vectorstore pipeline using Qdrant.from_texts, running in memory locally. I am looking to store and load the vectorstore from Qdrant Cloud so that I don't have to recreate a vectorstore on every run, and persist on Cloud. How can I achieve this with creating a vectorstore once, and being able to load it in my chain?"
  },
  {
    "threadId": "1239870229186740244",
    "name": "Indexing on keywords vs integer lookup=true for Ids",
    "messages": "I am wondering if it is more memory efficient to store IDs as integer index with lookup=true, range=false. vs storing them as keywords? If so how are the integers stored in memory vs keywords for this case. Are they stored as 8byte uints?\nI'm thinking I can just convert these ids to integers by removing the first letter, which could cause it to be more memory efficient:\nids: \n[\n0:\"A5070919682\"\n1:\"A5059467218\"\n2:\"A5088819443\"\n3:\"A5060298854\"\n4:\"A5074476359\"\n]\n\nvs\n\n[\n0:5070919682\n1:5059467218\n2:5088819443\n3:5060298854\n4:5074476359\n]"
  },
  {
    "threadId": "1173914187584520232",
    "name": "Dashboard not working",
    "messages": "Hi!\nI am taking a look at Qdrant to be the vector database we choose in my company to work on moving forward and I have a question. I have installed it locally in my Macbook Pro M1 Max via source code compilation and installation. The database starts correctly but when I try to access the dashboard I get a 404 Not Found. All I can see related to the UI in the logs is the following:\n```\n2023-11-14T08:23:26.473954Z DEBUG qdrant: Waiting for thread web to finish\n2023-11-14T08:23:26.476359Z  WARN qdrant::actix: Static content folder for Web UI './static' does not exist```\nAny idea of why is it happening? Thanks!"
  },
  {
    "threadId": "1239694763418980514",
    "name": "How to recover for a 3 way replicated shard where all three replicas seem can't recover",
    "messages": "Qdrant version: v1.9.1\n\nIn my test qdrant cluster,  all three replicas of a shard seem to have gone down and cannot recover. One of them remains \"active\", but seems like an invariant that's enforced to ensure all replicas don't get marked as dead (looking at the code).\n\nFrom /{collection-name}/cluster\n```\n{\n  \"shard_id\": 1,\n  \"shard_key\": \"152153bf-0ca0-4ad7-b373-118c647510ee\",\n  \"peer_id\": 2077220307586764,  // qdrant-19\n  \"state\": \"Dead\"\n},\n{\n  \"shard_id\": 1,\n  \"shard_key\": \"152153bf-0ca0-4ad7-b373-118c647510ee\",\n  \"peer_id\": 1109415645723081,  // qdrant-27\n  \"state\": \"Active\"  \n},\n{\n  \"shard_id\": 1,\n  \"shard_key\": \"152153bf-0ca0-4ad7-b373-118c647510ee\",\n  \"peer_id\": 1365122472831091,  // qdrant-31\n  \"state\": \"Partial\"\n},\n```\n\nI mapped peer_ids to nodes via response from /cluster endpoint. The logs from the nodes all seem to be trying to stream shard data from nodes that are down and therefore failing. How can I override recovery to get these nodes running, although they'd lose any recent updates? Is there a workaround for this case?\n\nThank you!"
  },
  {
    "threadId": "1234592398819197029",
    "name": "Authentication with JWT",
    "messages": "Hi Team, I upgraded to v1.9.0 to enable JWT authentication. It's disabled on left pane but I can able to access it through URL. I am not if I am missing something here. \n\nservice:\n  api_key: <api-key>\n  jwt_rbac: true"
  },
  {
    "threadId": "1235564117360906281",
    "name": "Delete (drop) vector index",
    "messages": "Hi everyone!\nHow to delete vector index that is already exist? I tried https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/delete_field_index endpoint with \"vectors\" as `field_name` but it didn't delete the index"
  },
  {
    "threadId": "1239526541600489532",
    "name": "points/search panicked - range end index out of range",
    "messages": "Hello,\n\nToday using Qdrant's Haystack integration I've been trying to query my collection with a new filter.\nI received the following error from the Qdrant container (attached log). Any idea where it could comes from ?\n```\n    \n2024-05-13T10:31:46.370746Z ERROR qdrant::startup: Panic occurred in file lib/segment/src/index/hnsw_index/graph_links.rs at line 460: range end index 530398396284928 out of range for slice of length 823090    \n2024-05-13T10:31:46.752885Z  WARN qdrant::actix::helpers: error processing request: 1 of 1 read operations failed:\n   Service internal error: task 614 panicked    \n2024-05-13T10:31:46.753009Z  INFO actix_web::middleware::logger: 172.18.0.4 \"POST /collections/pythia-bm25/points/search HTTP/1.1\" 500 126 \"-\" \"python-httpx/0.27.0\" 0.383909    \n```\nI can try to count the number of doc matching the filter maybe to see if it's ok, but it's weird. It's not that big.\nThe total collection is 653 144 points_count with 1 300 918 vectors (sparse+dense). The filter should be only a small (10%) subset of this. My VM is 2vCPU+8go RAM with ram optimisation (on disk vectors/payload, only int8 in memory)"
  },
  {
    "threadId": "1239069341715140679",
    "name": "Smart way to prevent duplicates",
    "messages": "Hey! \n\nI am building a RecSys pipeline, where as new content comes for a given user, it is inserted into a vector db. It might happen that the same piece of content comes in for multiple users. \n\nEach piece of content has a uuid. \n\nWhat would be the smartest way to prevent duplicates in Qdrant? \n\nSince I am generating chunks for each content, I cannot set the id to the content_uuid. \n\nHow would you prevent the same piece of content to being processed again? Do I need to retrieve all ids in the database (I have a field content_uuid)? \n\nAlso if I batch upload and the \"id\" of one of the newly inserted points is already in the database. Will this crash the entire batch or will this recognize the duplicate point and avoid it? Then I could figure out a way to hash chunks and use this as id."
  },
  {
    "threadId": "1239067306118611017",
    "name": "How to find payload indexes in a given collection ?",
    "messages": "I'm new to Qdrant DB. I need to know how can I find the payload indexes that are present in a collection and how many points are present in a given index"
  },
  {
    "threadId": "1238890069071237160",
    "name": "hybrid search not found",
    "messages": "i have this error \n```vValueError: Hybrid search is not enabled. Please build the query with `enable_hybrid=True` in the constructor.````\n\nits curious because i have \n\n``` retriever = VectorIndexRetriever(\n        index=index,\n        similarity_top_k=top_k,\n        sparse_top_k=10,\n        vector_store_query_mode=\"hybrid\",\n        # streaming=True,\n    )````\n\nand create vector i have also ```def create_vector_store(documents: list, collection_name: str) -> QdrantVectorStore:\n    vector_store = QdrantVectorStore(\n        client=client,\n        aclient=aclient,\n        collection_name=collection_name,\n        enable_hybride=True,\n        batch_size=20,\n    )\n    logger.success(\"Vector store created\")\n\n    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n    index = VectorStoreIndex.from_documents(\n        documents,\n        storage_context=storage_context,\n    )\n    logger.success(\"index created\")\n    return index```"
  },
  {
    "threadId": "1238099034736951316",
    "name": "Filtering out based on the results of main filters",
    "messages": "Hello dear support team of Qdrant, \nI hope you are all doing well. \nI am having a specific use case in filtering, which I do not have a clue on how to handle it. Maybe Qdrant has something to handle this, or maybe you could give me some tips. \nSo, this is the scenario:\nI have some filters for category_id, and gender_id. This part has no problems, I will just use `scroll` api alongside with `scroll_filters` to retrieve the products in this criteria. Also, keep in mind that I am using `limit` and `offset` to handle pagination in qdrant side. \nOne extra filter that I am having problem with, is that, there is a field named `group_id` in products. I want to get only one product of each `group_id`, so if scroll is going to return two or more products having the same `group_id`, it shouldn't. It should return only one of them (does not matter which one).\nOne easy way is to just remove those after getting the product list from `scroll` api, but as I mentioned before, I am using qdrant for pagination. So, this would mess up the pagination by potentially changing the number of final products by eliminating some of them. So, is there any way to handle this in the scroll api?"
  },
  {
    "threadId": "1236065223387906130",
    "name": "DEADLINE_EXCEEDED during upsert points operation",
    "messages": "Hi there! I'm using java driver `1.9.0` against Qdrant free tier to being developing. And when executing upsert points operation, I get this error,\n```\nio.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.894704011s. Name resolution delay 0.085216232 seconds. [closed=[], open=[[buffered_nanos=366183167, buffered_nanos=483288, remote_addr=ded78a51-8370-47d8-adb0-6147f0fcbba2.us-east4-0.gcp.cloud.qdrant.io/35.245.15.233:6334]]]\n    at io.grpc.Status.asRuntimeException(Status.java:537)\n    at io.grpc.stub.ClientCalls$UnaryStreamToFuture.onClose(ClientCalls.java:538)\n    at io.grpc.internal.DelayedClientCall$DelayedListener$3.run(DelayedClientCall.java:489)\n    at io.grpc.internal.DelayedClientCall$DelayedListener.delayOrExecute(DelayedClientCall.java:453)\n    at io.grpc.internal.DelayedClientCall$DelayedListener.onClose(DelayedClientCall.java:486)\n    at io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:574)\n    at io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:72)\n    at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:742)\n    at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:723)\n    at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n    at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n    at java.base/java.lang.Thread.run(Thread.java:1583)\n    5429 ERROR [grpc-default-executor-11] QdrantClient Upsert operation failed\n```\n\nWhat am I missing here? Any help is appreciated - Thank you!"
  },
  {
    "threadId": "1238141057342902272",
    "name": "Query planning",
    "messages": "I am getting slightly different results between search and search_batch with the python client over one single query input, is this possible/expected given that the query planning is decided at run time and may be somewhat different between the two, maybe using different distance metrics (if I understand correctly)?"
  },
  {
    "threadId": "1238373227525574686",
    "name": "Qdrant client timing out on rapid I/O",
    "messages": "Context:\nWe've a qdrant server running on AWS k8's with 6 collections.\n4 collections are very small (below 40k points each) with vector indexing.\nThe other 2 collections are huge. One collection has 20+ million and the other has 1+ million points.\nAnd we keep adding points at a rapid pace and also search for duplicates. So both read and write operations are happening concurrently.\nBoth these collections are indexed on both payload and vectors with the config shown in the attached screenshot.\nThese are running on a disk with 10000 IOPS.\n\nProblem:\nWe noticed that most of the calls to service are `The read operation timed out`  .\nAt first, we thought it might be due to slower IOPS. We used to have 3000 IOPS, increased it to 10000 IOPS now.\nAnother conjecture we've is that , most of the timeouts are happening when the status of teh collection is \"yellow\".\nI assume that the collection is getting optimized when in status \"yellow\" (may be indexing on the new items added).\nIs there a workaround that I can use my search during \"yellow\" status without timing out?\n<#1149327864936808529>"
  },
  {
    "threadId": "1238213468818112584",
    "name": "search on small datasets",
    "messages": "Hey all, with a small enough collection, is calculating cosine similarity (assuming you set up your collection as such) between your input query and each node in your collection effectively the same as running HNSW?"
  },
  {
    "threadId": "1232354587852148797",
    "name": "The Read Operation Timeout : Error",
    "messages": "within 8 hours is my project evaluation, and i am getting this error, but as i do a page referesh it start working, and suddenly atio the process and give this error, i dont know why.\n\nError : \nreturn self.api_client.request(\n  File \"D:\\Practice\\server\\myenv\\lib\\site-packages\\qdrant_client\\http\\api_client.py\", line 76, in request\n    return self.send(request, type_)\n  File \"D:\\Practice\\server\\myenv\\lib\\site-packages\\qdrant_client\\http\\api_client.py\", line 93, in send\n    response = self.middleware(request, self.send_inner)\n  File \"D:\\Practice\\server\\myenv\\lib\\site-packages\\qdrant_client\\http\\api_client.py\", line 202, in __call__\n    return call_next(request)\n  File \"D:\\Practice\\server\\myenv\\lib\\site-packages\\qdrant_client\\http\\api_client.py\", line 105, in send_inner\n    raise ResponseHandlingException(e)\nqdrant_client.http.exceptions.ResponseHandlingException: The read operation timed out"
  },
  {
    "threadId": "1230571603591561277",
    "name": "Hybrid search",
    "messages": "Hi, I am trying to implement Hybrid search in qdrant, I have posts from a social network I am building and I am trying to build a post recommendation system based on dense and sparse embeddings, the issue is the in the qdrant documentation I see that the only way to create a hybrid query  is to batch search the dense embeddings and sparse embeddings and only after to merge them. the issue is that with this approach I rarely have overlapping results that I can weight properly. it is basically separated queries and I loose the benefits of the hybrid vectors. \nis there other way to query hybrid embeddings?"
  },
  {
    "threadId": "1238086869116260412",
    "name": "DSPy: what is the embedding format?",
    "messages": "In the official example (https://qdrant.tech/documentation/frameworks/dspy/), it's retrieving \"the top-k passages that match a given query\", but there's no text embedding model to convert the question and search in Qdrant. So, how does it work?"
  },
  {
    "threadId": "1237729002706178048",
    "name": "Does updating few vectors and their payloads in a collection recreates the HNSW graph ?",
    "messages": "I am building a daily ingestion pipeline, where I ll be updating the vectors for given ids on daily basis. I am curious if a new ingestion/updation would recreate the HNSW graph ? If no, then how does it maintain that  high releavance during nearest neighbour search ?"
  },
  {
    "threadId": "1237709454972162058",
    "name": "I need to swap my subscription to another AWS Account using the same email",
    "messages": "I unfortunately setup Qdrant via AWS Marketplace while logged into the incorrect AWS Account.\nI cancelled the sub, and tried to set up a new sub in the correct account, but get an error when I tried to go through the setup, saying I need to log in via the correct billing account.\n\nIs it possible to wipe my account completely so I can set it up again via the correct AWS account using the same email address?\n\nI created a ticket with more details here \nhttps://support.qdrant.io/helpdesk/tickets/1778"
  },
  {
    "threadId": "1237994467475329044",
    "name": "Unable to find embedding vectors on my vector store",
    "messages": "I am able to search my vector store but unable to find the vectors so im questioning whether I've inserted them in.\n\nHere is two screenshots of dashboard attached. Can you tell me if im meant to see a vector of size 1024 here? Is it meant to be in variable embedding or down there where it says vector?\n\nAlso here is some code i ran to look at embeddings of a random vector and I got nothing.\n\nrandom_query_vector = np.random.rand(1024)  # Replace DIMENSION with the correct dimension\n    client = returnClient()\n    result = client.search(\n        collection_name=\"enigma-onedrive-admin-root\",\n        limit=1,\n        query_vector=random_query_vector.tolist(),\n    )\n    print(result[0].vector)"
  },
  {
    "threadId": "1237708943162347520",
    "name": "Help Needed: Persistent Storage with LangChain and Qdrant in Local Folders",
    "messages": "Can you please help me?┬а\nI spilled some PDFs using LangChain.┬а\nFor each PDF, I wanted to asynchronously add the PDF chunk to a collection named after the username, which will be saved in a local folder.┬а\nThen, after starting Chainlit, I want to check if the user-named collection exists in the folder. If it does, and it has any entry, then I want to use that collection.┬аIf not, I want to obtain PDFs from the user, split them, and add them to Qdrant disk storage."
  },
  {
    "threadId": "1237776823471837215",
    "name": "Multi vector + sparse vector rerank",
    "messages": "Do we have to manually do the reranking on the client side when we do a batch request with multiple vectors and sparse vectors?\n\nIs there any kind of reranking built into qdrant?  if not is it in the roadmap?"
  },
  {
    "threadId": "1237742878181167165",
    "name": "High memory alert email while Cluster in Dashboard shows low utilization",
    "messages": "I just received this email for our cluster. In the qdrant Dashboard I only see a utilisation of ca. 60%. Is there any to do here? I remember there were some \"false alarms\" some weeks ago.\n\nThanks"
  },
  {
    "threadId": "1234493043856379994",
    "name": "Distributed Qdrant behaviour",
    "messages": "Hi, I wanted to understand, can a distributed qdrant cluster have multiple primary nodes? If not, how can we make sure that a single primary node does not become single point of failure for entire cluster?"
  },
  {
    "threadId": "1248469103371620433",
    "name": "Accessing Qdrant on a remote machine",
    "messages": "Hi team, I am running Qdrant on a remote machine and have exposed it via Nginx via some domain name.\nWhen I do curl command to access the database from another machine it successfully happens.\n1. How to establish a connection with the database with python using Qdrant client using the API? So that i can use the python libraries as if i established it with localhost:6333 ?"
  },
  {
    "threadId": "1237646011476742197",
    "name": "Spark-Qdrant UUID",
    "messages": "Hi, \n\nI have a dataset that I'm using to create a collection on qdrant. The upload is through the spark-qdrant uuid. My id_field is a monotonically_increasing_id(). If I do not specify a id_field spark creates random uuid(). But I do not want that I want to use my id_field as the point id on the collection. But I get the below error. Can you please help me resolve it?\n\n**java.lang.IllegalArgumentException: Invalid UUID string : 28979650?**"
  },
  {
    "threadId": "1226942235400999045",
    "name": "Deadline exceeded on Upserts that don't set a deadline on the client request.",
    "messages": "I'm using the golang client to insert records in a qdrant cluster running on a kubernetes in a distributed deployment. My requests don't have a deadline set, and gRPC defaults to no deadline per their docs, but I'm seeing upsert requests timeout on healthcheck with the error below. Is there a way to control this timeout, say disable it / increase it? Also curious how healthchecks get issued? Does the client healthcheck the node before issuing the upsert request, or is this an internal request in the cluster?\n\nrpc error: code = DeadlineExceeded desc = Timeout: Timeout error: Deadline Exceeded: status: DeadlineExceeded, message: \"Healthcheck timeout 2000ms exceeded\", details: [], metadata: MetadataMap { headers: {} }\n\nThank you!"
  },
  {
    "threadId": "1236361130209972244",
    "name": "Python client: Using Scroll API with Payload Order By and Offset in Qdrant Python Client",
    "messages": "Hello all, I've encountered a scenario where I needed to apply sorting and pagination to my scroll requests. I found that I can use `order_by.start_from` on the Qdrant's UI, but I don't know how to apply that into my code (I am using python).\nPlease see the uploaded images for detail information. Thank you for help"
  },
  {
    "threadId": "1235840144821583892",
    "name": "How to integrate the Hybrid search with langchain and qdrant",
    "messages": "I have integrated the Hybrid search with langchina and qdrant db for the BM25 data getting stored inmemory, so i want replace that with vector db so may i know how is this possible to store the chunks for BM25 instead of Inmemory to vector db"
  },
  {
    "threadId": "1235583421334552647",
    "name": "Custom categories: Qdrant or not?",
    "messages": "I am facing a new challenge in our application, which is related to the custom categories, generated by our admins, to better provide categorized products for our users. I have concluded the details fully in this question: https://datascience.stackexchange.com/questions/128926/retrieving-products-based-on-pre-existing-filters-the-fastest-way-possible\n\nIf it is better to copy the question text in here, please say so. Thank you very much for your help and consideration."
  },
  {
    "threadId": "1237049014852128858",
    "name": "Merge multiple (disjoint) snapshots into one collection",
    "messages": "I have four snapshots of about 400k vectors each, which were created on separate machines, each running their own qdrant (in Docker, image `qdrant/qdrant:v1.7.4`). Now on a fifth machine, I would like to populate a collection with all vectors (roughly 4 x 400k = 1.6M).\nI moved all snapshots to my target qdrant Docker container and I can successfully restore a single snapshot using `curl -X PUT ...` and I see some 400k vectors in the qdrant dashboard info afterwards.\nBut when I restore the next snapshot, all data seems to be wiped and I only see the ~400k from the last restored snapshot. I tried all three settings for `priority` but the result was always the same. By the way, I think the documentation for priority (https://qdrant.tech/documentation/concepts/snapshots/#snapshot-priority) is not very clear.\n\nI was thinking, I could also restore my four snapshots into four different collections first and then copy all their vectors to a new collection, but I also don't see any way to do the last part. I can initialize a new collection from an existing one, but then I'm stuck at the same point as before.\n\nCan anyone recommend a way to go about this? Thank you!"
  },
  {
    "threadId": "1236669839137833011",
    "name": "Selective queries: Retrieve only a few fields, all with non-repeated condition",
    "messages": "There is this custom categories scenario which I am working on, which put simply, I need to retrieve some products based on some pre-defined filters. \nPreviously I discussed some of the aspects of this challenge, but now I am facing something new. \nSo, other than the product list itself, which I only need to use the scroll api with filters, I also need to extract the attributes of all the products that are in this custom category, and send them to client as well. \nIf I want to clarify more on that, imagine we have 10.000 candidate products that fits in the filters defined by the admin. e.g., \"women swim suits\". Now, for attribute filters, user should be able to filter based on size, color, material, and etc.  These filters should be inclusive of all th attributes of all the candidates of this specific custom category filter. But extracting them from all 10000 candidates every time the request is called is not the best solution. \n## What are my options?\nSo, one possible way to handle this is using redis cash (as the custom categories are limited, we can use redis and just update the filters to include newly added products periodically). But still, we would need to first retrieve all the possible candidates that fits in that custom category, and then process them in python to extract their attributes. \nSo, one may think that I have to just retrieve all the points at first, then separate their attributes. It may work, because I will give these to celery workers to run in a specified time in the background. But, can I just do something to only retrieve the needed attribute fields, and instead of repeating color=green for 3000 products, for example, just get color=green once, without the need to repeat it?\n\nSomething similar can be done in pg for example, using DISTINCT or something."
  },
  {
    "threadId": "1237259183452192799",
    "name": "Disk full issue",
    "messages": "Is it have another ways to increase disk without pay?"
  },
  {
    "threadId": "1237080835333685338",
    "name": "Qdrant panic error",
    "messages": "Im getting a panic error caused by the following code extract on my qdrant that is deployed on Google Compute Engine VM instance\n\n```\nquery_embedding = self._embed_model.get_query_embedding(query_str)\nquery_result = self._vector_store_client.search(\n            collection_name=self._collection_name,\n            query_vector=query_embedding,\n            limit=self._similarity_top_k,\n            with_vectors=True,\n            with_payload=True\n        )\n```\nThe qdrant deployed on the Vm uses a collection that is restored from a snapshot:-\n\n1.  The snapshot was created by going to the dashboard on my local machine -> snapshot, then creating and downloading the snapshot.\n2. The snapshot was then uploaded to Google Cloud Storage via the gcloud storage cp command and down downloaded into my VM instance\n3. The snapshot was then restored using the following code\n\n```\nnode_url = VECTOR_STORE_SERVER_PATH\nlocal_snapshot_path = [\"/home/user/site_insights/backend/FULL_SCRAPE_22042024-529262110188319-2024-05-04-10-22-47.snapshot\"]\nfor snapshot_path in local_snapshot_path:\n    snapshot_name = os.path.basename(snapshot_path)\n    requests.post(\n        f\"{node_url}/collections/{VECTOR_COLLECTION_NAME}/snapshots/upload?priority=snapshot\",\n        files={\"snapshot\": (snapshot_name, open(snapshot_path, \"rb\"))},\n    )\n\n```\n\nThe error is attached as a file"
  },
  {
    "threadId": "1235834854466392114",
    "name": "Qdrant-Spark write jobs fail",
    "messages": "Hi All,\n\nI have around 20 jobs that writes data from databricks to qdrant, parallely. I keep running into an error that's not very informative.\n\nERROR: **ResponseHandlingException: Timed Out**\n\nHowever, if I run these jobs one by one I'm not getting the error. Is there a work around for this?"
  },
  {
    "threadId": "1234831750539247721",
    "name": "Multitenancy with sub indexing",
    "messages": "Hi Team, i am looking for advice on an indexing strategy to get the best possible performance for my vector search. The documents only mention a single index for multitenancy. I am not sure what the right setup is for multi indexes.\n\nI have two collections. One to store messages  from chats of a bot (for long term memory) and one for documents (for RAG). \nMy data  is structured as shown in the little image.  A user can create multiple bots with bots.id. Each bot has its messages indexed in the messages collection. A user can also have multiple documents which are indexed in a documents collection.\n\nAs mentioned in the docs the indexing can become a bottleneck for multitenancy if all indexes are store in a global index. My current setup for the messages is as followed\n\n```\n// messages payload\n{\n  messageId: id,\n  content: content,\n  user_id: uid,\n  bot_id: botId,\n}\n\n//global index\nclient.createCollection(\"messages\", {\n ....\n  hnsw_config: {\n    m: 16,\n    ef_construct: 128,\n    payload_m: 0,\n    on_disk: false,\n  },\n});\n//index for bot_id\nclient.createPayloadIndex(\"messages\", {\n  field_name: bot_id,\n  field_schema: keyword,\n});\n```\n\nwhen searching for vectors the application will only look for the vectors associated with one bot at a time.\n\n```\nconst filterGlobal = {\n  must: [\n    {\n      key: \"bot_id\",\n      match: {\n        value: \"botId\",\n      },\n    },\n  ],\n};\n```\n\n\n90%  of the time the query will be limited to one bot at a time  (if searching multiple bots  their ids will always be known and passed in as filters) so  i think this  setup is not optimal? \nSo is it better to drop the global index and create sub indexes for each bot?"
  },
  {
    "threadId": "1236218017210433546",
    "name": "Vanna collection",
    "messages": "Hello, i'm using vanna with qdrant\ni have many collection in qdrant and in want to know how can i use qdrant with vanna to train read different database but not in the same collection to be rag more precise\nand i suspect their is some mistake in rag qdrant / vanna because when i'm training all ddl go to documentation, and i don't provide documentation \ni could give more details after\ni use the code provide by wanna and them tell me is from qdrant dev team"
  },
  {
    "threadId": "1236249916175814747",
    "name": "Combine Full-Text Search with Semantic Search",
    "messages": "Hello,\nI think about combining semantic search with a full-text search. I noticed that the `search` method has two parameters: `query_vector` and `query_filter`. As far as I know, it works as an \"and\"-condition. So the data is first filtererd by `query_filter` and then ranked using the `query_vector`. \nIs there a way to use it in an \"or\"-condition: So the points are returned that were either found by the query vector or the full-text filter? Or do I have to use the `scroll` method first for the full-text search and combine the results from the `search` function?\nMany thanks for any advice how to impement such as system."
  },
  {
    "threadId": "1236238897852190791",
    "name": "vector.on_disk vs optimizer.memmap_threshold_kb",
    "messages": "Hey everyone,\n\nI found these two parameters for keeping vectors on disk. But the doc has some contradictions regarding how these both work. On one hand, it says that memmap_threshold_kb must be activate to enable storing vectors on disk. But on the otherhand, if `on_disk` is configured in the vector, the doc says that it will be stored on disk. \n\nSo do they both need to be set? Or is it sufficient to only set `on_disk`?\n\nThanks!"
  },
  {
    "threadId": "1209414347227660369",
    "name": "Creating a snapshot for restoring in another server",
    "messages": "Hi,\nFirst, Thanks for the amazing product!\nI am using Qdrant Docker for my application. I have deployed a docker container in an ec2 instance(dev server) . Now I am trying to build my application in a new ec2 instance (think stage server), in this I want the same data and embeddings I've created in dev server. I want to restore the snapshot in my stg server.\nJust like how I can take a backup .tar or .sql file and restore it in another server, I want to be able to restore my qdrant data in the same way. How can I acheive this?"
  },
  {
    "threadId": "1235972119985131711",
    "name": "Qdrant Search",
    "messages": "Hello !\nI have 3 different collections in qdrant db is there any way that I specify more than one collection in qdarnt search I am using sparse search\n   search_result = connection.search(\n        collection_name=\"jira-data\",\n        query_vector = embeddings,\n        score_threshold = 0.5,\n        limit = 3,\n    )"
  },
  {
    "threadId": "1235938898669076490",
    "name": "Embeddings and QDrant for Image Quality Control and Labelling",
    "messages": "So we deploy models on Jetson Orins at the edge, there are several overlapping features I want to implement, and I could do with knowing where QDrant fits and where it doesn't. So I'll outline the very rough architecture I'm currently picturing. \n\nOne part is a scatter plot visualization of all of our images, I don't see much need for QDrant to be involved here, in my mind it's just for Engineering to decide how to store it to make it easy to integrate into our UI. I'll run some dimensionality reduction like UMAP on the edge device, and we'll upload that straight to the Database. \n\nBut then I'd also plan to store raw, larger embeddings in QDrant, we'll use it to classify / alert for images with poor quality (a variety of different core issues), and to find more images that look similar to something that's caused a model error. \n\nAny thoughts at all? \n\nFor the time-being we'll just be using image level embeddings, but down the line we'll add object level embeddings.\n\n<@844295650400534599> ?"
  },
  {
    "threadId": "1235964544594083945",
    "name": "Partition by payload",
    "messages": "Hello,\nI've seen that you can partition by payload, but can you take a payload that is a keyword list as a value for partitioning? \n\nexemple of my payload:\n```\npayload: {\n  idContent: 1,\n  channelIds: [1, 2], I \n  geographies: [],\n  insights: [],\n  companies: [],\n  sectors: []\n}\n```\n\nAnd my creation of collection:\n```ts\n  await client.createCollection(QDRAND_COLLECTION, {\n    vectors: { size: 256, distance: 'Cosine' },\n    hnsw_config: {\n      payload_m: 16,\n      m: 0,\n    },\n  });\n\n  client.createPayloadIndex(QDRAND_COLLECTION, {\n    field_name: 'channelIds',\n    field_schema: 'keyword',\n  });\n```"
  },
  {
    "threadId": "1235673367177068725",
    "name": "Qdrant Migration",
    "messages": "We have currently deployed Qdrant v1.8.1 with 3 nodes, which has around 5 collections and each of them has 100k chunks. When I did helm upgrade to Qdrant v1.9.0, it was successfully recreated new deployment with Qdrant version 1.9.0 with all the collections, which is a very good feature. \n\nMy question is how does helm upgradtion happens on the Qdrant ? My all collections are in-memory. When helm upgrade happens, lets say first it creates new pod (pod-qdrant-3) on new cluster and deletes pod-qdrant-3 from existing the cluster. But hoe does it copies collections from deployment to new deployment which are in-memory ? If there is any detailed blog or some materials on this topic will be a great help to understand in a better way. Thanks you !!"
  },
  {
    "threadId": "1235615423626481787",
    "name": "package repo for binary",
    "messages": "Dear team,\n\nWe want to use qdrant binary for every release. Currently getting it grom github, but want to automate the process. Do you guys have package repository like jfrog, which we can mirror with our internal repo and pull down latest versions? Really appreciate the help"
  },
  {
    "threadId": "1235578162986090527",
    "name": "Qdrant for searches in multiple vector fields",
    "messages": "Hi everyone!\n\nI discovered Qdrant recently during a vector database research and found the company transparency incredible along with everything you've already built on the platform! Congratulations to the team and everyone involved!!\n\nI'm looking for help to understand if it would be possible to use Qdrant for my use case. I have a search engine to find restaurants, bars, pubs and establishments in general, all in standardized JSON format. We do not vectorize all data into a single vector, we separate it by different data formats such as location, opening hours, location features, reviews, etc. We believe that the ideal option for our search is the possibility of searching in multiple vectors to find the location that has the best score among all the fields searched. Is it possible to build this solution on your platform?\n\nThanks!"
  },
  {
    "threadId": "1235576803821748304",
    "name": "Modifying the Score Function to Consider Publication Date",
    "messages": "Hello,\nI'd like to know if anyone knows of a way to modify the score function to include a publication date in the payload, to create a bias between score and publication date."
  },
  {
    "threadId": "1235480629844508712",
    "name": "Is there support for read and write only nodes?",
    "messages": "We are running few models which require real time updates. Usually the CPU of this cluster is >10% but during indexing time the CPU spikes to ~60%. No of points in the collection is in the range of 300-500k only.\nWe want to route these updates to write only nodes and replicate to replica nodes eventually to minimize latency impact on serving/read nodes."
  },
  {
    "threadId": "1235150073684426864",
    "name": "#storing anything in RAM Qdrant ?",
    "messages": "Hey IтАЩm building a project by using qdrant storing 50M above vectors in it I have deployed qdrant on AKS azure cloud but that Node of AKS memory going up and chowking system even I did payload_on_disk true and on_disk index is true so need help"
  },
  {
    "threadId": "1234694103938367569",
    "name": "Panic after a restart",
    "messages": "On v1.8.4, see panic below after restarting . Any ideas?\n\nCall stack attached"
  },
  {
    "threadId": "1234885146365136998",
    "name": "Automatic Vector encoding in rust",
    "messages": "Beginner Question:   I was following this article https://qdrant.tech/documentation/tutorials/search-beginners/\nI was able to get it working with python, I wold like to use it with rust for python there is encoder library is there anything similar to that in rust"
  },
  {
    "threadId": "1234866117428645999",
    "name": "Query multiple documents in the same collection",
    "messages": "Hello, I am starting to use Qdrant. I have code to process multiple documents and import the vector data into Qdrant. Sometimes, I found that when I query documents in the same collection, I did not get all answers that I need. For example, if I have several documents describe project A. When I query \"could you describe project A from all documents\". The results I get is only project A description from one document, missing description of project A from other documents. If I create one collection for each document, and query the same question from each of the collections, it works. But this is not what I want. In such a case, I have to post process all the answers from different collections. Is this a limitation in Qdrant or I can fix this problem by setting some parameters?"
  },
  {
    "threadId": "1234826896944926750",
    "name": "How to determine how relevant each of the returned vectors actually are",
    "messages": "I'd like to start a discussion on advanced techniques how to make the final determination on if each of the returned vectors are sufficiently relevant to the search query. I do notice that there is a similarity score returned with each vector but in practise trying to use this to filter the results seems to not work very well. Perhaps my technique/understanding/reasoning is flawed for using this property altho it seems that something more advanced might be required. \n\nWhat is your experience and do you have any advice?"
  },
  {
    "threadId": "1234663462592974858",
    "name": "embedding models for Binary Quantization",
    "messages": "Which embedding models can be used. I read BQ only works effectively if the embedding model has a centered distribution."
  },
  {
    "threadId": "1234403379992723487",
    "name": "Qdrant's hybrid-search backward compatibility",
    "messages": "Hi, we are using Qdrant as vector database as part of a RAG system using langroid. We wanted to add qdrant's SPLADE-based sparse retrieval ((ref)[https://qdrant.tech/articles/sparse-vectors/]).\n\nAs I understood, the only solution to add it is using each vector as a named vector. So, if we want to use dense+sparse retrieval for a collection, we have to add both dense and sparse as named vectors.\n\nThere is an issue of backward compatibility for our use case since we want to use a common code for retrieval from both dense and hybrid collections. If we already have a dense collection (& that's not a named vector), we cannot use the same retrieval code. We wanted to know if there is a workaround to use sparse retrieval along with being compatible for default dense-vector collections.\n\nYou can checked the PR I raised to langroid for adding sparse retrieval.\nPR: https://github.com/langroid/langroid/pull/450"
  },
  {
    "threadId": "1234513033909243914",
    "name": "Distinct Documents Without Vector",
    "messages": "I know there is scroll for vector-less search/filter and group for grouping but that requires a vector array.\n\nHow would I do a distinct search on a field e.g. file_name, without providing a vector?"
  },
  {
    "threadId": "1234486214346670140",
    "name": "Upsert Qdrant infos",
    "messages": "Hey ! Quick one can we have the info about how much data was insert / updated / ignored during an upsert ?"
  },
  {
    "threadId": "1234001282860322836",
    "name": "Search + Filtration alongside pagination",
    "messages": "I am using Qdrant + FastAPI, and I have a growing database of products, each containing 60 different fields. I have previously written recommendations api using Qdrant, and handled pagination in Qdrant side using `limit` and `offset` parameters. \nThe usage scenario is fairly straightforward in `recommend`; but there are some challenges in `search` api that I am not able to think of any solution.\nFirst of all, the `filter` api:\nSo when user searches for some query, I first embedd his query text, and use it's vector to call Qdrant search function; then I use a re-ranker model to evaluate similarity between the original query and the first-hand candidates returned by Qdrant search to furthor improve the similarity score (Although only 72 items of the first 1000 items (`limit` param) is sent to the re-ranker model). \n```\npayloads = search.search_query(\n    search_lang=search_lang,\n    search_query=translated_query,\n    limit=QDRANT_RESULT_LIMIT,\n)\ndata = search.rerank_results(\n  primary_results=data, search_query=translated_query\n)\n```\nThen after this, I run a helper function on products to extract filters. Filters, are a set of values extracted for different attributes, like color, material, price range, source country, and etc. that are used to filter the results after the search. \n```\nfunctions.get_or_extract_filters(\n    item_list=data[:250],\n    search_query=search_query,\n    search_lang=search_lang,\n    country=country,\n    currency=currency,\n)\n```\nAfter this, I save the filters on redis cache, and use the combination of `query + country + language + currency` as it's key, and also return the filters to the client side. Note that I have to generate the filters based on the search results and they are different based on the products list returned in search.\nNow that the user has completed first step of the search, he/she can filter the results more regarding their likings."
  },
  {
    "threadId": "1234432206013861934",
    "name": "Image Pair Data Loader",
    "messages": "Hi All!\n\nJust getting started on Quaterion and am in need of a little advice for my data loader. \nI want to fine tune an image embedding model for similarity search. \nMy training dataset consists of image pairs rather than groups. \n\nMy plan is to: \n* Use triplet loss\n* Apply augmentation to the anchor \n* Provide the positive example \n* Negative would be random example from the other known positives\n\nDoes this sound like a reasonable approach? Any advice on how to write my data loader?\n\nAppreciate any advice."
  },
  {
    "threadId": "1233361450437836861",
    "name": "Wait for indexing of small update to index",
    "messages": "Hi,\nIn the change logs of version 1.9.0, it is stated that \"Remove vectors_count from collection info because it is unreliable\".\nI have been using `vectors_count` to check if the indexing of the new vectors I added has been finished on top of the status (GREEN, YELLOW) of the collection.\nThe reason I have been using `vectors_count` is because from time to time I have a very small update to the index that will not trigger the indexing because it is smaller than the default `indexing_threshold`. So basically if I see that the status is green but `vectors_count` >  `indexed_vectors_count`,  I temporarly change the `indexing_threshold` to force the indexing.\n\nWith `vectors_count` gone/not being reliabale, anyone has an alternative or a good strategy for this case?\nIs setting `indexing_threshold` at basically 1 all the time viable?\n\nNote that I cannot afford not to index those new vectors until new ones come in so that the default `indexing_threshold` is trigerred."
  },
  {
    "threadId": "1234143164776976405",
    "name": "Qdrant takes long time to run concurrent requests",
    "messages": "So, previously I have asked a similar question, but back then I did not have async or connection pool or something like this. Question link in SO is (https://stackoverflow.com/questions/78326924/qdrant-takes-25-second-to-retrieve-1000-single-products).\nBut now, I have implemented FastAPI, Async functions, and AsyncQdrantClient alongside with connection pooling using httpx `Limits`. But still, results are more or less the same. \nSo, let's see what I have done here:\n## 1-connection\nFirst of all, I want to create a client:\n```\nlimits = httpx.Limits(max_keepalive_connections=5, max_connections=10)\nqdrant_client = AsyncQdrantClient(\n    host=kwargs.get(\"host\", \"192.168.1.122\"),\n    port=kwargs.get(\"port\", 6333),\n    timeout=kwargs.get(\"timeout\", 100000),\n    # limits=limits,\n)\n```\nNote: adding or ignoring `limits` had no effect in the result. \n\n## 2-API setup\nI am using FastAPI with the following setup:\n```\nsearch_app = FastAPI()\n@search_app.on_event(\"startup\")\nasync def startup():\n    async_qd_client = qd_funcs.async_qdrant_client()\n    search_app.state.async_qd_client = async_qd_client\n```\nthen the api itself:\n```\n@search_app.get(\"/v1/test/{pk}/\")\nasync def test(pk: int, request=Request):\n    async_qd_client = search_app.state.async_qd_client\n    products = await qd_funcs.get_single_product_similars(\n        product_id=pk,\n        lang_int=1,\n        page=1,\n        per_page=1,\n        qd_client=async_qd_client,\n    )\n    return {\"product\": products}\n```"
  },
  {
    "threadId": "1244694691098660934",
    "name": "Using binary for desktop application",
    "messages": "I want to use Qdrant as a vector database in my desktop application. I built Qdrant following the instructions on your website. Now I want to use it, but can't. I tried it:\n\n```\nfrom qdrant_client import QdrantClient\nfrom qdrant_client import models\n\nclient = QdrantClient('qdrant.exe', port=6334, prefer_grpc=True)\n\nclient.recreate_collection(\n    collection_name='test',\n    vector_config=models.VectorParams(size=1024, distance=models.Distance.COSINE)\n)\n```\n\nIt throws an error: \n```\n_InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n  status = StatusCode.UNAVAILABLE\n  details = \"DNS resolution failed for qdrant.exe:6334: UNAVAILABLE: WSA Error\"\n  debug_error_string = \"UNKNOWN:DNS resolution failed for qdrant.exe:6334: UNAVAILABLE: WSA Error {created_time:\"2024-05-27T16:18:21.5949373+00:00\", grpc_status:14}\n```\n\nI think the path to binary must be another, but I don't know how to correct it properly."
  },
  {
    "threadId": "1244849001921253476",
    "name": "Hybrid Search",
    "messages": "Currently I am using qdrant, llamaindex, I want make hybrid search, after I load my files using llamaindex, I split them into nodes, and I want to add the nodes to qdrant, but I didn't find a clear documentation on how to upsert points with both sparse and dense vectors for one single node, I was also confused how to use the collection for later retrieval, hope there to be a documentation on this."
  },
  {
    "threadId": "1244926746797543506",
    "name": "cant parallelize upload_points in kubernetes",
    "messages": "Hi, I am trying to upload batch data to qdrant server using python client. Both python client and qdrant server are deployed in kubernetes. I am using upload_points function. The code works well if parallel argument is set to 1. However if paralle is more than 1 it results in a strange error related to multiprocessing.\nAdditionally this same code works fine if it is deployed as a standalone docker container  outside kubernetes.\nI would truly appreciate any help in this regard."
  },
  {
    "threadId": "1241025895704039606",
    "name": "Qdrant isnt handling fast uploading of points",
    "messages": "I'm using threading to upload my points faster in qdrant, but I think points are not being uploaded and there is no error as well, how can I implement threading in qdrant so it can take points quickly?\n\n\nThanks in advance!"
  },
  {
    "threadId": "1244948065085952051",
    "name": "Can't access data in qdrant collections",
    "messages": "Hello,\n\nSince this morning I've been unable to access my resources stored in qdrant. Either via the dashboard or the API. The list of collections is accessible via the dashboard but when I try to scroll on the points, I end up getting an error. \n\nI don't have much more information than that. It should be noted that I host my own instance of qdrant.\n\nI had this warning this morning :\n```\n2024-05-28T09:18:33.374796Z  WARN collection::collection_manager::holders::segment_holder: Trying to read-lock all collection segments is taking a long time. This could be a deadlock and may block new updates.\n```\nExcept this one there is only regular INFO logs.\n\nSometimes data becomes accessible again. Honestly I'm a bit lost ЁЯШж"
  },
  {
    "threadId": "1233828063939399791",
    "name": "Replace key name with other key.",
    "messages": "In my qdrant database there are 2 keys episodes, and episode , some data contains episode some contains episodes, I want all of them to become episode and standardize all of the database. If anyone has the idea please tell me."
  },
  {
    "threadId": "1233353583794196521",
    "name": "Qdrant filtration -> prioritize a condition, but not inforce it",
    "messages": "Hello dear Qdrant support team. There is a specific scenario in our filtration and search. So, we have a field in our payload named `source_country`:\n```\n{\n  ... other fields in payload,\n  \"source_country\": \n    {\n      \"id\": 45,\n      \"iso\": \"cn\", \n      \"name\": \"china\"\n    },\n  ... other fields in payload,\n}\n```\nWe also may have other source countries, like Turkey, etc. The filtration  that I want to add, is to retriece the china products at first, but also return others as well. I just want to prioritize the china source, without excluding other countries. \nThe reason I want to do this with Qdrant and not after retrieving the results, (which makes more sense to just re-order the results myself), is due to the limitation I am facing for pagination. I am using FastAPI with Qdrant, which forces me to handle pagination in Qdrant side using `offset` and `limit` keywords in `search` and `recommend` apis. So, I am only getting `limit` number of products each time, which is equal to `per_page` param sent by the client, which is 12 or 10 mostly. \nSo, re-ordering 12 products is not going to work here. \nIf you think my approach is wrong in other places, and I can do something else entirely to not face this prioritizing issue, please let me know. If more details is needed, just tell me what you need to know in order to offer help.\n\nP.S. I checked this `order_by` payload key doc in the scroll api, (I am not using scroll, but anyway), it says:\n\"When you use the order_by parameter, pagination is disabled.\"\n\nThanks in advance for your time and help."
  },
  {
    "threadId": "1233377237768867851",
    "name": "Can't restart Qdrant Cluster after out of memory issue",
    "messages": "I am facing an issue with my Qdrant Cluster. After encountering an out of memory (OOM) issue, I am unable to restart the Qdrant Cluster. When I try to restart the cluster, it fails to start up and I'm seeing error messages related to the OOM issue.\nAny help or advice on troubleshooting and resolving this problem would be greatly appreciated. Please let me know if you need any additional information from me."
  },
  {
    "threadId": "1233359068484079697",
    "name": "Pool of clients, does it make sense?",
    "messages": "My api has to work with different databases, which are PG (for retrieving data regarding user info, product source country, etc), Qdrant (which is my main product db, implemented to use semantic search and recommendation scenarios), MongoDB (which is used for logs) and Redis (which I use for caching results). \nMy first approach was to create async clients for each one, and share it through the api (which causes each api to create a new client, and close it when it is finished:\n```\n@rec_app.get(\"/v1/products/rec/\", status_code=200)\nasync def recommend(\n    request: Request,\n    product_id: int = None,\n    language: str = DEFAULT_SEARCH_LANG,\n    currency: str = DEFAULT_CURRENCY,\n    country: str = DEFAULT_COUNTRY,\n    category: int = None,\n    page: int = 1,\n    per_page: int = 10,\n    authorization: str = Header(None),\n):\n  # create an async mongoDB client to use in all the api\n    async_mongo_client = mongo_funcs.get_async_mongo_client()\n\n    # create an async qdrant client to use in all the api\n    async_qd_client = qd_funcs.async_qdrant_client()\n\n    # create an async redis client to use in all the api\n    async_redis_client = await redis_funcs.get_redis_client()\n\n    # Create an async PostgreSQL client to use in all API endpoints.\n    async_pg_conn = await product_funcs.pg_connect()\n\n... use the clients thorugh the api, and close them at the end:\nasync_mongo_client.close()\n    await async_qd_client.close()\n    await async_pg_conn.close()\n```\nBut then I did some more research, finding out that for pg for example, I can create async connection pool, and use it application level instead of creating single async connections in api level. \nI wonder if I can do something similar with qdrant client, as there will be a lot of users visiting these pages simultaneously, hence a lot of clients will be created."
  },
  {
    "threadId": "1232635710893326366",
    "name": "building index on array payload.",
    "messages": "My payload  contains array as field  .i.e. item_sub_ids = ['id1', 'id2', 'id3']. I want to build the index based on id values in the ids field to improve the search time but it remains the same even after indexing. Here I add the search query and indexings. \n\n**Search Query: **\ndef get_search_result(vector, item_id, collection_name=collection_name, \n                             no_of_vecs=50, with_vectors=False, params=None):\n \n    if not vector:\n        print(f\"invalid vector {vector}\")\n        return None\n    url = f\"{base_url}/collections/{collection_name}/points/search\"\n    data = {\n        \"vector\": vector,\n        \"limit\": no_of_vecs,\n        \"with_vectors\": with_vectors,\n        \"with_payload\": [\"pid\", 'item_sub_ids'],\n        \"filter\": {\n            \"must\": [\n                {\n                    \"key\": \"item_sub_ids\",\n                    \"match\": {\n                        \"value\": item_id\n                    }\n                }\n            ]\n        }\n        ,\n        \"params\": params\n    }\n    \n    resp = req.post(url=url, headers=headers, data=json.dumps(data))\n    return resp\n\n**Indexing**:\nurl=f\"{base_url}/collections/{collection_name}/index\"\ndata = {\n    \"field_name\": \"item_sub_ids\",\n    \"field_schema\": \"keyword\"\n}\nrequests.put(url=url, headers=headers, data=json.dumps(data)).json()"
  },
  {
    "threadId": "1233317569159495743",
    "name": "Pricing of Qdrant in AWS Marketplace",
    "messages": "What is per unit usage? I want to understand what unit refers to exactly?\n\nThanks"
  },
  {
    "threadId": "1232340736821821614",
    "name": "Checking on qdrant for running locally on Apple silicon",
    "messages": "I have an M2 Studio Max (96 GB integrated memory), and I'm wondering if qdrant is my solution for vector store on Apple silicon. I read an article that indicates that there are some ARM-based Docker images - which is great. I'm wondering if anyone has experience with them. This is for personal work, and it doesn't have to be so blazing fast that the queries are imperceptible. Stretch breaks are nice."
  },
  {
    "threadId": "1233070618983923732",
    "name": "Slow Search Speed",
    "messages": "Hi,\nWe have a database of around 80 million dense vectors. All 7 payload fields are indexed. (int/float/keyword fields only)\nSearch queries with a limit of 500 and a few filters take randomly between 5-30 seconds. \nWe have a fairly low read volume for these larger requests but a continuous stream of around 30/minute small searches.\nWe have a write rate of around 200 points/minute (batched)\nI have tried to improve the search speed with these params (returning one payload field and id only)\n ```\"search_params\": models.SearchParams(\n hnsw_ef=16,\n exact=False,\n quantization=models.QuantizationSearchParams(rescore=False),\n indexed_only=False,\n )```\nThe server specs are: \nr4.4xlarge - https://instances.vantage.sh/aws/ec2/r4.4xlarge\nCPUs 16\nMem(gb)    122.0\nI imagine there is a more optimal setup with more nodes/replicas/shard config?\nConfig:\n```{\n  \"params\": {\n    \"vectors\": {\n      \"\": {\n        \"size\": 768,\n        \"distance\": \"Cosine\",\n        \"hnsw_config\": {\n          \"m\": 16,\n          \"ef_construct\": 100,\n          \"full_scan_threshold\": 10000,\n          \"on_disk\": false\n        },\n        \"quantization_config\": {\n          \"scalar\": {\n            \"type\": \"int8\",\n            \"always_ram\": false\n          }\n        },\n        \"on_disk\": true\n      }\n    },\n    \"shard_number\": 4,\n    \"replication_factor\": 1,\n    \"write_consistency_factor\": 1,\n    \"on_disk_payload\": true\n  },\n  \"hnsw_config\": {\n    \"m\": 16,\n    \"ef_construct\": 100,\n    \"full_scan_threshold\": 10000,\n    \"max_indexing_threads\": 0,\n    \"on_disk\": false\n  },\n  \"optimizer_config\": {\n    \"deleted_threshold\": 0.2,\n    \"vacuum_min_vector_number\": 1000,\n    \"default_segment_number\": 0,\n    \"max_segment_size\": null,\n    \"memmap_threshold\": 20000,\n    \"indexing_threshold\": 50000,\n    \"flush_interval_sec\": 5,\n    \"max_optimization_threads\": null\n  },\n  \"wal_config\": {\n    \"wal_capacity_mb\": 32,\n    \"wal_segments_ahead\": 0\n  },\n  \"quantization_config\": null\n}```"
  },
  {
    "threadId": "1233075174153130014",
    "name": "Variables in Qdrant console",
    "messages": "How do I create variables in the console to consume them.\nEx.: collection_name = \"collection_test\"\n\nPOST collections/<collection_name >/points/scroll\n\nThank you in advance for your efforts to help."
  },
  {
    "threadId": "1232724635024625744",
    "name": "Hi, I'm starting with Qdrant today and need some java client examples",
    "messages": "I did look at https://github.com/qdrant/java-client but there is not enough examples there. When I try to add maven dependency of 1.9.0 in my project which is having JDK 21, I'm getting the following error and unsure what other dependencies are needed to be added in my project. I thought only the below is sufficient, but it looks like that's not the case,\n```\n        <dependency>\n            <groupId>io.qdrant</groupId>\n            <artifactId>client</artifactId>\n            <version>1.9.0</version>\n        </dependency>\n```\nAny help is appreciated! Thank you in advance!"
  },
  {
    "threadId": "1233033792592285739",
    "name": "ResponseHandlingException",
    "messages": "Hey i am running a cluster and having trouble inserting points into my vector store. It was working few days ago on a test cluster now its not and wonder if i misconfigured something. \n\nHere is how im initialising my client and vector store. I am using llamaindex for ingestion.\n\ndef returnClient():\n    return QdrantClient(\n        url=\"https://a9bf2042-7061-4e54-9e4b-561dfd41c4af.us-east-1-0.aws.cloud.qdrant.io\", \n        api_key=os.getenv(\"QDRANT_API_KEY\"),\n        port=6333\n    )\n\ndef returnVectorStore(collectionName):\n    client = returnClient()\n    return QdrantVectorStore(client=client, collection_name=collectionName)\n\n    vector_store = returnVectorStore(indexName)\n\npipeline = IngestionPipeline(\n        documents=documents,\n        transformations=[\n            SentenceSplitter(chunk_size=512, chunk_overlap=102),\n            Settings.embed_model\n        ],\n        vector_store=vector_store\n    )\n\n    pipeline.run(documents=documents, show_progress=True)"
  },
  {
    "threadId": "1232201799822213130",
    "name": "What's the recommended way to upgrade official Helm chart with minimal data loss",
    "messages": "Hi community,\nI'm using the offical Qdrant Helm chart from GitHub page and recently I plan to upgrade the qdrant version to leverage the performance boost.\nAs I'm cautious about the data, I planed to implement a green blue deployment - I created a new release of the latest Qdrant (Chart verion 0.8.4) and I copied data volume from the existing Qdrant(Chart veresion 0.2.10) to the override corresponding one.\ndata transferring was carried by https://github.com/utkuozdemir/pv-migrate\n\nThe new qdrant cluster managed to load collections but it failed consensus check. Please see the logs in the attachment.\nIn short it's \"2024-04-23T04:59:14.408019Z ERROR qdrant::startup: Panic occurred in file src/main.rs at line 307: Can't initialize consensus: Failed to recover Consensus from existing Raft state: Failed to recover from any known peers\"\n\nCan anyone suggest if it's expected error since I used the data from old Helm release for new ones?\n\nBesides, what's the recommended way to upgrade qdrant version / official qdrant Helm chart?"
  },
  {
    "threadId": "1232351605765181460",
    "name": "Supported dspy versions",
    "messages": "Hi guys, is someone using dspy here? \n\nbecause when you install dspy-ai\n\nit installs `dspy-ai version 2.4.5`\n\n\nbut when you install `pip install dspy-ai[qdrant]`\n  it downgrades dspy-ai to `2.0.4`\n and it raises error `ModuleNotFoundError: No module named 'dspy.retrieve.qdrant_rm'`\n\n\nany idea or clue which versions work together?"
  },
  {
    "threadId": "1229777476268458095",
    "name": "Point Sets as Versions",
    "messages": "Hey, short use case question:\nWe have a multi-tenant system which requires strong isolation, no downtime and optimize for cost efficieny. Tenants are everything but equally sized. Embedding models may be diverse but searches will be local to one embedding model.\n\nForm the docs I think a multi-node, single collection per embedding model setup is advised. We should then build a payload index over a tenant_id. Tenants will often have below 100k relevant dense vectors per search.\n\n**Now: We need to support multiple versions of VectorStores. In the common case, a set of vectors is extended by a small set of additions, deletions.** The customer needs to query both only the old version as well as only the new version. The trivial implementation would assign a versions: [] array string/int field to every Point and perform a MATCH ANY version_8978 clause in every search. \n-> Is this the best configuration for our use case? \n-> Are there known performance penalties for such a set membership pre-filtering?\n-> Do you have a better idea about how to encode versions? (e. g. since_version, until_version values, that at least reduce duplications for consecutivly included points per version?)"
  },
  {
    "threadId": "1219624192169869392",
    "name": "Problem with node",
    "messages": "We had a problem in a node (we have a master and another node) and no data replication. The extra node is not able to get consensus with the master, but the data is in the disk.  We also tried upgrading both nodes to 1.8.2 (before they were at 1.7.3) and it is throwing the following error:\n\nI attach the output of all errors and raft states.\n\nCan anyone help on how could we solve it?"
  },
  {
    "threadId": "1202894907098791936",
    "name": "Shard move error",
    "messages": "Im moving shared from one cluster node to another cluster not - Getting this error \"No space left on device (os error 28)\"\nIm running qdrant on docker.\nQuestion - \nIts transfer dependent on OS disk storage of source or destination.\n\n\nLogs\n2024-02-02T07:43:11.041818Z ERROR collection::shards::transfer::driver: Failed to transfer shard us_fto_collection_v7:2 -> 6881351561678924: Service internal error: Can't move file from /qdrant/./storage/tmp/us_fto_collection_v7-shard-2-2024-02-02-07-06-32.snapshot-6HFH3x to ./snapshots/us_fto_collection_v7/shards/2/collection_v7-shard-2-2024-02-02-07-06-32.snapshot due to No space left on device (os error 28)"
  },
  {
    "threadId": "1232041362602328084",
    "name": "what is the Maximum collection size can be in quadrant?",
    "messages": "what is the Maximum collection size in the quadrant that we can store?"
  },
  {
    "threadId": "1232005924638687242",
    "name": "How to avoid duplicate results when indexing news phrases by phrase?",
    "messages": "I'm facing an issue with indexing news phrases by phrase. The problem is that I have multiple phrases from the same news article, and when I search, I want to avoid having multiple results from the same article based on `idContent` who is in payload showing up separately. To solve this, I'm trying to merge the results with the same `idContent` together after search. However, this is causing an unexpected issue.\n\nWhen I request 30 results, I'm only getting 9 unique results because 21 of them are being merged with other news articles with the same idContent. Is there a way to avoid this reduction in results per page while still merging duplicate results from the same news article? Any help or suggestions would be greatly appreciated!"
  },
  {
    "threadId": "1231993829394223184",
    "name": "Copy vector from one collection to another collection",
    "messages": "Hello, anyone know if it's possible to copy vector from one collection to another ? \nFor example i'm trying to move a single file and it's associated vector from a collection to another collection. \nAfter a lot of time, It seem to me, i cannot do it without reencoding a second time my file.\nIs it possible without reencoding ?"
  },
  {
    "threadId": "1231949212208599040",
    "name": "Visualization fails on cloud console",
    "messages": "Something is tripping up the visualization process in the cloud dashboard for one of my collections:\n```\n{\n  \"limit\": 50, \n  \"vector_name\": \"vec_Message_huggingface_TaylorAI_bge-micro-v2_dim384\"\n}\n```\nI can consent to have someone look into the logs (https://d57afa86-62c8-4b17-9eb5-6143a248abb7.us-east4-0.gcp.cloud.qdrant.io:6333/dashboard#/collections/vdf_2024_9-11/visualize)"
  },
  {
    "threadId": "1229857226294825020",
    "name": "Shard per tenant  VS Partition (Payload filtering) per tenant",
    "messages": "Why would I want to use payload filtering over a shard per tenant? What are the semantics around shards? Wouldn't I want to restrict searches/writes by tenant? About about moving tenant with ever growing vectors to their own node? \nPlease go into the technical details behind why payload filtering is more suited than shards. \n\nBest, LIam"
  },
  {
    "threadId": "1217327445904986162",
    "name": "How to check Shards count and which Shard is on Which node",
    "messages": "I have qdrant running in AKS with helm.\nAKS has 2 Nodes but my 2STS(statefulsets) deployed automatically on same node.\nDevelopers pushed 1k collections with 10shards.\n1.Now we want to see shards count and which collection and Nodes have shards?\n2.in documentation mentioned that Node*2 Shards but where is it configured, how to check?\n3.i have only 2pods running on same node,what is the use of maintaining other node? Only for shards? Shards are in disk level or pod level or CPU/RAM?"
  },
  {
    "threadId": "1231796528377499720",
    "name": "Retrieve all points in collection",
    "messages": "I need to retrieve all of the points in my collection together with payloads and vectors. I've written something like this\n`all_points = qdrant_client.get_collection(collection_name).points_count\nrecords_qdrant = qdrant_client.scroll(\n    collection_name=collection_name,\n    scroll_filter=models.Filter(\n        must_not=[\n            models.IsEmptyCondition(is_empty=models.PayloadField(key=\"document\"),)\n        ]\n    ),\n    limit=all_points,\n    with_payload=True,\n    with_vectors=True,\n)`\nIt is working but feels a bit strange. Especially because of this dummy filter condition I added. Field \"document\" will always have value, so effectively this filter is always returning true.\nIs there better way to achieve the same result?"
  },
  {
    "threadId": "1231633658461556786",
    "name": "undo payload indexing",
    "messages": "I want to undo a payload key which I indexed earlier, cant figure out how to do that now ?\nhow should i undo payload indexing ??"
  },
  {
    "threadId": "1231077437278785636",
    "name": "Replicated cluster failing after losing single node",
    "messages": "ЁЯСЛ  \nWhat exactly are the guarantees of runing in cluster mode?\n\nFor context, we have 8 nodes. \n\nWe have one collection, 8 shards, with a replication factor of two.\n\n\nRecently we upgraded our kubernetes cluster in place, and it caused a catastrophic failure of qdrant.\n\nWe were only able to get back up and running by a complete fresh qdrant cluster reindexing from source.\n\n\nWe have a pod disruption budget as well. It appears to happen after the first node has been replaced, other nodes get into a crashloopbackoff. It's strange.\n\nI have a hunch this is related to the fact that we are storing the data on ephemeral storage now, and not a network connected ssd, because it was too slow.\n\nSo my question is - is qdrant _supposed_ to be able to recover shards from its peers? Or am I misunderstanding how qdrant in cluster mode is supposed to work?\n\n\nWhen we were using persistent volumes, we were able to delete a node and bring it back - but it seems when using local disk we cant currently. Curious as to why"
  },
  {
    "threadId": "1242449098116694096",
    "name": "Very long qdrant start",
    "messages": "After 1.9.2 update qdrant take more than 2 hours (actually not finish).\nLogs tell :\n```\n2024-05-21T09:48:33.053776Z  INFO collection::shards::local_shard: Recovering collection <name>: 0/1085 (0%)\n2024-05-21T09:48:33.053776Z  INFO collection::shards::local_shard: Recovering collection collection_plos_en_all-minilm-l6-v2_v1: 0/1085 (0%)\n2024-05-21T10:17:51.876113Z  INFO collection::shards::local_shard: 1/1085 (0%)\n2024-05-21T10:19:02.644289Z  INFO collection::shards::local_shard: 3/1085 (0%)\n2024-05-21T10:20:03.328713Z  INFO collection::shards::local_shard: 6/1085 (0%)\n2024-05-21T10:21:04.009752Z  INFO collection::shards::local_shard: 9/1085 (0%)\n...\n2024-05-21T12:01:29.160265Z  INFO collection::shards::local_shard: 306/1085 (28%)\n2024-05-21T12:02:32.097746Z  INFO collection::shards::local_shard: 309/1085 (28%)\n2024-05-21T12:03:34.816198Z  INFO collection::shards::local_shard: 312/1085 (28%)\n2024-05-21T12:04:36.118811Z  INFO collection::shards::local_shard: 315/1085 (29%)\n2024-05-21T12:05:37.550847Z  INFO collection::shards::local_shard: 318/1085 (29%)\n2024-05-21T12:06:39.112181Z  INFO collection::shards::local_shard: 321/1085 (29%)\n```\nHow can I speed up this ? And what happen ?\nThis collection got 1.2 Millions vectors."
  },
  {
    "threadId": "1231591873894813788",
    "name": "vector none",
    "messages": "I want to load a PDF file into Qdrant.\nSplit into pages and multiple chunks per page.\nThat works just fine with the client.add()  method.\nI can also create an embedding generator object.\nThe resulting Qdrant collection show  POINTS with the vector as None.\nWhat am I missing here?"
  },
  {
    "threadId": "1231587849757790289",
    "name": "uploading point on no existing collection",
    "messages": "when I add a point using collection name of which is not existing in the qdrant instance, it does not give me any error. Is there a way to handle this or should I do that on a seperate layer out of qdrant."
  },
  {
    "threadId": "1231526608985260122",
    "name": "Multiple vector search",
    "messages": "I have a collection with 200M points, each point has 2 types of vectors. I want to search with the first vector and get N points, then search with the second vector but only from the N points.\nIs there a way to do that? to create a filter based on a previous search?\nmaybe search with the first vector and get N point IDS and then make another search and filter for the N IDS?"
  },
  {
    "threadId": "1231319697496998009",
    "name": "Hybrid Search (NotImplementedError: Subclasses must implement this method)",
    "messages": "Hey Folks\nI am new to the vector db space. exploring qdrant\n\nI followed this article\nhttps://qdrant.tech/documentation/tutorials/hybrid-search-fastembed/\n\nto Create a Hybrid Search Service with Fastembed\n\nwhen i run this \n\n```client.add(\n    collection_name=\"startups\",\n    documents=documents,\n    metadata=metadata,\n    parallel=0,  # Use all available CPU cores to encode data. \n    # Requires wrapping code into if __name__ == '__main__' block\n)``` \n\nI am getting this error\n\n```NotImplementedError: Subclasses must implement this method``` \n\nPython Version: Python 3.11.8\nOS: Mac (M1 chip)\n\ncan anyone please advice on this   ЁЯШГ  \n\nThank you"
  },
  {
    "threadId": "1231302763686662186",
    "name": "how to seach subsequence in payload key in database ?",
    "messages": "So, I want to perform search based on subsequence in the database but I couldnt find any method, for ex: if the query text is \"nar\", then I want to filter all the points where \"certain_key\" has \"nar\" as its subsequence, How should I achieve that ?\nI want to use this feature in Auto searchComplete in the frontend side, shall I use text-indexing will it eat up my RAM ? I have 25K records, and counting, so I dont want to waste my RAM either.\n\nAny Help or Advice is appreciated."
  },
  {
    "threadId": "1231038537764306965",
    "name": "Regarding handling of duplicated vectors",
    "messages": "Hi!\nI've noticed this GitHub issue - https://github.com/qdrant/qdrant/issues/1788 regarding handling of duplicated vectors. I'd like to get better understanding what are current limitations and whether you have any experience with regards to any threshold when this issue might start being noticeable?\n\nI have DB of product reviews and I noticed there are instances where I'll get similar reviews, meaning that distance between both records will be 0.0. Those occur because reviewer just copy-pasted same text in multiple places.\n\nAt the moment it is really small subset of **0.4%** of total DB so I don't think I should be very concerned. But anyhow I'm asking just in case."
  },
  {
    "threadId": "1231148076258037791",
    "name": "Qdrant create_collection vs recreate_collection",
    "messages": "I want to create a collection if the collection does not exist. Out of the two methods mentioned which one does a check if a collection already exsist and if it does not exsist create one.\n\nI think it should be create_collection so i wanted to create one like below:\n\n`qdrant.create_collection(collection_name=collection_name,\n                                   vectors_config = models.VectorParams\n                                   (\n                                   size=model.get_sentence_embedding_dimension(),\n                                   distance=models.Distance.COSINE,\n                                    )\n                                    )`\n\nBut it give me an error:\n\n**create_collection = grpc.CreateCollection(\nTypeError: bad argument type for built-in operation**\n\nThe same parameters for recreate_collection works fine."
  },
  {
    "threadId": "1230493147562246155",
    "name": "What's the time complexity for the upsert and delete operation of a single point in Qdrant ?",
    "messages": "I am  curious what's the time complexity (average and worst case) for upsert and delete operation in Qdrant ? Is it same as the complexity of insertion, searching and deletion in Qdrant i.e long(n).  What are the best practices for performing these operations in Qdrant. Currently I am using the REST API for batch insertion and deletion."
  },
  {
    "threadId": "1230842329137418310",
    "name": "Hybrid Search with keywords",
    "messages": "Hi guys. I have followed an example of hybrid search using llama index and Qdrant vector store. I am using splade for sparse vectors and allmptnet for dense vectors. Although, I would like to have results that match for specific terms, such as IDs. \n\nFor example, if the user asks:\nWhen the product XYZ was inserted?\n\nI would like to have results only about XYZ. Now I cannot guarantee that XYZ is even on the retrievals. Is that a way to force that?"
  },
  {
    "threadId": "1230841583641956412",
    "name": "Document Hierarchy",
    "messages": "Hi guys. I have been working on a RAG project. I need some clarification on how I can structure my data (chunks) in a hierarchical way. I would like to have a tree of documents. For example, if I have documents the talk about Dogs and Cats, I would like to have leaf nodes (sub documents) for each breed of Dog and Cat. \nIf the user asks \"What is the oldest golden retriever alive?\", the system should retrieve only the chunks of golden retrievers, which are inside Dogs index. Is there a way to do that in Qdrant?\n\nThanks in advance"
  },
  {
    "threadId": "1230517852243628032",
    "name": "Aleph Alpha integration",
    "messages": "Hello, i am trying to integrate Aleph Alpha in google collab, and i am getting this issue: \n```\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-12-26efa69a3081> in <cell line: 18>()\n     28         }\n     29         query_request = SemanticEmbeddingRequest(**query_params)\n---> 30         query_response = await aa_client.semantic_embed(request=query_request, model=model)\n     31 \n     32         # Finally store the id, vector and the payload\n\n2 frames\n/usr/local/lib/python3.10/dist-packages/aleph_alpha_client/aleph_alpha_client.py in _raise_for_status(status_code, text)\n     81             raise BusyError(status_code, text)\n     82         else:\n---> 83             raise RuntimeError(status_code, text)\n     84 \n     85 \n\nRuntimeError: (403, '{\"error\":\"Access forbidden\",\"code\":\"UNAUTHORIZED\"}')\n```\ni first started by downloading and unzipping the dataset: \n```\n!wget http://images.cocodataset.org/zips/train2017.zip\n\n!unzip -q train2017.zip\n\n!ls train2017 | head\n```\nand this is the code i ran that gave me the error:"
  },
  {
    "threadId": "1230455651860484157",
    "name": "using API-key auth",
    "messages": "I am trying to add api-key to authunticate using qdrant instance, and this is what I did:\n- added this in the custom_confid.yaml outside the qdrant docker in config folder with the down text\n- added cert.pem and key.pem at tls folder outside qdrant docker\n- using the down command to start qdrant docker container [tls is false as I am trying first to use api-key then will try tls] [api-key is different from the one inside the client http request to see my request refused]\n- the problem is that when tls and https are false, there is no effect of using api-key at all. And when they are open it gives me errors like wronge version of ssl or illigal request\n```\ndocker run -p 6333:6333 \\\n    -v $(pwd)/qdrant_storage:/qdrant/storage:z \\\n    -v $(pwd)/tls:/qdrant/tls \\\n    -v $(pwd)/config/custom_config.yaml:/qdrant/config/custom_config.yaml \\\n    qdrant/qdrant \\\n    ./qdrant --config-path config/custom_config.yaml\n```\n```\nclient = QdrantClient(db_machine_ip,\n                       port=6333, \n                       api_key=\"12345\" , https=False)\n```\n \n\n```\nservice:\n  # Set an api-key.\n  # If set, all requests must include a header with the api-key.\n  api-key: 123456\n  #\n  # If you enable this you should also enable TLS.\n  # (Either above or via an external service like nginx.)\n  # Sending an api-key over an unencrypted channel is insecure.\n   # Enable HTTPS for the REST and gRPC API\n  enable_tls: false\n# Check user HTTPS client certificate against CA file specified in tls config\n  verify_https_client_certificate: false\n\n# TLS configuration.\n# Required if either service.enable_tls or cluster.p2p.enable_tls is true.\ntls:\n  # Server certificate chain file\n  cert: ./tls/cert.pem\n\n  # Server private key file\n  key: ./tls/key.pem\n\n```"
  },
  {
    "threadId": "1230399845676744805",
    "name": "Moving data after changing the embedding model",
    "messages": "Hi everyone! I hope that you're doing great!\nWe've switched from using an embedding model to another that has different victor dimensions. \nNow, we need to move the data from the collection with the previous vector dimension to a new one.\nWe're going to make a script that retrieves the points along with the payload, adds the point to a json file, embed the points again with the new model and then upsert them to the new collection and delete from the previous collection.\nDoes this sound good? Also, How do we retrieve all of the points? Couldn't find a start at and stop at in the docs"
  },
  {
    "threadId": "1230468490868031578",
    "name": "ID field with spark connector",
    "messages": "as you wrote here, it's should be possible to add id field with a simple string.\nI got error when I'm trying to do that. \nusing maven 2.2.0"
  },
  {
    "threadId": "1230424637129625672",
    "name": "error sparse vectors is none for hybrid search",
    "messages": "Hi ! I am following this article https://qdrant.tech/documentation/tutorials/hybrid-search-fastembed/ to implement hybrid search on Qdrant cloud. \n\nHowever, I am getting error \"sparse_vector_field_name in collection_info.config.params.sparse_vectors\" - Argument NoneType is not iterable. \n\nFetching 7 files: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 7/7 [00:00<00:00, 3276.80it/s]\nFetching 9 files: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 9/9 [00:00<?, ?it/s]\n  0%|          | 0/2 [00:00<?, ?it/s]exception occurred\nTraceback (most recent call last):\n  File \"\\hybrid_search_qd.py\", line 83, in add_data\n    self.qdrant_client.add(\n  File \"hybrid-search\\lib\\site-packages\\qdrant_client\\qdrant_fastembed.py\", line 496, in add\n    self._validate_collection_info(collection_info)\n  File \"hybrid-search\\lib\\site-packages\\qdrant_client\\qdrant_fastembed.py\", line 369, in _validate_collection_info\n    sparse_vector_field_name in collection_info.config.params.sparse_vectors\nTypeError: argument of type 'NoneType' is not iterable\nargument of type 'NoneType' is not iterable\n  0%|          | 0/2 [00:00<?, ?it/s]\n\nOn debug, i found that, in the function _validate_collection_info, collection_info.config.params.sparse_vectors is None.  I have called get_fastembed_sparse_vector_params in recreate_collection. Any idea?"
  },
  {
    "threadId": "1221804967803752478",
    "name": "Having one vector column for multiple text columns on Qdrant - V2",
    "messages": "I previously asked a question regarding \"Having one vector column for multiple fields\" asking if the method I tried is correct or not. (Please go to the link: https://stackoverflow.com/questions/78095982/having-one-vector-column-for-multiple-text-columns-on-qdrant). Now I have another idea thinking I can do this with separate columns and weight distribution. I do not know why, but your input is appreciated here:\nhttps://stackoverflow.com/a/78219166/4429265"
  },
  {
    "threadId": "1230226876912505024",
    "name": "Compressed file ended before the end-of-stream marker was reached",
    "messages": "I'm currently using Resume-Matcher, a project on github which uses the qdrant library, and I keep running into the same EOF error in fast_embed even after switching python versions and redownloading the necessary modules. Let me know if I should provide more details, and I'm using macOS (mac m1, 16 gb ram). I'm using Qdrant locally, using qdrant_client-1.8.2 on python 3.11. I've also tried this on python 3.9.6."
  },
  {
    "threadId": "1229854287514046465",
    "name": "Having issues with the deployment of Hybrid Cloud on GKE",
    "messages": "Hello.\n\nI was trying the new Hybrid Cloud approach.\n\nWas able to run all the installation comands.\n\nBut I get this error on the pod on Google Cloud - Back-off pulling image \"registry.cloud.qdrant.io/qdrant/qdrant-cloud-agent:1.0.0\": ImagePullBackOff\n\nWhat can I do?\n\nThank you"
  },
  {
    "threadId": "1240745137869553796",
    "name": "Qdrant compared to Elasticsearch?",
    "messages": "Hey all, I've been using Qdrant as a vector search engine for the objects in my application. I'd like to start doing keyword based matching, and I saw that Qdrant has some index'ing (keyword + full text) options with nested key support. I remembered Elasticsearch has those features as well, so I'm wondering if anyone has compared the two platforms? Besides RAG (which Qdrant vector search is great at), my other main use case is the following:\n\nGiven an object (with properties like `name`, `age`), find other objects with the same `name`."
  },
  {
    "threadId": "1240761710978138222",
    "name": "Collection have incompatible vector params?",
    "messages": "I trying to build my own RAG Chatbot using Flask, LangChain, and Qdrant.    I'd like to use Qdrant Cloud as my vectorstore.   I've gotten most of the initial prototyping done and I am testing locally but I am running into a problem with trying to upload documents to my Qdrant collection.  No matter what embeddings I use or how how I create a collection, I always run into this error when trying to pass in documents:\n\n\"Collection have incompatible vector params: size=384 distance=<Distance.COSINE: 'Cosine'> hnsw_config=None quantization_config=None on_disk=None datatype=None\"\n\nI can't tell if the issue is tied to my embeddings (FastEmbedEmbeddings) or how I am using the add method, but I'd appreciate any insight you folks can offer.   A simplified (character limits) version of my code is posted below.  I am using:\n\nPython 3.10.4\n\nName: qdrant-client\nVersion: 1.9.1\n\nName: langchain-community\nVersion: 0.0.36"
  },
  {
    "threadId": "1237696047627767869",
    "name": "Internal Service Error",
    "messages": "Hi team facing this issue. I copied the qdrant storage directory (volume mounted) from one machine to another machine. When i am trying to run the qdrant start commad on the machine where i copied it, i am facing this error:\n\n2024-05-08T09:17:14.580745Z ERROR qdrant::startup: Panic occurred in file /qdrant/lib/collection/src/shards/replica_set/mod.rs at line 261: Failed to load local shard \"./storage/collections/storage_T9MQ/0\": Service internal error: Invalid json path at line 1 column 57\n\nI am running on Docker and the command is:\ndocker run -d -p 6333:6333 -v ~/qdrant_storage:/qdrant/storage qdrant/qdrant\n\nCould you please help in this?\nTh commad is running fine on the original machine where this storage was created and data was written."
  },
  {
    "threadId": "1229870165991362671",
    "name": "What language does FastEmbed support?",
    "messages": "Is there a model, which supports Russian language? I wanted to use FastEmbed, but not sure If it supports Russian language or not."
  },
  {
    "threadId": "1229878684068876338",
    "name": "single-node snapshot loading fail due to Panic (using qdrant binary)",
    "messages": "I am using qdrant binary on windows wsl2 linux, simple snapshot creation works, but reload fails every time, this means that any ctrl-c pause on the qdrant server would completely destroy existing data since snapshot cannot be reloaded, is it expected? log trace attached"
  },
  {
    "threadId": "1229362281297612810",
    "name": "How to persist data accross restarts and updates on Google Cloud",
    "messages": "Hello everyone.\n\nI've deployed qdrant on google cloud, used directly the docker hub official image on a google cloud run.\n\nWhat happens is that I think sometimes the service restarts and everything is deleted.\n\nHow can I persist everything?"
  },
  {
    "threadId": "1229747802649923645",
    "name": "Kubernetes deployment fails to restore collection or snapshot (only full snapshots render a problem)",
    "messages": "Restarting Qdrant Pod fails to restore collection with full snapshot doesn't work while snapshot for each collections work -- leading to infinite CrashLoopBackOff with following message:\n\n```\nтФВ Stream closed EOF for default/qdrant-0 (qdrant)  \nRestoring snapshot into new collection in Kuberenes environment fails with:\nERROR qdrant::startup] Panic occurred in file lib/storage/src/content_manager/consensus/consensus_wal.rs at line 22: called `Result::unwrap()` on an `Err` value: Os { code: 11, kind: WouldBlock, message: \"Resource temporarily unavailable\" }\n```"
  },
  {
    "threadId": "1227277948923543654",
    "name": "Hybrid search",
    "messages": "Hi I am exploring Qdrant for my company's use. I would like to be able to create a standard inverted-index for full text search, and vectors for semantic search. Ideally, the framework selected would be able to perform a hybrid search with ranking (and deduplication) itself, as well as be able to automatically create the inverted index from a json payload (like how meilisearch does) with weightings (e.g. title more important than body).\n\nI see hybrid search is on your public roadmap (https://github.com/qdrant/qdrant/blob/master/docs/roadmap/README.md#core-milestones). Is this something you aim to do? Sparse vectors with splade look interesting but I would prefer a regular full-text search + vectors.\n\nMeilisearch-like sparse vector capabilities built into Qdrant would be amazing (though meilisearch is limited to 10 words per query so that's not great for RAG). I think Weaviate can do this but from testing I prefer Qdrant for it's speed and DX.\nThanks"
  },
  {
    "threadId": "1229384432968794122",
    "name": "creating config.yaml on local docker qdrant-storage",
    "messages": "I have a qdrant run as docker storage folder in my local machine. I want to add the api-key line in the config.yaml as mentioned in the documentation, but I can not found the config.yaml file in my folder hierarchy. As I understand, this file is different from the config file of each collection."
  },
  {
    "threadId": "1228867044829696050",
    "name": "fine-tuned BGEm3 custom sparse embedding",
    "messages": "I have a fine-tunned custom bgem3 model for sparse embeddings is there a way to integrate into qdrant and use that instead of spade?\nhere's the example\n\nhttps://huggingface.co/BAAI/bge-m3"
  },
  {
    "threadId": "1228461268554350592",
    "name": "Wildcard Search in Payload?",
    "messages": "I'm having a really difficult time \"querying\" my vectorized data by payload text.  I am using the /scroll and /search endpoints and trying to perform a wildcard search on payload key=text value like '%hello world%' (i know this isn't a valid api, but just trying to explain what i'm looking to achieve).  i've tried several iterations of filter, but not getting any results.  the entire sentence in the \"text\" payload is \"i am luke.  hello world.  goodbye\".  thanks!"
  },
  {
    "threadId": "1228368024869015623",
    "name": "For performance optimization, can we remove indexing from certain payload (not all)?",
    "messages": "I have 1 million data points of summary, and other data as a payload, i want to remove the indexing from ONLY SUMMARY  so it'll search faster does qdrant has any facility like this?\n\nalso can we add data type to qdrant payload? like date, string etc.?\n\nthanks in advance ЁЯЩВ"
  },
  {
    "threadId": "1228267085277167636",
    "name": "I've deleted some points based on filtering but still its showing in the qdrant collection.",
    "messages": "code for deletion: \n\nfor doc_type in filtered_document_types:\n    client.delete(\n        collection_name=\"Doctype_dataset_1m\",\n        points_selector=models.FilterSelector(\n            filter=models.Filter(\n                must=[\n                    models.FieldCondition(\n                        key=\"Doctype\",\n                        match=models.MatchValue(value=doc_type),\n                    ),\n                ],\n            )\n        ),\n    )\n\nBefore and after collection:\n\n\nwill it take time to remove the points nd vectors from collection? or qdrant just doesn't refer those points in future but points stays there?"
  },
  {
    "threadId": "1228239379290394625",
    "name": "How to know QDRANT_MAIN_URL",
    "messages": "I have a cluster with 3 nodes (image). I wonder how to connect to the cluster using the Python qdrant_client. While searching, I saw this (img). It has 3 ips for 3 nodes (\"https://node-x.my-cluster.com:6333\") and one main url (\"https://my-cluster.com:6333\"). How can I find this URL in my settings?"
  },
  {
    "threadId": "1228207522482094190",
    "name": "Cannot restore a snapshot",
    "messages": "I have 2 setup of qdrant:\n- One with 1 node (no cluster enable) (1).\n- One with 3 nodes (cluster enabled) (2).\nI create a snapshot with setup (1) -> file.snapshot. Then, in setup (2), I upload that snapshot. Yesterday, everything worked fine. But when I recreated the cluster today, it only had information about collection, but there was no data.\nCan you help me with this? Thanks!"
  },
  {
    "threadId": "1228005880868634695",
    "name": "Slow search performance with is_empty even with index?",
    "messages": "Hi, I am struggling to identify the performance issue causing is_empty to slow down searches even with an index created over the field (present on 1% of docs).\n\nI assume its probably a configuration error:\n\n```\n{\n    \"status\": \"green\",\n    \"optimizer_status\": \"ok\",\n    \"vectors_count\": 36417058,\n    \"indexed_vectors_count\": 36417058,\n    \"points_count\": 36059989,\n    \"segments_count\": 20,\n    \"config\":\n    {\n        \"params\":\n        {\n            \"vectors\":\n            {\n                \"size\": 512,\n                \"distance\": \"Cosine\"\n            },\n            \"shard_number\": 3,\n            \"replication_factor\": 2,\n            \"write_consistency_factor\": 1,\n            \"on_disk_payload\": true\n        },\n        \"hnsw_config\":\n        {\n            \"m\": 16,\n            \"ef_construct\": 100,\n            \"full_scan_threshold\": 10000,\n            \"max_indexing_threads\": 0,\n            \"on_disk\": true\n        },\n        \"optimizer_config\":\n        {\n            \"deleted_threshold\": 0.2,\n            \"vacuum_min_vector_number\": 1000,\n            \"default_segment_number\": 0,\n            \"max_segment_size\": null,\n            \"memmap_threshold\": 20000,\n            \"indexing_threshold\": 20000,\n            \"flush_interval_sec\": 5,\n            \"max_optimization_threads\": 1\n        },\n        \"wal_config\":\n        {\n            \"wal_capacity_mb\": 32,\n            \"wal_segments_ahead\": 0\n        },\n        \"quantization_config\": null\n    },\n    \"payload_schema\":\n    {\n        \"group_id\":\n        {\n            \"data_type\": \"keyword\",\n            \"points\": 36057546\n        },\n        \"label\":\n        {\n            \"data_type\": \"keyword\",\n            \"points\": 449696\n        }\n    }\n}\n```\n\nThe queries that I am trying to run are:\n- a recommend operation only over documents with `label`s AND a filter against a specific group\n- a scroll operation over documents with a `label` and a specific group id.\n\nWe thought creating an index would help resolve this issue, but so far it has not."
  },
  {
    "threadId": "1227807132552925236",
    "name": "\"Wrong input: Vector params for fast-bge-small-en-v1.5 are not specified in config\"",
    "messages": "Hi, \nI've indexed data into Qdrant Cloud using the rust client, wtih fastembed-rs -- indexing and querying data works great, the cluster is healthy and everything is smooth. \n\nI'm now trying to access the same cluster using the python qdrant client with the fastembed mixin and get the error message above. I've tried setting the model on the client with `qdrant_client.set_model(\"BAAI/bge-small-en-v1.5\")` but still get the same error. \n\nCan you point me in the right direction on where I need to configure things?"
  },
  {
    "threadId": "1227911787333029958",
    "name": "How to speed up Creating new shard replicas",
    "messages": "Hi all!\nWhen I add a new node to the cluster, I have to move data from the old node to the new one. The process is fine, but when I check the log, I see this (image). Perhaps my server is only using one CPU for this process. Actually, I feel the process is a bit slow. Can you help me with this?\nThanks."
  },
  {
    "threadId": "1227891656058212372",
    "name": "How to log as json format?",
    "messages": "I deployed qdrant server in kubernetes using helm chart.\nI want to set qdrant logs to json format.\nBut current(default) option is string format like this\n`[{timestamp} INFO  {logger}] {ip} \"POST {url} HTTP/1.1\" 200 132 \"-\" \"python-httpx/0.27.0\" 0.000221`\nIs it possible to change the log on the qdrant server to json format?"
  },
  {
    "threadId": "1227700250681151508",
    "name": "health checks on 1.7.1",
    "messages": "I have qdrant deployed to production on version 1.7.1 (readyz health check unavailable).\nWe are aware that upgrading the version from 1.7.1 -> 1.7.2 is a way to fix the problem with the readyz health check. \nThis upgrade adds some downtime for us which IтАЩd like to avoid. \nGiven that, are there any other alternatives for us to get these health checks working?"
  },
  {
    "threadId": "1227633508139139103",
    "name": "Shard Not Active Error",
    "messages": "Hi, \nI am trying to implement distributed qdrant. I have already qdrant with several collections and cluster INFO looks like this: \n`{\n  \"result\": {\n    \"status\": \"enabled\",\n    \"peer_id\": 2979974762066354,\n    \"peers\": {\n      \"2979974762066354\": {\n        \"uri\": \"http://172.31.24.252:6335/\"\n      },\n      \"1418878837160198\": {\n        \"uri\": \"http://172.31.24.177:6335/\"\n      }\n    },\n    \"raft_info\": {\n      \"term\": 9607,\n      \"commit\": 257,\n      \"pending_operations\": 0,\n      \"leader\": 2979974762066354,\n      \"role\": \"Leader\",\n      \"is_voter\": true\n    },\n    \"consensus_thread_status\": {\n      \"consensus_thread_status\": \"working\",\n      \"last_update\": \"2024-04-10T14:48:21.798830364Z\"\n    },\n    \"message_send_failures\": {}\n  },\n  \"status\": \"ok\",\n  \"time\": 0.00000901\n}`\n\nI updated my collections replication factor to 2, and I want to have 2 nodes with 1 replace each. So I want to copy my shards to new node by following command: \n`POST /collections/test_collection/cluster\n{\n  \"replicate_shard\": {\n    \"shard_id\": 0,\n    \"from_peer_id\": 2979974762066354,\n    \"to_peer_id\": 1418878837160198\n  }\n}`\n\nbut I got this error: \n`{\n  \"error\": \"Bad request: Shard 0 is not active on peer 2979974762066354\"\n}`\nError as a log: \n\n`2024-04-10T14:52:56.673689Z  WARN storage::content_manager::consensus_manager: Failed to apply collection meta operation entry with user error: Bad request: Shard 0 is not active on peer 2979974762066354    `\n\n\nHowever, it looks active when I look info section from qdrant web UI.  Any idea why I am getting this error ?\n\n\nLeader node version : v1.8.1\nFollower node version : 1.8.4\nBoth served on docker containers in differrent servers"
  },
  {
    "threadId": "1227089302207467520",
    "name": "migrate consistently fails with count mismatch error",
    "messages": "We are moving cloud providers and I am trying to use the migrate method in qdrant client to migrate our 4 node cluster but it's erroring saying the source count of points doesn't match the destination count.\n\nWhat would cause an error like this? It's strange to me because it's wait true on the add points and I have its write consistency and replicas set to 2 so it's definitely writing them down in multiple places. I'm tempted to delete the assert"
  },
  {
    "threadId": "1225480019552239626",
    "name": "Update only id for points in the collection",
    "messages": "Hi, I have a question, Is there a way to update / change the id of each point in a collection and remains everything else the same? Or is there a proper way to do this?\nAfter checking https://qdrant.tech/documentation/concepts/points/?q=points#update-vectors, \nit seems the update vector (following example): id=1 and id=2 are the points to be updated, but there's no place to  put the new ids? say if I want to update this two points (id=1 and id=2) to (id=3 and id=4) while keeping everything else untouched, what's the best way to do it other than retrieve the points (and vector) and re-upload to the collection?\n\n\nExample in the doc for python client:\n`client.update_vectors(\n    collection_name=\"{collection_name}\",\n    points=[\n        models.PointVectors(\n            id=1,\n            vector={\n                \"image\": [0.1, 0.2, 0.3, 0.4],\n            },\n        ),\n        models.PointVectors(\n            id=2,\n            vector={\n                \"text\": [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2],\n            },\n        ),\n    ],\n)`"
  },
  {
    "threadId": "1227211327852777492",
    "name": "Qdrant Custom Prefix Not working when accessing the dashboard",
    "messages": "My current configuration: I have istio setup for routing stuff. I have created an virtual service to route it to the actual service. Istio hits the virtual service. When i try to access the url \nhttps://dummy/qdrant/prefix/dashboard\nit just drops the prefix `qdrant/prefix`\n\nFrom the request:\nauthority: dummy\n:method: GET\n:path: /dashboard/manifest.json\n:scheme: https\n\nalso while it says https in the logs of istio it shows http 2 which is grpc\nand the /dashboard is only giving an issue\n\nit works fine with port forwarding or whenever i assign an ip\n\nThe python client does not work when i use the whole given url but when i pass prefix as a seperate param it works fine"
  },
  {
    "threadId": "1227218889356607550",
    "name": "fastembed query",
    "messages": "Hi all,\n\nThis query is regarding using fastembed with qdrant.\nAm checking the following link:\nhttps://qdrant.github.io/fastembed/\nwith default embedding it advices  \"query\" and \"passage\" prefixes for the input text.\n\nMy question is that, when we use the query method, example:\nsearch_result = client.query(\n    collection_name=\"demo_collection\",\n    query_text=\"This is a query document\"\n)\n\nshould we add the 'query: ' prefix for query_text as well?"
  },
  {
    "threadId": "1226910655035080785",
    "name": "How do I delete a metadata keyword within a payload - child_keywords",
    "messages": "When I was building my rag. I created what I thought would be helpful but seems it actually made the retreival worse.\n\n```\nchild_node.metadata[\"child_keywords\"] = child_node_keywords_list\n```\n\n```\nchild_keywords\n[\n0:\"keyword1\"\n1:\"keyword2\"\n2:\"keyword3\"\n3:\"keyword4\"\n]\n```\n\nIs there a way to delete just this metadata field without deleting the entire point/payload and or having to recreate the entire rag? \n\nThe one issue I could see is that the child keyword is still inside the `_node_content`"
  },
  {
    "threadId": "1226782983915307059",
    "name": "MMR search with QdranrClient",
    "messages": "Hi,\n\nI don't see an implementation of \"MMR\" search with \"QdrantClient\" API (https://github.com/qdrant/qdrant-client/blob/master/qdrant_client/qdrant_client.py#L242). Do we have search_type=\"mmr\" to be added in the future release.\n\nIs this feature only available for orchestrators like Langchain/LLAMA-Index, or can we apply that with OOB client as well.\n\nIn langchain, we can do the search in the following way:\n\nqdrant.max_marginal_relevance_search (FOR MMR)\nqdrant.similarity_search (FOR SIMILARITY search)\n\nhttps://python.langchain.com/docs/integrations/vectorstores/qdrant/\n\nAny help appreciated."
  },
  {
    "threadId": "1226185059204202526",
    "name": "How to use Qdrant for classification?",
    "messages": "I have a list of topics, e.g: [expenses, profit, risks] etc.\n\nI have a big pdf (around 500 pages). \n\nI want to classify each page in that pdf; e.g.:\n\npage-1: [expenses, risks]\npage-2: [profit]\npage-3: []\n\netc.\n\nIs it possible to acheve with Qdrant?"
  },
  {
    "threadId": "1237346398505144350",
    "name": "Exploring the Optimal Vector Database",
    "messages": "I am currently searching for the best vector database available and noticed that qdrant seems to be the best performing one. However, I would like some clarification regarding the single-store vector database compared to Qdrant. Can anyone provide a detailed explanation of the differences between these two vector databases? I know that the single store is more like a database than a specialized vector database. Still, I am interested in the performance comparison between the single store vector database and Qdrant. Additionally, I would be grateful if you could provide any recent benchmarks comparing Qdrant with other vector databases. Thank you!"
  },
  {
    "threadId": "1236427673056706682",
    "name": "Hello all,",
    "messages": "Hello all, I just started using Qdrant cloud today using Python. When I try to use the API keys, they keep expiring. I had to get a new key every 10-20 mins. This is not normal. Am I missing something?"
  },
  {
    "threadId": "1237339957396508694",
    "name": "Upload snapshot",
    "messages": "I got a problem that I can't upload snapshot(I use local) It always stuck at this page"
  },
  {
    "threadId": "1221863648884953098",
    "name": "Upsert binary embedding with Go",
    "messages": "Good afternoon, \n\nI am relatively new to Go, so forgive me if my understanding is wrong. I am receiving binary embeddings from Cohere and wish to upsert these vectors into Qdrant via Go. The examples are all using float32 vectors, and it appears that the structs expect such. Is it not currently possible to use binary embeddings with the Go client library?\n\nI am using the Qdrant helm repo in Minikube. The connections are done via the `pointClient` with `gRPC`."
  },
  {
    "threadId": "1225726611542769736",
    "name": "frequent crash on inserting into collection",
    "messages": "I am using qdrant cloud. After upserting every 2-3 point, there is bad gateway crash. I visit the ui end of the collection. It shows not ready. After a bit the collection turns healthy and again accepts 2-3 insertion (MAX). So far point count is 19962. Error message is:\nUnexpectedResponse: Unexpected Response: 502 (Bad Gateway)\nRaw response content:\nb'Bad Gateway'\n\nTIA"
  },
  {
    "threadId": "1225629973188972655",
    "name": "What does \"unit\" refer to in the pricing of Qdrant on AWS Marketplace",
    "messages": "Hello, I conducted some tests and found Qdrant to be super cool and super fast. I have one question, please: What does \"unit\" refer to in the pricing structure of Qdrant on AWS Marketplace? Also, how can I estimate the cost for, let's say, 1M data points of 384-dimensional vectors?\n\nClarifying the meaning of \"unit\" will provide clarity on how charges are calculated and aid in decision-making processes.\n\nThank you ЁЯЩВ\n\nhttps://qdrant.tech/documentation/cloud/aws-marketplace/"
  },
  {
    "threadId": "1224989631842488340",
    "name": "Slow search performance on Qdrant Cloud",
    "messages": "Hi, we are currently struggling to identify the cause of slow search performance on Qdrant Cloud.\nAny help would be greatly appreciated.\n\n**уАРIssue SummaryуАС**\nSearch requests on Qdrant cloud cluster are very slow, compared to a local machine with the same specs.\n\n**Our qdrant cloud cluster**\nCluster ID: b8e93649-d79a-458d-b7fd-209c934a2aee\nSpec: CPU 4 vCores / RAM 16.00 GB\nversion: 1.8.4\n\n**Local environment (for comparison)**\nLaptop with spec: CPU 16 vCores / RAM 64.00 GB\nRun docker container with flags: -m 16g --cpus 4 (тА╗Should be same spec as the cluster)\nversion: 1.8.4\n\nTarget collection: report_chunks_ja\nData amount: 768 dims ├Ч 10000000 points\n\n\n**уАРPerformance ComparisonуАС**\nуГ╗Qdrant cloud\n```\nтЭп for i in {0..2}; do QDRANT_ENDPOINT=${QDRANT_CLUSTER_ENDPOINT} QDRANT_API_KEY=xxx python reproduce_script.py; done\nFinished search (time): 10.517\nFinished search (time): 9.852\nFinished search (time): 11.569\n```\n\nуГ╗local\n```\nтЭп for i in {0..2}; do QDRANT_ENDPOINT=http://localhost:46333 python reproduce_script.py; done      \nFinished search (time): 0.592\nFinished search (time): 0.504\nFinished search (time): 0.526\n```\n\nAs above, two environments should be with the same spec machine, but the cloud is noticeably slower.\nWhat could be the cause? We would like to get some hints on how to investigate.\nAlso, please let us know if there is any additional information we should provide."
  },
  {
    "threadId": "1225379879701450772",
    "name": "is fastembeed support the sentence-transformers/all-mpnet-base-v2 model",
    "messages": "I am implementing semantic search with the 'sentence-transformers/all-mpnet-base-v2' model. However, while installing it in Docker, it is consuming excessive memory. Therefore, I want to implement this using FastEmbed."
  },
  {
    "threadId": "1225253874995298415",
    "name": "Qdrant search timeout",
    "messages": "How can I extend the timeout?\n\n```\ncalled `Result::unwrap()` on an `Err` value: status: Internal, message: \"Service internal error: 1 of 1 read operations failed:\\n  Timeout error: Operation 'Search' timed out after 60 seconds\", details: [], metadata: MetadataMap { headers: {\"content-type\": \"application/grpc\", \"date\": \"Thu, 04 Apr 2024 01:16:18 GMT\", \"content-length\": \"0\"} }\n```\n\nTimeout is supposed to be set in RecommendPoints, isn't it?\n\n```\nfor uuid in uuids.clone() {\n    recommend_queries.push(RecommendPoints {\n        collection_name: COLLECTION_NAME.to_string(),\n        positive: vec![uuid],\n        limit: 100,\n        with_payload: Some(true.into()),\n        filter: recommendation_filter.clone(),\n        timeout: Some(args.timeout.clone() as u64),\n        params: Some(SearchParams {\n            quantization: Some(QuantizationSearchParams {\n                ignore: Some(false),\n                rescore: Some(true),\n                oversampling: Some(3.0),\n            }),\n            ..Default::default()\n        }),\n        ..Default::default()\n    });\n}\n\nlet recommend_response: qdrant::RecommendBatchResponse = client\n    .recommend_batch(&RecommendBatchPoints {\n        collection_name: COLLECTION_NAME.to_string(),\n        recommend_points: recommend_queries,\n        ..Default::default()\n    })\n    .await\n    .unwrap();\n```"
  },
  {
    "threadId": "1225489445113954387",
    "name": "search returning negative scores",
    "messages": "Hi guys, quick question: I am working with someone else's Qdrant instance and I am noticing that the server is returning **negative** scores for my queries, and this is unusual to me. Was an API updated? Are the search results that bad? I can't find anything about the score values being negative I am used to values always being >= 0"
  },
  {
    "threadId": "1225481312635195392",
    "name": "Rust Client with Qdrant Cloud AWS missing GRPC Status Header",
    "messages": "Ok -- here is a pretty basic question -- I've just started to use (created it last night, but finally getting quality time with keyboard) Qdrant Cloud in AWS . Using the Rust client if I try just the example authentication example from here: https://qdrant.tech/documentation/cloud/authentication/ -- I get the following error:\nError: status: Unknown, message: \"grpc-status header missing, mapped from HTTP status code 464\", details: [], metadata: MetadataMap { headers: {\"server\": \"awselb/2.0\", \"date\": \"Thu, 04 Apr 2024 16:18:39 GMT\", \"content-length\": \"0\"} }\n\nI'm guessing it has something to do wtih Tonic, I did install it according to the readme, perhaps I'm missing something else"
  },
  {
    "threadId": "1225208964552327291",
    "name": "Qdrant .NET Client Issue moving from Docker to Qdrant Cloud",
    "messages": "I have been testing Qdrant using a local docker version of Qdrant (v1.74) with the C# client  (v1.8.0 -- latest available).  Everything worked locally so I switched to a free cloud account (v1.8.4).  I can push data to it but I'm now getting an error at the Grpc level within the client (see below) when I perform a search.  Any ideas on what I can look into to resolve this?  Thanks!\n\n\nfail: Qdrant.Client[99999]\n      Operation failed: Search\n      Grpc.Core.RpcException: Status(StatusCode=\"InvalidArgument\", Detail=\"Invalid json path\")\n         at Qdrant.Client.QdrantClient.SearchAsync(String collectionName, ReadOnlyMemory`1 vector, Filter filter, SearchParams searchParams, UInt64 limit, UInt64 offset, WithPayloadSelector payloadSelector, WithVectorsSelector vectorsSelector, Nullable`1 scoreThreshold, String vectorName, ReadConsistency readConsistency, ShardKeySelector shardKeySelector, Nullable`1 sparseIndices, Nullable`1 timeout, CancellationToken cancellationToken)"
  },
  {
    "threadId": "1224460329388609596",
    "name": "optimizers config",
    "messages": "Hi,\nI have a collection with 202M points, each point contains a single vector with a length of 384 and 3 other fields.\nwhat should be the values of the parameters in the collection optimizers config? I have a cluster with 600GB of RAM, I want to focus on accuracy and speed\nHere is my config:\n{\n  \"params\": {\n    \"vectors\": {\n      \"embeddings\": {\n        \"size\": 384,\n        \"distance\": \"Cosine\"\n      }\n    },\n    \"shard_number\": 2,\n    \"replication_factor\": 1,\n    \"write_consistency_factor\": 1,\n    \"on_disk_payload\": true\n  },\n  \"hnsw_config\": {\n    \"m\": 16,\n    \"ef_construct\": 100,\n    \"full_scan_threshold\": 10000,\n    \"max_indexing_threads\": 0,\n    \"on_disk\": false\n  },\n  \"optimizer_config\": {\n    \"deleted_threshold\": 0.2,\n    \"vacuum_min_vector_number\": 1000,\n    \"default_segment_number\": 0,\n    \"max_segment_size\": null,\n    \"memmap_threshold\": null,\n    \"indexing_threshold\": 200000,\n    \"flush_interval_sec\": 5,\n    \"max_optimization_threads\": null\n  },\n  \"wal_config\": {\n    \"wal_capacity_mb\": 32,\n    \"wal_segments_ahead\": 0\n  },\n  \"quantization_config\": null\n}"
  },
  {
    "threadId": "1225038144819167273",
    "name": "How to improve Search with Filter",
    "messages": "Hi, i add the script below.\nEach vector has a payload, which contain field \"publish_date\". I want to search with filter \"publish_date > specific_value\".\nPerformance without filter: 27ms.\nPerformance with filter: 250ms, which is pretty high.\nHow to improve performance with filter? Type of publish date is int (timestamp). Timestamp of vectors are not the same.\nThanks."
  },
  {
    "threadId": "1225135761733718068",
    "name": "Shard unavailability with peer restarts in custom sharded cluster",
    "messages": "qdrant version: v1.8.3\ndeployment: distributed self hosted cluster on kubernetes\ndata size:  over 1M points\nclient: golang client v1.8.0\n\nWe have a cluster that's taking a reasonably high upsert traffic, ~5k req/s and is also serving query requests to user clients, though query traffic is relatively small ~50req/min. When creating a shard, we specify a replication factor of 2 as follows:\n\n_, err := p.collectionsClient.CreateShardKey(ctx, &pb.CreateShardKeyRequest{\n        CollectionName: collectionName,\n            Request: &pb.CreateShardKey{\n                ShardKey: &pb.ShardKey{\n                    Key: &pb.ShardKey_Keyword{Keyword: <uuid>},\n                },\n                ReplicationFactor: 2\n            },\n        })\n\nHowever, when any qdrant node is reassigned by kubernetes, the cluster temporarily becomes unavailable. When upserting points, I get a lot of errors like the one below:\n\nrpc error: code = Internal desc = Service internal error: Tonic status error: status: Unavailable, message: \"Failed to connect to http://qdrant-52.qdrant-headless:6335/, error: transport error\", details: [], metadata: MetadataMap { headers: {} }\n\nAssuming the shard was replicated, I'd expect the write to succeed accessing the available shard? I use ordering WEAK on upserts and set wait to false\n\nIt sometimes takes ~30min for the node to finish recovery and start serving. There are currently no deletes in the collection,  only upsert traffic with sporadic search requests for testing.\n\n1. Is there a requirement for all nodes (peers) to be available? How can we leverage replication to avoid getting these errors?\n\n2. How can I confirm that the shards in the cluster are replicated? What endpoint can I use to get the per shard replication state, and confirm that created custom shards are actually replicated?\n\nThank you!"
  },
  {
    "threadId": "1224183064167710831",
    "name": "How to run the server without docker",
    "messages": "Hi am looking to run the server without using docker. Is there a document that will help accomplish this?"
  },
  {
    "threadId": "1224620006050893864",
    "name": "How delete vectors for deleted points",
    "messages": "When i delete the points using the ID but its not deleting the vectors associated with the that ID."
  },
  {
    "threadId": "1220725896181448725",
    "name": "Multiple vectors for the same point",
    "messages": "Is there any option to load multiple vectors into the same point?\nCan I do it with spark connector?"
  },
  {
    "threadId": "1225011699850018827",
    "name": "Restore collection from RAM (on_disk=False)",
    "messages": "Hi,\nI am currently save collections on disk, so when i accidentally remove docker container, i can re-run at `$(pwd)` and my collections are still there.\nSo what if i choose to save them on RAM?\nI run qdrant with docker."
  },
  {
    "threadId": "1224986879296929842",
    "name": "Qdrant: Search by two named vectors",
    "messages": "Hi All,\n\nI'm having a collection on qdrant which I need to search on. The collection has records indexed with two named vectors. I'm looking for a way to use two query vectors, one for each indexed vector. Is there a way to do it?\n\nAll examples show only querying by one vector."
  },
  {
    "threadId": "1224932643687235677",
    "name": "Search with DatetimeRange Filter",
    "messages": "Hi!\nWhen I followed this: https://qdrant.tech/documentation/concepts/filtering/#datetime-range, I got an error (attachment). The type of field \"publish_date\" when I push it into the collection is <class 'datetime.datetime'>.\nCan someone help me with this?\nThanks."
  },
  {
    "threadId": "1224985857132335115",
    "name": "qDrant not releasing lock",
    "messages": "I'm currently trying to insert embeddings and other data into qDrant as part of LlamaIndex's IngestionPipeline.\n```\nclient = qdrant_client.QdrantClient(path=vector_store_path)\n        client.create_collection(collection_name=VECTOR_COLLECTION_NAME,vectors_config=models.VectorParams(size=param_size,distance=models.Distance.COSINE))\n        return client\n# other code here...\ntransformations=[\n                TitleExtractor(nodes=3,llm=llm,num_workers=1),\n                QuestionsAnsweredExtractor(questions=3,llm=llm,num_workers=1),\n                SummaryExtractor(summaries=[\"prev\",\"self\",\"next\"],llm=llm,num_workers=1),\n                KeywordExtractor(llm=llm,num_workers=1),\n                SentenceSplitter(chunk_size=2048,chunk_overlap=512),\n                # TokenTextSplitter(chunk_size=1024,chunk_overlap=256),\n                HuggingFaceEmbedding(model_name=embed_model)\n                ],\n                vector_store=vector_store\n            )\n            nodes = pipeline.run(documents=docs,show_progress=True)\n```\n\nWhile everything works well, the qdrant vector store created has a .lock text file created containing\n```\ntmp lock file\n```\nand is not releasing the lock even after everything has finished running.(The same problem occurs in .py and .ipynb files)\nIs there any way for me to get qDrant to release the lock after all the data has been inserted?\n\nAm i asking in the right place, or should i be raising this to LlamaIndex instead?\n\nThanks!"
  },
  {
    "threadId": "1224965783151575093",
    "name": "Segments are missing",
    "messages": "I have faced the strange issue, after qdrant restart in which some the shard segments are missing, which is strange, please give your comment."
  },
  {
    "threadId": "1224675583544393818",
    "name": "How best_score work?",
    "messages": "Hi, I am new here.\nCan someone explain to me the best_score strategy?\nhttps://qdrant.tech/documentation/concepts/explore/#best-score-strategy\nAs you can see, I have 2 results, one from best_score, and the other from the similar search for each post in pos_set ([198240402142240491, 198240402110415657]). But the score and the mechanic for arranging them are unclear.\nMany thanks тЭдя╕П"
  },
  {
    "threadId": "1224822483287543818",
    "name": "SoC 2024",
    "messages": "Hi everyone! I am trying to apply for SoC 2024, but I am kinda lost. The guidance said \"You are supposed to make an application with a proposal at the GSoC website via email Please do not request individual GitHub issues to be assigned to you. Instead, make sure that you correctly understand the project scope and requirements, craft a proposal detailing your approach to the problem, and file your application at the GSoC website VIA EMAIL between March 18 and April 2. You may want to read our guide to learn how to apply for a GSoC internship at Qdrant.\" I went to GSoC website but I can't find information about the project. This is my first time apply for this role. I really appreciate your help !!!"
  },
  {
    "threadId": "1224692885019164872",
    "name": "Cant connect to qdrant through python client but can connect through js client",
    "messages": "Python Code:(3.11.7)\nfrom qdrant_client import QdrantClient\n\nclient = QdrantClient(host=\"redacted\", port=6333, api_key=\"redacted\")\n\nIssue:\nsite-packages/qdrant_client/http/api_client.py\", line 105, in send_inner\n    raise ResponseHandlingException(e)\nqdrant_client.http.exceptions.ResponseHandlingException: [SSL] record layer failure (_ssl.c:1006)\n\nJS Code(works fine):\nconst QdrantClient = require('@qdrant/js-client-rest').QdrantClient;\n\n// or connect to Qdrant Cloud\nconst client = new QdrantClient({\n    url: 'http://redacted:6333',\n    apiKey: 'redacted',\n});\n\n// Asynchronous function to get collections\nasync function getCollections() {\n    try {\n        const result = await client.getCollections(); // Wait for the result\n        console.log('List of collections:', result.collections);\n    } catch (error) {\n        console.error('Error getting collections:', error);\n    }\n}\n\n// Call the asynchronous function\ngetCollections();"
  },
  {
    "threadId": "1224647729418993815",
    "name": "Pods in crashLoop for multiple replicas",
    "messages": "Questions:\n1) For distributed deployment when we need multiple nodes we increase the number of replicas in the values.yaml right ? To increase the number of pods right ?\n2) I have attached the manifest that is generated using values.yaml . For any values of replica > 0 only the first pod works fine all the rest of the pods get stuck in CrashLoop. How to reslove this. I need to perform this programatically so instead of kubectl i am using the kubernetes python client. With the client it works perfectly fine for 1 replica. with kubectl apply -f file.yaml it works for any number of replicas so what is the issue ?"
  },
  {
    "threadId": "1224634501225906268",
    "name": "How does the threshold works for Similarity Cosine vector metrics.",
    "messages": "Does higher thresholds lead to stricter similarity requirements, resulting in fewer but more relevant results. Lower thresholds retrieve more items, potentially including less relevant ones?"
  },
  {
    "threadId": "1210134320606478366",
    "name": "Qdrant helm chart+ distributed deployment",
    "messages": "Please help me\n1. When I install qdrant helm chart with distributed deployment mode, should I create PV with hostPath volume (each node has its own data) or nfs to mount all node to a external data dir or cloud?\n\n2. I created PersistanceVolume with hostPath volume kind to mount from /home/data. I installed helm chart for qdrant in a cluster including 4 node. I have saw qdrant is running on node4. Then I created a collection with 2 shards, replication_factor=2 so I just see a clloection in node4 (home/data/collections). How I can know which shard is currently on which node, which replica is currently on which node?"
  },
  {
    "threadId": "1208509498470432778",
    "name": "Tune CPU and Memory (Latency is 2 sec now)",
    "messages": "We have about 5K search load and I want to know what is the best configuration for this usage. \nHow many replicas should we have? \nHow many CPUs and how much Memory is needed?\n\nNow we are running a 400 RPS load-test with 16 cores and 32Gi of memory and it is awful. The upper-99 latency is 2 seconds.\n\nAlso our vectors dimension are 256 and we usually have 2 keyword filters in our searches.\n\nThank you for your advice in advance."
  },
  {
    "threadId": "1224282555247759441",
    "name": "Auto Scaling Groups",
    "messages": "Do you provide support for auto-scaling groups? If not, can you recommend a way to create a similar functionality? The database is usually inactive, except when there is a sudden increase in traffic. Therefore, it is not logical to use a fixed plan that may not be suitable for most of the time just to handle peak periods.\n\nI'm hosting my Qdrant database in GCP."
  },
  {
    "threadId": "1222587085761024010",
    "name": "Filter not returning nothing ([], None)",
    "messages": "```python\ndef create_payload(self, word:str, sentence: str, paragraph: str, words_list:list):\n        payload = {}\n        payload = {\n                \"word\": word,\n                \"sentence\": sentence,\n                \"paragraph\": paragraph,\n                \"words\":words_list\n            }\n        return payload\n```\n\n```python\n def search_point_by_keyword(self,keyword: str) -> str:\n        return client.scroll(\n        collection_name=self.collection_name,\n        scroll_filter=models.Filter(\n            must=[\n                models.FieldCondition(\n                    key='word',\n                    match=models.MatchValue(value=keyword),\n                )\n            ]\n        ),\n    )\n```\n\nWhat am I doing wrong?\n\nI see this solution:\nhttps://stackoverflow.com/questions/78118020/qdrant-client-scroll-filter-does-not-work\n\nhe problem is that the data was inserted using LangChain. That means that metadata is within the payload. Which further means that the fields have to be accessed by FieldCondition(key=\"metadata.field_name\", ... instead of FieldCondition(key=\"field_name\", \n\nBUT I am using python without any framework ..."
  },
  {
    "threadId": "1224306031765618740",
    "name": "How to delete the payload based on the ID",
    "messages": "I want to delete the question based on the provided ID and i also want to do the update and  insert based on the ID for the points in the vector db."
  },
  {
    "threadId": "1224293897426047107",
    "name": "Datatype issue: DoubleType not working when searching",
    "messages": "Hi All,\n\nI'm writing my data to Qdrant. My vector field is of type ArrayType(DoubleType()). While uploading I don't find any issue. But when I try to do a search it doesn't work. But when I convert my vector field type to ArrayType(FloatType()), the search is working. \n\nThe problem however is I have a precision of 20 decimal points when I use double. If I use float, spark decreases the precision to 5 decimal points by default. \n\nWhy is this happening and is there a way to upload and search the vector as ArrayType(DoubleType())?"
  },
  {
    "threadId": "1224099895355506718",
    "name": "From mem to disk",
    "messages": "I had a collection with more than 40M vectors in memory. It occupied up to 140gb, but I needed to downgrade the instance. Now I only have 60gb available. Now the configuration is the following\n\n```\n{\n  \"params\": {\n    \"vectors\": {\n      \"size\": 384,\n      \"distance\": \"Cosine\",\n      \"on_disk\": true\n    },\n    \"shard_number\": 8,\n    \"replication_factor\": 1,\n    \"write_consistency_factor\": 1,\n    \"on_disk_payload\": true\n  },\n  \"hnsw_config\": {\n    \"m\": 64,\n    \"ef_construct\": 512,\n    \"full_scan_threshold\": 10000,\n    \"max_indexing_threads\": 0,\n    \"on_disk\": true\n  },\n  \"optimizer_config\": {\n    \"deleted_threshold\": 0.2,\n    \"vacuum_min_vector_number\": 1000,\n    \"default_segment_number\": 0,\n    \"max_segment_size\": null,\n    \"memmap_threshold\": 20000,\n    \"indexing_threshold\": 20000,\n    \"flush_interval_sec\": 5,\n    \"max_optimization_threads\": 1\n  },\n  \"wal_config\": {\n    \"wal_capacity_mb\": 32,\n    \"wal_segments_ahead\": 0\n  },\n  \"quantization_config\": {\n    \"binary\": {\n      \"always_ram\": false\n    }\n  }\n}\n```\n\nHowever, docker is not starting since it is still consuming all the available memory. What do I need to do?"
  },
  {
    "threadId": "1234807551854514207",
    "name": "Max Size of input document",
    "messages": "Hi,\n\nI am trying to upload list of documents using langchain api to qdrant database as below:\n\n`from langchain_community.vectorstores import Qdrant\nQdrant.from_documents(\n        documents=documents,\n        embedding=embedding,\n        url=<qdrant_cloud_url>,\n        prefer_grpc=False,\n        collection_name=\"Test_Documents\",\n        force_recreate=True,\n        )`\n\nIt is giving me the error as below:\n\n**Too many input tokens. Max input tokens: 8192, request input token count: 8287**\n\nI have also tried batch approach where i upload the ducments in a batch of 10, but it still give me the error above.\n\nIs there a limit to what the size of each **document** in the list of **documents** should be to be uploaded to Qdrant.\n\nI am using amazon embedding model: \"**amazon.titan-embed-text-v1**\""
  },
  {
    "threadId": "1234708797025419284",
    "name": "I can't recover Qdrant Cloud Cluster Node",
    "messages": "Issue:\n\nA Qdrant Cloud cluster node has become unresponsive and cannot be recovered. This is due to an out-of-memory error caused by the accidental insertion of a large amount of data into the cluster.\n\nSteps Taken:\n\nAn attempt was made to delete the affected collection from the node, but this was unsuccessful.\n\nRestarting the Qdrant cluster resulted in the following error:\n\nPlease check memory consumption, increase memory limit or remove some collections and restart\nService internal error: Qdrant was killed during initialization. Most likely it's Out-of-Memory.\nCaused by:\n\n2024-04-30T03:18:59.374293Z ERROR qdrant::startup: Panic occurred in file src/main.rs at line 326: Can't initialize consensus: Failed to apply collection meta operation entry\nQuestion:\n\nIs there a way to recover the unresponsive node?\n\nAdditional Information:\n\n- The Qdrant Cloud cluster is running in a qdrant cloud environment.\n- The affected node is running out of memory due to the large amount of data that was accidentally inserted.\n- Restarting the Qdrant cluster results in an error message indicating that the node is out of memory.\n- The goal is to recover the unresponsive node and restore the Qdrant Cloud cluster to functionality.\n- I hope this translation is helpful. Please let me know if you have any other questions."
  },
  {
    "threadId": "1234820095470206976",
    "name": "Filter query integration",
    "messages": "ЁЯШл \n\nHello, I try to filter result with word contained in the user query :\n\n`\ndef create_filter(service):\n    return Filter(must=[FieldCondition(\n        key=\"service\",\n        match=MatchValue(value=service) \n    )])\n\ndef search():\n    data = request.json\n    query = validate_search_query(data)\n    print(f\"Original query: {query}\")  # Debugging statement\n    \n    service_filter = None\n    if \"recette\" in query.lower():\n        service_filter = create_filter(\"Recettes\")\n    elif \"chef\" in query.lower():\n        service_filter = create_filter(\"Chefs\")\n    elif \"produit\" in query.lower():\n        service_filter = create_filter(\"MarketPlace\")\n\n    print(service_filter)\n\n    #entities = extract_entities(query)\n    client = get_qdrant_client()\n    cohere_client = get_cohere_client()\n    \n    response = cohere_client.embed(\n        texts=[query],\n        model=\"embed-multilingual-v3.0\",\n        input_type=\"search_query\",\n    )\n\n    results = client.search(\n        collection_name=\"valrhona\",\n        query_vector=response.embeddings[0],\n        query_filter=service_filter,\n        limit=5,\n    )`\n\ndebug: Original query: recette de bonbons\nshould=None min_should=None must=[FieldCondition(key='service', match=MatchValue(value='Recettes'), range=None, geo_bounding_box=None, geo_radius=None, geo_polygon=None, values_count=None)] must_not=None\n\nBut result is not filtered...\n\nStructure of my data is :\ndef validate_document(point):\n    return {\n        \"titre\": point.payload.get(\"titre\", \"\"),\n        \"text\": point.payload.get(\"text\", \"\"),\n        \"url\": point.payload.get(\"url\", \"\"),\n        \"service\": point.payload.get(\"service\", \"\"),\n        \"score\": point.score\n    }\n\nHow to use correctly the filter function <:thankyou:1212491179715985428>"
  },
  {
    "threadId": "1231669971684429824",
    "name": "Low accuracy",
    "messages": "I have a collection with 200M records, this is the info:\n{\n\"params\":{\n\"vectors\":{\n\"embeddings\":{}\n\"name_embeddings\":{}\n}\n\"shard_number\":3\n\"replication_factor\":1\n\"write_consistency_factor\":1\n\"on_disk_payload\":true\n}\n\"hnsw_config\":{\n\"m\":16\n\"ef_construct\":100\n\"full_scan_threshold\":10000\n\"max_indexing_threads\":0\n\"on_disk\":true\n}\n\"optimizer_config\":{\n\"deleted_threshold\":0.2\n\"vacuum_min_vector_number\":1000\n\"default_segment_number\":2\n\"max_segment_size\":\nNULL\n\"memmap_threshold\":20000\n\"indexing_threshold\":200000\n\"flush_interval_sec\":5\n\"max_optimization_threads\":24\n}\n\"wal_config\":{\n\"wal_capacity_mb\":32\n\"wal_segments_ahead\":0\n}\n\"quantization_config\":{\n\"scalar\":{\n\"type\":\"int8\"\n\"always_ram\":true\n}\n}\n}\n\nI'm trying to search for a vector which I uploaded into the collection but I didn't get the vector, The score should be 1 because this vector is path of the collection.\nI'm using the search client api without any changed in the search params or something, \nas you can see in the added picture, when i'm filtering for the original ID I got the result I want.\nwhat can be the problem? what I have to change?"
  },
  {
    "threadId": "1223891312827306005",
    "name": "Does there exist way to modify payload nestedly without overwrite?",
    "messages": "I'm facing a problem to solve. For now, I have millions of Points with a payload like:\n\n```\n{\n    \"metadata\": {\n        \"uuid\": \"xxx\",\n        \"name\": \"xxx\",\n        \"description\": \"xxx\",\n    }\n}\n```\n\nNow I want to add a new `metadata.type` to them.\n\nAll metadata will be overwrited If I use set_payload api, with payload like this: \n```\n\n{\n    \"metadata\": {\n        \"type\": 123\n    }\n}\n```\n\nAnd if I use set_payload api, with payload as `{\"metadata.type\": 123}`. I will get a Point like this:"
  },
  {
    "threadId": "1223769572717498419",
    "name": "fastembed Cloud Support or server requirements",
    "messages": "Does qdrant cloud already support fastembed on the server side for the queries, similar to weaviate vectorizers? With this I could directly search with my query string.\n\nIf not, are there recommendations for the server resource requirements roughly? I am using 'intfloat/multilingual-e5-large' and I am worried that scaling my server cpu will be complicated for the query embedding generation when I have some users querying in parallel."
  },
  {
    "threadId": "1222260948648202394",
    "name": "Issue datetime range filtering",
    "messages": "Hi, I am new member \nItтАЩs a pleasure to post it here ЁЯЩВ\nI have noticed an issue associated with filtering based on datetime range. IтАЩm trying to perform similarity search on vector store with filtering. \nYou can see an error below (attachment):\n\nCode snippet:\nfrom qdrant_client.http import models as rest\nfrom qdrant_client.http.models import Filter, FieldCondition, MatchValue\n \nqdrant.similarity_search(\n    query=query_input,\n    k=3,\n    filter=rest.Filter(must=[\n        models.FieldCondition(\n            key=\"metadata.Kategoria\",\n            match=models.MatchValue(value=\"ABC\"),\n        ),\n        models.FieldCondition(\n            key=\"metadata.Podkategoria\",\n            match=models.MatchValue(value=\"XYZ\"),\n        ),\n        models.FieldCondition(\n            key='metadata.Data rejestracji',\n            range=models.DatetimeRange(\n                gte=\"2022-05-15T14:00:41\"\n            ),\n        ),\n    ]) \n)"
  },
  {
    "threadId": "1222362785023791185",
    "name": "Qdrant suggested snapshot best practices when deployed to an AKS cluster with official helm chart.",
    "messages": "Hi all!\n\nI have deployed  a couple 3 node 1.8.3 Qdrant clusters via helm to AKS clusters in Azure. I am currently focusing on automating snapshots and restores.\n\nRight now, I am modifying the helm chart to attach a VolumeClaimTemplate to the snapshot folder location on each pod in the stateful set. I am also running a k8s job that I created which iterates through each sts endpoint and collection and takes a snapshot of the each. I have also attached the snapshot pvcs and an Azure file share to the k8s job. I do know that I can attached the files share as the path for snapshots, but its really hard to tell which node in the cluster the snapshot originated from since they are all named similarly. \n\nWith that said, I do have a few questions.\n\n1. Is it possible to uniquely name a snapshot?\n2. Is there a recommended approach for taking and storing multinode snapshots in Azure? \n3. Is there something similar to an Elasticsearch repo plugin available for Qdrant? \n4. Is there a way to check the status of a currently running snapshot?  I am running snapshots with 'wait=false''. The only GET I see is to download snapshots.\n5. Does the snapshot in the snapshots folder only show up once the snapshot has completed?\n6. Does a snapshot creation/deletion impact data ingestion/search?\n7. Can snapshot creation happen at the same time as deletion?\n8. In the snapshots section of a collection in the Qdrant dashboard, the snapshots that are available seem to alternate between snapshots on each node when the browser page is refreshed. Is this the expected way to view all snapshots on all nodes when viewing the dashboard?"
  },
  {
    "threadId": "1223105029506990121",
    "name": "Recovering collection use too long to start",
    "messages": "Hi everyone. I'm facing a problem after restart qdrant in docker.\n\nhere is my console logs:\n\n```\n2024-03-29T02:37:41.197096Z  INFO storage::content_manager::consensus::persistent: Loading raft state from ./storage/raft_state.json\n2024-03-29T02:37:41.220462Z  INFO storage::content_manager::toc: Loading collection: jingask\n2024-03-29T02:37:41.495067Z  INFO collection::shards::local_shard: Recovering collection jingask: 0/51710 (0%)\n2024-03-29T02:38:41.803126Z  INFO collection::shards::local_shard: 112/51710 (0%)\n2024-03-29T02:39:42.152482Z  INFO collection::shards::local_shard: 223/51710 (0%)\n2024-03-29T02:40:42.606059Z  INFO collection::shards::local_shard: 334/51710 (0%)\n2024-03-29T02:41:42.990821Z  INFO collection::shards::local_shard: 447/51710 (0%)\n2024-03-29T02:42:43.056916Z  INFO collection::shards::local_shard: 556/51710 (1%)\n2024-03-29T02:43:43.172029Z  INFO collection::shards::local_shard: 665/51710 (1%)\n2024-03-29T02:44:43.314831Z  INFO collection::shards::local_shard: 774/51710 (1%)\n2024-03-29T02:45:43.454485Z  INFO collection::shards::local_shard: 886/51710 (1%)\n2024-03-29T02:46:43.860092Z  INFO collection::shards::local_shard: 999/51710 (1%)\n2024-03-29T02:47:44.040174Z  INFO collection::shards::local_shard: 1106/51710 (2%)\n2024-03-29T02:48:44.383892Z  INFO collection::shards::local_shard: 1215/51710 (2%)\n2024-03-29T02:49:44.393449Z  INFO collection::shards::local_shard: 1322/51710 (2%)\n2024-03-29T02:50:44.559853Z  INFO collection::shards::local_shard: 1430/51710 (2%)\n2024-03-29T02:51:44.746735Z  INFO \n```\n\nit takes half an hour to reach 10% or less."
  },
  {
    "threadId": "1223291343065841704",
    "name": "Adding lots of documents - database always turns red",
    "messages": "Background: I have embedded and saved a 1.6 GB dataset to my drive in 384 word chunks because that is the input size for my embedding model: sentence-transformers/all-mpnet-base-v2. I then transfer these embeddings and original texts to qdrant by running a loop to upsert batches of 100 PointStructs at a time into the database.\n\nProblem: When I come back when the loop is finished I see that the number of indexed vectors is much less than the vectors count (~6% of total vectors). The database is also status red. I can't find in the docs what status red means and if this means the indexed docs are the only vectors allowed to be searched. Is there a way to fix the red issue and reindex all of the vectors and/or improve my process?\n\nI can provide code if needed! Thanks!"
  },
  {
    "threadId": "1222672135731019777",
    "name": "Unable to reproduce benchmark results",
    "messages": "Hi I am trying to emulate the vector benchmarking that is mentioned on the qdrant official website, but instead of docker I am using VMs for my use case. I have created a 3node cluster with default configurations and my collection has 1M vectors without any payload and each vector has 512 dimensions -- very similar to the kind of dataset used for benchmarking. \nThe results I am getting are nowhere near to the ones published in the benchmark. Am I missing something or qdrant performs better in dockerised env than VMs? Can someone help me understand this? \nWhile setting up, I have followed the same config as mentioned on the QDrant website."
  },
  {
    "threadId": "1221571139382284470",
    "name": "Qdrant TLS with Letsencrypt",
    "messages": "Hello, I'm trying to self host qdrant on a VPS. Using Nginx to install Letsencrypt and port it over to a domain.\n\nWhen I change the config to use the certificate, I can't access qdrant anymore.\n\nAny tips?"
  },
  {
    "threadId": "1223243180321935481",
    "name": "Updating config of existing collection",
    "messages": "Hi, I was in the process of upserting a dataset to qdrant free cluster, and hit the RAM limit.\n\nOne of the suggestions is to store the vectors on disk (mmaped files).\nI want to do that by issuing the following REST API query:\n```\nPUT /collections/openai-community-posts\n{\n  \"vectors\": {\n    \"post_content_cluster_embedding\": {\n      \"size\": 768,\n      \"distance\": \"Cosine\",\n      \"on_disk\": true\n    },\n    \"post_content_classification_embedding\": {\n      \"size\": 768,\n      \"distance\": \"Cosine\",\n      \"on_disk\": true\n    },\n    \"post_content_search_document_embedding\": {\n      \"size\": 768,\n      \"distance\": \"Cosine\",\n      \"on_disk\": true\n    }\n  }\n}\n```\nbut the result says `{  \"error\": \"Wrong input: Collection `openai-community-posts` already exists!\"}`\nWhat is the correct way to update the config of an existing index/colleciton?"
  },
  {
    "threadId": "1234070684238479471",
    "name": "Recommend API - exclude positive vectors from result",
    "messages": "When using the recommend API with best score strategy, the result of the recommend query contains the positive vectors passed as input. Is there a way to tell Qdrant to not include them in the result?"
  },
  {
    "threadId": "1222521894528483338",
    "name": "408 (Request Timeout)",
    "messages": "Getting http://0.00./collections/tst/points/search/batch"
  },
  {
    "threadId": "1223139669399834674",
    "name": "rename collection",
    "messages": "How do I rename and existing collection? I tried the update_collection_aliases method to do it. But it doesn't seem to change the collection_name inplace on the Qdrant dashboard. Any ideas?"
  },
  {
    "threadId": "1222420715219849308",
    "name": "Scroll points is taking more time",
    "messages": "I'm creating indices in a collection based on payload data. When I search, I filter using these index keys. Before making a search request, I need to verify whether a particular payload index exists in the collection or not. For this purpose, I'm using the \"scroll points\" method (with return vectors and payload set to False and limit is 1) result. However, this method takes around 15 milliseconds, which is quite significant. I'm seeking an alternative approach that can perform this check more quickly.\n<@844295650400534599>"
  },
  {
    "threadId": "1222589846028288030",
    "name": "When creating snapshots, only checksums are left",
    "messages": "Hello, I try to create snapshots which exist beforehand some days ago, but when trying to create new ones only checksums are left.\n\nWhy?\n\n```\n# cd 485e999c185a4447be5b8e1d_1038025315506867898\n# ls\n485e999c185a4447be5b8e1d_1038025315506867898-7597366898200330-2024-03-25-16-53-30.snapshot         485e999c185a4447be5b8e1d_1038025315506867898-7597366898200330-2024-03-27-15-57-33.snapshot.checksum\n485e999c185a4447be5b8e1d_1038025315506867898-7597366898200330-2024-03-25-16-53-30.snapshot.checksum\n```"
  },
  {
    "threadId": "1222556490045325355",
    "name": "Excessive use of RAM crashes application.",
    "messages": "Hi all,\n\nI am developing a system using langchain. It runs on a 64GB RAM 16 core machine with 16TB of flash mass storage, so a swap file is not an option, there is also a Tesla P40, but I don't think it matters because running everything on the CPU has the same results (although things like embedding get much slower).\nThe OS is Ubuntu Linux 22.04. All the python dependencies are in a Python 3.10 venv with dependencies themselves installed via pip.\n\nThe initial tests were okay, but now that the database is being filled, the RAM requirements are out of scale. The current database is around 1M points, 1024 bit embedding vector, the on disk size is 10.4GB, and was created with swap enabled by mistake, the used swap was around 40GB. Trying to just load that database without any swap result in the process killed by the OS because of an out of memory error.\n\nThe final size of the database is expected to be in the order of 100M points.\n\nThe database was created and updated using a tool, this edited bit is the one that pushes documents into the database:\n\nfor element in pdf_list:\n\n      filename = element[0]\n      loader = PyPDFLoader(filename)\n      docs = loader.load_and_split(splitter)\n\n      if None == qdrant:\n          # not initialized yet, do it now\n          texts = []\n          metas = []\n          for doc in docs:\n            texts.append(doc.page_content)\n            metas.append(doc.metadata)\n\n          qdrant = Qdrant.from_texts( texts, embeddings, metas, path=PERSIST_DIRECTORY, collection_name=\"the_data\", on_disk_payload=True, on_disk=True)\n\n      else:\n          qdrant.add_documents(docs)\n\n\nFollowing some guides, searches in internet and documentation, I edited the meta.json file, the one attached to this message. \n\nBut stills fills the RAM beyond capacity.\n\nAny help is appreciated.\n\nCheers."
  },
  {
    "threadId": "1233077506555510794",
    "name": "Fix the Position brackets to run & check the json",
    "messages": "POST /collections/documentos_xxxxxx_llama2_7b/points/search\n{\n  \"vector\": [...],\n  \"filter\": {\n    \"must\": [\n      {\n        \"key\": \"page_content\",\n        \"match\": {\n          \"text\": \"nos permite entender padr├╡es e volumes de deslocamentos,\"\n        }\n      }\n    ]\n  },\n  \"limit\": 10\n}\nERROR:\n\n{\n  \"method\": null,\n  \"endpoint\": null,\n  \"reqBody\": null,\n  \"error\": \"Fix the Position brackets to run & check the json\"\n}\nI've already checked the brackets and didn't see the error!"
  },
  {
    "threadId": "1233053362837262416",
    "name": "Application Search about text in field \"page_content\".",
    "messages": "How do I search for words in a field  text?"
  },
  {
    "threadId": "1233019400987344926",
    "name": "hi getting error, ... ient.http.exceptions.ResponseHandlingException: timed out",
    "messages": "hi i have deployed qdrant docker container to my private cloud aks cluster. But I am unable to connect via python client.. i can do curl request but when i try to send data i get connection time out error.. \n    raise ResponseHandlingException(e)\nqdrant_client.http.exceptions.ResponseHandlingException: timed out"
  },
  {
    "threadId": "1232726403913486489",
    "name": "Qdrant Docker Instance",
    "messages": "I have deployed docker image on my machine, But from today I'm getting this error in docker logs:\n\n```py\n2024-04-24T16:10:46.244112Z ERROR collection::update_handler: Failed to flush: Service runtime error: Failed to flush id_tracker mapping: Service runtime error: RocksDB flush_cf error: IO error: While open a file for appending: ./storage/collections/PSO_customer_support/0/segments/0dfd7824-2e09-494f-8ab5-1302f4beda57/010746.sst: Too many open files\n2024-04-24T16:10:46.524257Z ERROR actix_server::accept: Error accepting connection: Too many open files (os error 24)\n2024-04-24T16:10:47.024796Z ERROR actix_server::accept: Error accepting connection: Too many open files (os error 24)\n\n```\n\nDue to this error I'm unable to see the Qdrant UI and also not able to connect to the instance with my app"
  },
  {
    "threadId": "1232296641495830538",
    "name": "UnexpectedResponse: Unexpected Response: 403 (Forbidden) with local client",
    "messages": "Hello everyone!\n\nI want to use qdrant to store my embeddings but I always get this 403 error. here is my code : \n\nin terminal :\n```py\nunset http_proxy\nunset https_proxy\n\ndocker run -p 6333:6333 \\\n    -v $(pwd)/qdrant_storage:/qdrant/storage \\\n    qdrant/qdrant\n```\nin notebook :\n\n```\nfrom fastembed.embedding import TextEmbedding\nfrom qdrant_client import QdrantClient\nimport pandas as pd\n\nembedding = TextEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\ndf=pd.read_csv('my_path')\ndf=df[0:5]\nembeddings = list(embedding.passage_embed(df['lemmatize_words'], axis=1))\n# Connect to Qdrant\nclient = QdrantClient(host=\"localhost\", port=6333)\n\n# Cr├йation d'une nouvelle collection\nmy_collection = \"test_collection\"\ntry:\n    # Suppression de la collection si elle existe d├йj├а (pour ├йviter des erreurs de duplication)\n    client.delete_collection(collection_name=my_collection)\n    \n    # Cr├йation de la collection\n    test_collection = client.create_collection(\n        collection_name=my_collection,\n        vectors_config=VectorParams(size=100, distance=Distance.COSINE)\n    )\n\n    # Pr├йparation des ID des points pour chaque embedding\n      point_ids = list(range(len(embeddings)))\n\n    # Upload des embeddings dans la collection\n    upload_result = client.upsert(\n        collection_name=my_collection,\n        points=[\n            {\n                'id': point_id,\n                'vector': embedding,\n                'payload': {}  # Optionnellement ajouter des m├йtadonn├йes\n            }\n            for point_id, embedding in zip(point_ids, embeddings)\n        ]\n    )\n\n    print(upload_result)\nexcept Exception as e:\n    print(f\"Une erreur est survenue : {e}\")\n```\n\nhave you an idea of the problem?"
  },
  {
    "threadId": "1232189142398668800",
    "name": "Use Quantization after upload collections",
    "messages": "Hello, I tried to optimize my VDB (both accuracy and speed). I tried reading some of the threads here and I found that sometimes Qdrant didn't fully quantize the vectors if you put the `quantization_config` after you upload the collections. I saw that if you see through the telemetry like this (https://discord.com/channels/907569970500743200/1224989631842488340/1225421020836397076) then you can actually see whether the collections is quantized or not.\n\nWhat makes me confuse is that in one part, there's the config but in other part's there's None. Which of the telemetry should I look if I want to see if my collections is already quantized or not?"
  },
  {
    "threadId": "1222503737222762699",
    "name": "Ordering based on param",
    "messages": "Is it possible that when applying search we can order the result based on the param. I want to get the text of each point_id the same way it got in so I can recreate the input content.\n\nI was thinking something like this  but is not quite working\n`response = client.search(\n            collection_name=\"your_collection_name\",\n            filter=filter_conditions,\n            with_payload=True,\n            limit=1000,  # Adjust based on your needs\n            sort=[\n                {\"key\": \"inserted_at\", \"order\": Ascend}  # Adjust sorting as needed\n            ],\n        )`"
  },
  {
    "threadId": "1221956515083063346",
    "name": "6000 Tb data in the vector store",
    "messages": "Hey, \nwe are gonna build RAG system with internal KB. Approximately we have 6000Tb data that we want to put into the store. 300 daily active users. \neach user can upload a document into a personal conversation. \neach document category will be part of different indexes. \n\nHelp me to understand better of qdrant configuration.\nMy assumption is starting with 16Gb,  2 or 4vcpu, 10 Nodes(not sure), 1Tb Disk space (total 10Tb)\n\nthanks for your thoughts and advice."
  },
  {
    "threadId": "1222478432110252052",
    "name": "Spark: Creating payload index",
    "messages": "Hi All. I'm writing my dataset to Qdrant using spark. I have vectors that I have indexed, no issues with that. But I have payload fields to index as well. Which parameter in spark will allow me to do this?"
  },
  {
    "threadId": "1222436460913754142",
    "name": "vectors_count  тЙа indexed_vectors_count",
    "messages": "Whenever I upsert vectors into a collection there always 16.000 vectors, with 8.000 points but just 7997 indexed_vectors_count. (Numbers are just for explanation, but the values are similiar to this)\n\nI am upserting both dense and sparse vectors, which are both being displayed on the points.\n\nI read that this can be normal but wanted to ask if this can have negative impacts when retrieving.\n\nIf this might have negative impact on the retrieval process I would be happy to get an explanation how to fix it.\n\nThank you in advance!\n\n\nconfig:```\n{\n  \"params\": {\n    \"vectors\": {\n      \"text-dense\": {\n        \"size\": 1536,\n        \"distance\": \"Cosine\",\n        \"on_disk\": false\n      }\n    },\n    \"shard_number\": 1,\n    \"replication_factor\": 1,\n    \"write_consistency_factor\": 1,\n    \"on_disk_payload\": true,\n    \"sparse_vectors\": {\n      \"text-sparse\": {\n        \"index\": {\n          \"on_disk\": false\n        }\n      }\n    }\n  },\n  \"hnsw_config\": {\n    \"m\": 16,\n    \"ef_construct\": 100,\n    \"full_scan_threshold\": 10000,\n    \"max_indexing_threads\": 0,\n    \"on_disk\": false\n  },\n  \"optimizer_config\": {\n    \"deleted_threshold\": 0.2,\n    \"vacuum_min_vector_number\": 1000,\n    \"default_segment_number\": 0,\n    \"max_segment_size\": null,\n    \"memmap_threshold\": null,\n    \"indexing_threshold\": 20000,\n    \"flush_interval_sec\": 5,\n    \"max_optimization_threads\": null\n  },\n  \"wal_config\": {\n    \"wal_capacity_mb\": 32,\n    \"wal_segments_ahead\": 0\n  },\n  \"quantization_config\": null\n}```\n\n\nHere a screenshot about my Collection info:"
  },
  {
    "threadId": "1222446825232007169",
    "name": "Spark-Qdrant Sharding",
    "messages": "Hi All,\n\nI am upserting records to qdrant. I want to shard the collection on the basis of a field. The field has 11 unique values(i.e \"1\",\"2\"......,\"11\"). This field should decide which record goes into which shard. How do I do that? I used the shard_key_selector parameter. It says \"INVALID ARGUMENT: shard_key 1 not found\". What is the work around for this?"
  },
  {
    "threadId": "1222097300369969224",
    "name": "Applying filters cause performance degradation",
    "messages": "Applying filters cause performance degradation\nWe use Qdrant in production. It performed good enough. In last update we decided to use filters to filter out old vectors in search result. It cause a degradation in Qdrant performance. Response timeтАЩs increased from 8ms up to 4s through several hours after filters were applied.\n\n\n\n### Settings\nWe use Qdrant in k8s via official helm chart\nQdrant version: 1.8.3\nWe connect to Qdrant via grcp from python client.\n\nCollection settings\nsee json file.\nvector count in collection: 161624 \n\nOs version: Debian GNU/Linux 12 (bookworm)\nCPU: requests: 11cpu\nRAM: requests: 2Gi\nSize of storage: ssd disks 200gb\n\n\n### Logs\nMonitor logs:  Qdrant search batch operation executes in 4 seconds\n\n|     debug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"Deadline Exceeded\", grpc_status:4, created_time:тАЭ\"}тАЭ\n\n4seconds - timeout of our client\n\n### Common comments\n\nwe created an index for payload and moved payload into RAM.\nHere is a code how we apply filter\n```\nfrom qdrant_client.models import (\n    Filter,\n    SearchRequest,\n)\n\nfilter = Filter(\n    must=[\n        FieldCondition(\n            key=\"loaded_at\",\n            range=Range(\n                gte=1711440147,\n            ),\n        )\n    ]\n)\n  \nrequest = SearchRequest(\n                vector=np.array(vector).tolist(),\n                filter=filter,\n                with_payload=True,\n                with_vector=True,\n                limit=limit,\n            )\n...\nawait asyncio.gather(..)\n```"
  },
  {
    "threadId": "1222226341714464808",
    "name": "Binary Embedding Billing",
    "messages": "Hey I wanted to know how the Binary Embeddings are billed. I didn't saw an option on the price calculator for it."
  },
  {
    "threadId": "1222198537346089050",
    "name": "Error while deleting points from Qdrant Node",
    "messages": "We are getting errors in qdrant node logs while trying to delete points. It started happening from last couple of hours. The upserts seem to run fine. Could someone pls help?"
  },
  {
    "threadId": "1204868366242943037",
    "name": "Created ingress on quadrant http service to enable ssl.",
    "messages": "- we have up and running ingress load balancer with my doamin host name on 443.\n- We are wondering how to connect to the qdrant endpoint using ingress as i did not see any document to pass the cert file parameter.\n- We are using our custom certificates. we need to pass them as client."
  },
  {
    "threadId": "1222122741285785611",
    "name": "Discard very similar vectors.",
    "messages": "Hello, I have my qdrant collection. Some of the vectors inside this collection are very similar to each other. If I use a k=30 for example and with the output I compute the cosine similarity of all the pairs possible, I see that some of them are very high, above 0.9.\n\nI was wondering if there is an option inside qdrant to discard from the output/results the vectors that are very similar to each other. If not, looking for advice on the best way to approach this.\n\nExample:\nIf in my output I have:\n\"sun glasses\", \"sunglasses\", \"necklace\" I would like to only have as a result \"sunglasses\" and \"necklace\" due to \"sun glasses\" and \"sunglasses\" being very similar to each other."
  },
  {
    "threadId": "1222104811445686292",
    "name": "Time out using search_batch",
    "messages": "I upserted 6.000 points with each dense and sparse vectors. (model: ada-002 & thenlper/gte-base)\n\nWhen I try to use search_batch with both vectors it times out after 5 seconds using the same models.\n```qdrant_client.http.exceptions.ResponseHandlingException: The read operation timed out```\n\nWhenever I try to use a different model (for sparse retrieval), it works fine.\n\nWhat could be causing this and how can I troubleshoot this?\nThanks in advance!"
  },
  {
    "threadId": "1222959173495558265",
    "name": "Unable to Benchmark Qdrant trough VectorDB bench",
    "messages": ""
  },
  {
    "threadId": "1229344674159001611",
    "name": "Qdrant takes 25 second to retrieve 1000 single products",
    "messages": "So, before I try to blame Qdrant, I explained the problem in details and what comes to my mind in this question:\nhttps://stackoverflow.com/questions/78326924/qdrant-takes-25-second-to-retrieve-1000-single-products\nPlease share any helpful tips that comes to your mind. I appreciate your time and help deeply."
  },
  {
    "threadId": "1230441554020139178",
    "name": "Qdrant + FastAPI pagination",
    "messages": "I am trying to use FastAPI alongside with Qdrant to get products using search or recommend apis. But the problem is with the pagination. Neither FastAPI nor Qdrant does not provide a detailed docs or a good way to do this. \nFor example, in Django, it first queries the DB in a lazy manner, saving the primary results in the database storage (NOT the RAM), and then retrieves the result by pagination parameters. \nI can handle pagination in FastAPI using some packages like fastapi-pagination, but it is not a good practice at all, as there is no integration between fastapi-pagination package and qdrant, so this package will save the primary results in RAM instead of storage, which for a service with a lot of users, is not a good idea at all. \nI wonder if there is a way for this problem, which is a really important one in the real world applications specially for those with a lot of users."
  },
  {
    "threadId": "1216788494685442209",
    "name": "High Memory Usage",
    "messages": "Hi, i'm currently try to build image search similarity system with some payload filtering\nMy `embeddings` is essentially phash: https://www.phash.org/ . The volume is 2B+\n\nI have 3 nodes with 16CPU and 30Gb RAM each\nI tried to ingest subset of my data(200M) but my nodes crashed after 100M because of OOM...\nQdrant version 1.8.0\n\nGET /collections/{collection_name} \n\n`{\"result\": {\"status\": \"green\", \"optimizer_status\": \"ok\", \"vectors_count\": 98574239, \"indexed_vectors_count\": 14519082, \"points_count\": 985526205, \"segments_count\": 78, \"config\": {\"params\": {\"vectors\": {\"size\": 64, \"distance\": Manhattan, \"hnsw_config\": {\"m\": 0, \"ef_construct: 128, \"full_scan_threshold\": 2000, \"on_disk\": true, \"payload_m\": 16}, \"quantization_config\": {\"binary\": {\"always_ram\": true}}, \"on_disk\": true}, \"shard_number\": 9, \"replication_factor\": 1, \"write_consistency_factor\": 1, \"on_disk_payload\": true}, \"optimizer_config\": {\"deleted_threshold\": 0.2, \"vacuum_min_vector_number\": 1000, \"default_segment_number\": 0, \"max_segment_size\": null, \"memmap_threshold\" 200, \"indexing_threshold\": 1000, \"flush_inverval_sec\": 5, \"max_optimization_threads\": null}, \"wal_config\": {\"wal_capacity_mb\": 32, \"wal_segment_ahead\": 0}, \"payload_schema\": {\"field_1\": {\"data_type\": \"integer\", \"points\": 98526205}, \"field_2\": {\"data_type\": \"keyword\", \"points\": 98525406}, {\"field_3\": {\"data_type\": \"integer\", \"points\": 98525406}, \"group_id\": {\"data_type\": \"integer\", \"points\": 98525170}}}, \"status\": \"ok\", \"time\": 0.002288314}`\n\nHow can i decrease memory usage? It seems like everything is on disk except quantized version of vectors and payload indexes.\nIndexes for fields(except group_id) are mostly used just to filter out irrelevant results and not for search speed, so these indexes probably may be removed, but anyway memory usage seems to high considering 64dim binary vectors\n\nby the way after crush \"status\" became \"green\", meaning that indexing stopped?"
  },
  {
    "threadId": "1221912328765702234",
    "name": "Cluster stuck on \"restarting\"",
    "messages": "I am on Qdrant Cloud 1.8.3 (free tier) and my Cluster is stuck on \"restarting\".\nBackground:\nI upserted 6.000 vectors as sparse vector and when I came back to check I got an error in the cluster, so I restarted it and now it is stuck in this."
  },
  {
    "threadId": "1221421908092915762",
    "name": "FastEmbed default is sucky?",
    "messages": "Hi, I want to check something. \n\nI was reading this:\nhttps://github.com/qdrant/fastembed/issues/22\n\nThis made me switch back to the default which is BAAI/bge-base-en\n\nBut the similarity of results with BAAI/bge-base-en actually seem worse than with sentence-transformer.\n\nDid anybody experience the same? I am recreating the qdrant collections right now just in case to confirm. I will post some examples soon..\nMy code for fastembed is:\n```python\nfrom typing import List\nfrom fastapi import FastAPI\nfrom fastembed import TextEmbedding\nfrom pydantic import BaseModel\n\nclass Documents(BaseModel):\n    documents: List[str]\n\napp = FastAPI()\nembedding_model = TextEmbedding()\nprint(\"The model BAAI/bge-small-en-v1.5 is ready to use.\")\nprint(embedding_model)\n\n@app.post(\"/embed\")\nasync def embed(documents: Documents):\n    embeddings = list(embedding_model.embed(documents.documents))\n    embeddings = [embedding.tolist() for embedding in embeddings]\n    return embeddings\n\n@app.get(\"/dimensions-size\")\nasync def params():\n    return 384\n```"
  },
  {
    "threadId": "1219212735489900625",
    "name": "similarity score and retrieved data.",
    "messages": "<@893511736441843785> \nWhy does the vector database always retrieve data even if the query is non-meaningful? Also, regarding similarity search, the similarity score is always high for all retrieved data, even for non-relevant text. Because of this, I am unable to set a threshold. How can I get only relevant text, and how can I optimize the similarity score?"
  },
  {
    "threadId": "1221863803738914936",
    "name": "Storage location as env var?",
    "messages": "Is there an environment variable that can be set to set the volume for storage? I see in the docs that passing in -v is one method, but not seeing if this can be set via environment"
  },
  {
    "threadId": "1221820885875560520",
    "name": "Qdrant Diagrams",
    "messages": "Slightly random question but are you able to say what tool you use for the diagrams in your documentation?\n\nExample here: https://qdrant.tech/documentation/overview/#high-level-overview-of-qdrants-architecture"
  },
  {
    "threadId": "1220700100632772660",
    "name": "How to perform topic detection and map it to a hierarchical taxonomy",
    "messages": "We have a text comprising of various metadata (like title description, text from images ..) and we would like to perform topic detection and map it to a hierarchical taxonomy like IAB, something similar to what the following tool does. What would be a preferred approach if we use qdrant's features along with other libraries\n\nhttps://www.assemblyai.com/blog/new-improved-topic-detection-and-iab-classification/"
  },
  {
    "threadId": "1221749900916817941",
    "name": "Delete Index or node",
    "messages": "how to delete specific not node with node its ID, by compare metadata"
  },
  {
    "threadId": "1220795185928339586",
    "name": "Filtering very slow",
    "messages": "Hello there, I have a collection with about 10M vectors. Each vector has a payload with a field \"uuid\" (which is not the qdrant point's ID). I need to filter on this field, so I created an index. But the filtering is really slow and get a timeout in Python after 60 seconds.\n\nHere is the qdrant log\n`2024-03-22T17:53:53.430137Z ERROR qdrant::tonic::logging: gRPC /qdrant.Points/Search unexpectedly failed with Internal error \"Service internal error: 1 of 1 read operations failed:\\n Timeout error: Operation 'Search' timed out after 60 seconds\" 60.001472`\n\nAnd here is my config\n```\n{\"status\":\"green\",\"optimizer_status\":\"ok\",\"vectors_count\":9972368,\"indexed_vectors_count\":9972368,\"points_count\":9239218,\"segments_count\":6,\"config\":{\"params\":{\"vectors\":{\"size\":512,\"distance\":\"Cosine\",\"on_disk\":true},\"shard_number\":1,\"replication_factor\":1,\"write_consistency_factor\":1,\"on_disk_payload\":true},\"hnsw_config\":{\"m\":16,\"ef_construct\":100,\"full_scan_threshold\":10000,\"max_indexing_threads\":0,\"on_disk\":false},\"optimizer_config\":{\"deleted_threshold\":0.2,\"vacuum_min_vector_number\":1000,\"default_segment_number\":0,\"max_segment_size\":null,\"memmap_threshold\":null,\"indexing_threshold\":20000,\"flush_interval_sec\":5,\"max_optimization_threads\":1},\"wal_config\":{\"wal_capacity_mb\":32,\"wal_segments_ahead\":0},\"quantization_config\":null},\"payload_schema\":{\"uuid\":{\"data_type\":\"keyword\",\"points\":0}}}\n```\nWhat am I missing?\n\nThanks for the help!"
  },
  {
    "threadId": "1221217886614454283",
    "name": "easiest way to feed large PDF files",
    "messages": "hi, which would be the easiest/more efficient ways to feed Qdrant with some large PDF files (500-700 pages) so we can search them later and craft LLM prompts to \"talk to the pdfs\"? Any code samples out there? Thanks!"
  },
  {
    "threadId": "1221678580551909558",
    "name": "Memory Usage Estimation",
    "messages": "Hello experts, may I ask if there is a formula to estimate the amount of memory required based on the number of collections and vectors? For example, with 1000 collections and 100000 vectors, how much memory is needed?"
  },
  {
    "threadId": "1221232115606224937",
    "name": "Disabling dashboard all together",
    "messages": "HI there, new here. I know that I can use API key for adding a layer of security to the dashboard as well, but I'm wondering if there is a way to disable dashboard all together? Since it shares the same endpoint as the HTTP API, I'm looking to expose that but disable the dashboard in production.  I didn't see any env vars or anything.\n\nI noticed that there are some 'least privileges' docker images, are those possibly configured with no dashboard?"
  },
  {
    "threadId": "1221439038943264818",
    "name": "change in QdrantClient search method?",
    "messages": "Was there a recent change in the QdrantClient search method, in the python package or something? Somehow my app doesn't work anymore."
  },
  {
    "threadId": "1225315545226809435",
    "name": "Concurrency in Qdrant",
    "messages": "Hi team, needed your help in understanding concurrency in Qdrant and that how many parallel requests can it handle. When we start the container it can be accessed on localhost:6333, so how many parallel request can it handle? I am running Qdrant with docker. No shards and replicas. Could you help me in this or point to any documentation/article?"
  },
  {
    "threadId": "1228046544046002177",
    "name": "Need suggestions in Meta Query filtering",
    "messages": "I need assistance to write a qdrant seach query with meta filtering.\n\nMy data In qudrant dashboard looks like this\n{\n    \"id\":1,\n    \"payload\":{\n        \"metadata\":{\n            \"category\":[\"category_1\",\"category_2\"],\n            \"country\":\"1001\",\n            \"data_id\":1,\n            \"data_type\":\"file\",\n            \"file_id\":\"1\",\n            \"timestamp\":\"2024-04-10T22:03:00Z\"\n        },\n        \"page_content\":\"Content\"}\n}\n\nI have to filter data with following conditions\n    1. With timestamp greater than less than particilar time.\n    2. Data where country code is matching\n    3. Data where category name matches.\n\nI have tried below query which is throwing AssertionError: Unknown arguments: ['scroll_filter'] \n\nclient.search(\n    collection_name=\"1002\",\n    scroll_filter=models.Filter(\n        should=[\n            models.FieldCondition(\n                key=\"metadata.file_id\", match=models.MatchValue(value=\"5\")\n            ),\n        ],\n    ),\n    search_params=models.SearchParams(hnsw_ef=128, exact=False),\n    query_vector=embeddings[0],\n    limit=3,\n)\n\nLet me know if I am missing anything in this query,"
  },
  {
    "threadId": "1229763715193110650",
    "name": "Find look alikes based on multiple verctors",
    "messages": "Hi,\nI have an article and I  dvided it in to paragraphs.\nI saved each  paragraph as a point along side with some article unique.\n\nI have an article in my hand, and want to search for similar articles in the database. \n\nif i was looking for similar paragraphs this was an easy task, one to one search.\n\nbut in my case, each article is constructed our from several paragraphs, and need to match many paragraphs to many paragrapsh and understand which article is the best match.\nWhats the best way to do that?\nIm kind of newbie in this world, so pls be gentle....\n\nThanks"
  },
  {
    "threadId": "1230293859280359468",
    "name": "Sparse Vector Indexing Optimazation",
    "messages": "Hi there. (Just for context, I don't use cloud platforms.)\n\nI'm exploring ways to optimize sparse vector indexing to reduce memory usage, similar to how quantization benefits dense vector indexing. \n\nFor example, quantization can reduce memory usage to a quarter of the original. However, when I add sparse vectors without quantization into the mix, the RAM cost for this hybrid setup increases by more than 5 times.\n\nWe've noticed that sparse vector indexing with `on_disk: true` is quite slow, but upgrading to larger machines is too costly. Any suggestions for improving this situation would be greatly appreciated.\n\nOr, are there any plans to introduce such optimizations in the future?\n\nThank you!"
  },
  {
    "threadId": "1220962003376668712",
    "name": "Metadata addition - LlamaIndex vs Qdrant SDK",
    "messages": "I'm currently using LlamaIndex with Qdrant as the vector database.\n\nWhen adding metadata to nodes via LlamaIndex like this:\n```\ndocument.metadata = {\n    \"source_id\": source_id,\n    \"document_name\": document_name\n}\n```\n\nand retrieving it with:\n```\nretriever = index.as_retriever(...)\nretrieved_nodes = retriever.retrieve(query)\n```\nI can access the added metadata through `retrieved_nodes[0].metadata`.\n\nHowever, when I add metadata using Qdrant's Python SDK (https://qdrant.tech/documentation/concepts/payload/#:~:text=%7D-,You%20donтАЩt%20need%20to%20know%20the%20ids%20of%20the%20points%20you%20want%20to%20modify.%20The%20alternative%20is%20to%20use%20filters.,-http), the metadata isn't returned by LlamaIndex's retrieval process, even though it's visible on the Qdrant UI.\n\nCould anyone please help with this?"
  },
  {
    "threadId": "1217811819431854110",
    "name": "Operation 'Search' timed out after 60 seconds",
    "messages": "Hello, I am getting this error `Operation 'Search' timed out after 60 seconds\" 60.052528` for the collection info below. Can you help me to look at the root cause of the issue?\n\nNote: unindexed but vectors_count is only 64008\n\n\n```\n{\n    \"result\": {\n        \"status\": \"green\",\n        \"optimizer_status\": \"ok\",\n        \"vectors_count\": 64008,\n        \"indexed_vectors_count\": 47624,\n        \"points_count\": 32004,\n        \"segments_count\": 5,\n        \"config\": {\n            \"params\": {\n                \"vectors\": {\n                    \"cohere_v3\": {\n                        \"size\": 1024,\n                        \"distance\": \"Dot\",\n                        \"on_disk\": true\n                    },\n                    \"ko_roberta_large\": {\n                        \"size\": 1024,\n                        \"distance\": \"Dot\",\n                        \"on_disk\": true\n                    }\n                },\n                \"shard_number\": 1,\n                \"replication_factor\": 1,\n                \"write_consistency_factor\": 1,\n                \"on_disk_payload\": true\n            },\n            \"hnsw_config\": {\n                \"m\": 16,\n                \"ef_construct\": 100,\n                \"full_scan_threshold\": 10000,\n                \"max_indexing_threads\": 0,\n                \"on_disk\": false\n            },\n            \"optimizer_config\": {\n                \"deleted_threshold\": 0.2,\n                \"vacuum_min_vector_number\": 1000,\n                \"default_segment_number\": 0,\n                \"max_segment_size\": null,\n                \"memmap_threshold\": 1024,\n                \"indexing_threshold\": 20000,\n                \"flush_interval_sec\": 5,\n                \"max_optimization_threads\": 1\n            },\n            \"wal_config\": {\n                \"wal_capacity_mb\": 32,\n                \"wal_segments_ahead\": 0\n            },\n            \"quantization_config\": null\n        },\n        \"payload_schema\": {}\n    },\n    \"status\": \"ok\",\n    \"time\": 0.000063299\n}\n```"
  },
  {
    "threadId": "1220445425195028510",
    "name": "Replication factor",
    "messages": "As for my limited knowledge, qdrant collections default replication factor to 1, what are the consequences of setting it to None?"
  },
  {
    "threadId": "1219287655523549204",
    "name": "Moving data from single-node to multi-node cluster",
    "messages": "Once snapshot is taken on a single node config - what is the proper procedure to migrate data to a multi-node cluster?"
  },
  {
    "threadId": "1220247755650367538",
    "name": "Filters + GET ids",
    "messages": "Hi there, \n\nI have a list of ids corresponding to points in my database\n\nI would like to retrieve them but applying filters to only retrieve those that match the filters\n\nWhat would be the correct way to do that?"
  },
  {
    "threadId": "1220292791301574748",
    "name": "Qdrant Cluster suspension due to inactivity in Free Tier",
    "messages": "Can someone tell me, after how long does a Qdrant Cluster get suspended due to inactivity, in FREE TIER?\n\nI am using Qdrant free tier for my chatbot. Chatbot is not being actively used these days and due to inactivity, cluster is getting suspended after some time."
  },
  {
    "threadId": "1220100959028641792",
    "name": "Batch Search should support with_payload and with_vectors",
    "messages": "https://qdrant.tech/documentation/concepts/search/#batch-search-api\n\nHello, when doing a batch search, it is impossible to retrieve the payload and vectors. It return something like \"ScoredPoint\", while a classic search returns also ScoredPoint but have the option to get payload with it.\nSee `search` and `search_batch` here: https://python-client.qdrant.tech/qdrant_client.qdrant_client.html?highlight=qdrantclient#qdrant_client.qdrant_client.QdrantClient\nShould I create an issue for this ?"
  },
  {
    "threadId": "1220038219492495471",
    "name": "Method to create point if it doesn't exist and update if it does in one operation",
    "messages": "Is there any way to have upload_points update payload and vectors if the point already exists instead of overwriting? I would like to be able to update both payload and vector at the same time without having to check if the point already exists. Thank you for your time!"
  },
  {
    "threadId": "1220039066884509779",
    "name": "multiple collection",
    "messages": "PUT /collections/{collection_name}\n{\n    \"vectors\": {\n      \"size\": 100,\n      \"distance\": \"Cosine\"\n    },\n    \"init_from\": {\n       \"collection\": \"{from_collection_name}\"\n    }\n} This is not working for me any suggestions?"
  },
  {
    "threadId": "1215318306811150387",
    "name": "Collection doest not exist",
    "messages": "Hi is anyone help me this, i am getting this while recreating and uploading the documents\n\"qdrant_client.http.exceptions.UnexpectedResponse: Unexpected Response: 404 (Not Found)\nRaw response content:\nb'{\"status\":{\"error\":\"Not found: Collection abc doesn't exist!\"},\"time\":0.000218834}'\""
  },
  {
    "threadId": "1215339775817678920",
    "name": "Build/Run Qdrant ubuntu18.04",
    "messages": "Hi. I need to build qdrant on ubuntu18.04. But after source docker building with changed system image (FROM debian:12-slim AS qdrant)  I got:\n```\noot@c763cf3ff654:/qdrant# ./qdrant \n./qdrant: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.32' not found (required by ./qdrant)\n./qdrant: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.33' not found (required by ./qdrant)\n./qdrant: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.34' not found (required by ./qdrant)\n./qdrant: /lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by ./qdrant)\n./qdrant: /lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by ./qdrant)\n./qdrant: /lib/x86_64-linux-gnu/libstdc++.so.6: version `CXXABI_1.3.13' not found (required by ./qdrant)\n```\nIt seems like cargo-chef builded on newer version. Is quick fix exist? Like make qdrant static and etc?"
  },
  {
    "threadId": "1219932047435497552",
    "name": "validation error for ParsingModel[InlineResponse2005] (for parse_as_type)",
    "messages": "Hi i am getting the this issue while uploading the data to the vector db so could anyone help me with this"
  },
  {
    "threadId": "1219726478397276201",
    "name": "How can vectors be created dynamically?",
    "messages": "I've created a collection and now want to upsert my Point to the collection. However, every example shows the vector being hard-coded with values. Shouldn't the vector values be dynamically generated? Do I have to provide the vector values? Is it possible to just upsert the Point and have the vector and Ids automatically generated?"
  },
  {
    "threadId": "1230426850912895057",
    "name": "Collection optimization options for a use case for rapidly inserting and querying points",
    "messages": "I have a data processing application with lots of small documents (e.g., news articles etc), where I have to ingest these documents into a database post de-dupe. For de-duping I am using Qdrant. I read these docs in a batch of 100 from a database, check for de-dupe in qdrant (in a range from last 2 days), do a bunch of processing, and write these articles back to database, and also to qdrant so that it can be used for de-dupe for the next batch. \n\nCurrently, I am running qdrant from docker on a singe AWS node with enough disk-space. This DB contains a few other collections which are small (~500 vectors). Only this collection that is being used for de-dupe contains ~3M docs (spanning 2 days). I have a separate cron job that deletes docs from this collection that are older than 2 days, so that the number of docs are around 3M. The delete job uses a search filter on a timestamp payload, which is indexed (as is available in the current version of qdrant). Each point has some small amount of payload, which I have moved to disk (`\"on_disk_payload\":true`)\n\nI am noticing that each batch_query with 100 docs starts off at ~0.35s and then keeps increasing linearly. At some point, the client starts timing out. \n\nQuestions:\n- Should the batch-query with 100 docs on a collection containing ~3M vectors take 0.35s? \n- What else optimizations I should use? Should I increase the shard count?"
  },
  {
    "threadId": "1229355832987353088",
    "name": "Is Qdrant a good fit for us?",
    "messages": "Hey!\n\nTrying to figure out if Qdrant is a good fit for us, little bit hard to navigate in the vector DB space. We've been using OpenSearch, but for our new feature the scale (start with 100M vectors with dim 512 as of now, but likely around 500M quite soon), and filtering requirements (most often 1-20% of the records relevant, but can be significantly more), this might be better dealt with using a dedicated vector DB. \n\nThe dataset will be quite static, maybe monthly updates on small parts of the docs to start with but later most likely 1-2 times a year. Latency is more important than RPS. Most likely couple of thousands request per month to start with.\n\nIs this workload ill suited for one index? One alternativ is to split up the docs into multiple indexes and then query those relevant to the query, but that comes with the overhead of managing multiple indexes, as well as more complex search logic.\n\nCould such a workload be well dealt with using Qdrant, you think? Would really appreciate any input and guidance!"
  },
  {
    "threadId": "1219584084553240637",
    "name": "Error while using python qdrant client",
    "messages": "Hi I am trying to run bellow code snippet but getting error.\n Its Qdrant 1.8.1 (Distributed mode) with qdrant-client==1.8.0\n\ncollection_info = client.get_collection(collection_name=collection_name)\nlist(collection_info)\n\nI am getting\n\nResponseHandlingException: 1 validation error for ParsingModel[InlineResponse2005] (for parse_as_type)\nobj.result.config.optimizer_config.max_optimization_threads\n  Input should be a valid integer [type=int_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.5/v/int_type\n\nQdrant container has log :\n2024-03-19T09:35:31.113681Z  WARN storage::content_manager::consensus_manager: Failed to apply collection meta operation entry with user error: Wrong input: Replica 8042769742135828 of shard 0 has state Some(Active), but expected Some(Initializing)"
  },
  {
    "threadId": "1219520318851383326",
    "name": "Need help with deployment",
    "messages": "Hi, I need help in deploying a Qdrant docker instance on the server, I have successfully done it using Docker, Nginx and Certbot for the SSL, the docker container works with https, I can see the collections and interact with it from the webui, but when I tried using it from the Python Client but I face an issue...\nqdrant_client.http.exceptions.ResponseHandlingException: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1007)\n\nIs there a clear guide on deployment?\n\nAny help in this matter will be greatly appericiated.\n\nRegards,\nRaj Kapadia"
  },
  {
    "threadId": "1218358268078850120",
    "name": "504 Gateway Time-out",
    "messages": "how to avoid 504 Gateway Time-out on taking snapshot via curl POST to collections/XXXXX/snapshots"
  },
  {
    "threadId": "1216611240785481778",
    "name": "search in data which vectors represents a list of strings",
    "messages": "My question is, If my data is list of food orders (strings), but with no fixed size; \n[\"meat\",\"milk\",\"egg\"]\n[\"ice\",\"milk\",\"tea\"] \n[\"meat\",\"juice\"] \n[\"meat\",\"milk\",\"egg\", \"pasta\" , \"pizza\"] \nWhat would be the best way to input this data in qdrant and to make the best searching results using embeddings?"
  },
  {
    "threadId": "1229736265562329089",
    "name": "Unexpected Response: 400",
    "messages": "Hi, I created an API (fastapi) for querying in Qdrant, given some points. It works fine (but slow) when I test by hand. But when I used wrk for benchmarking, I got an error. I don't know how to trace this.\nThnks"
  },
  {
    "threadId": "1214901889406210048",
    "name": "How to update every payload in a collection",
    "messages": "Hi! I have a collection created in Qdrant with a lot of points. I would like to know what it's the best way to upload all the payloads.\nI want to add a new property to each one of the payloads. I get all the points with the `client.scroll` method, iterating over them using the limit and offset properties. For each batch of points I calculate the new payload property value (each point has a particular one). The problem is updating this points again to the database. Updating one by one is raising a socket error (because it is creating a huge huge number of conections).\nI have tried to do a batch update like this:\n```\nclient.batch_update_points(\n    collection_name=collection_name,\n    update_operations=[\n        models.OverwritePayloadOperation(\n            overwrite_payload=models.SetPayload(\n                payload=[p.payload for p in new_points],\n                points=[p.id for p in new_points],\n            )\n        ),\n    ],\n)\n```\nBut I am getting a validation error because... payload expects the same dict for each point? I want to update each point's payload.\nSo, what would be the rigth approach to do this?\n\nThanks for the response"
  },
  {
    "threadId": "1229879126739910696",
    "name": "Config Details for Minimum RAM usage",
    "messages": "I am using free tier of Qdrant DB and I currently have 25K points, and counting, but I have already exhaused  66% RAM and processing power, please tell me config details such that RAM usage becomes minimum...I am ready to compromize disk space, below is my current configaration\n\n {\n    \"status\": \"green\",\n    \"optimizer_status\": \"ok\",\n    \"vectors_count\": 25446,\n    \"indexed_vectors_count\": 25446,\n    \"points_count\": 25446,\n    \"segments_count\": 2,\n    \"config\": {\n      \"params\": {\n        \"vectors\": {\n          \"fast-bge-small-en\": {\n            \"size\": 384,\n            \"distance\": \"Cosine\",\n            \"on_disk\": true\n          }\n        },\n        \"shard_number\": 1,\n        \"replication_factor\": 1,\n        \"write_consistency_factor\": 1,\n        \"on_disk_payload\": true\n      },\n      \"hnsw_config\": {\n        \"m\": 16,\n        \"ef_construct\": 100,\n        \"full_scan_threshold\": 10000,\n        \"max_indexing_threads\": 0,\n        \"on_disk\": false\n      },\n      \"optimizer_config\": {\n        \"deleted_threshold\": 0.2,\n        \"vacuum_min_vector_number\": 1000,\n        \"default_segment_number\": 0,\n        \"max_segment_size\": null,\n        \"memmap_threshold\": 10000,\n        \"indexing_threshold\": 20000,\n        \"flush_interval_sec\": 5,\n        \"max_optimization_threads\": 1\n      },\n      \"wal_config\": {\n        \"wal_capacity_mb\": 32,\n        \"wal_segments_ahead\": 0\n      },\n      \"quantization_config\": null\n    },"
  },
  {
    "threadId": "1227517848264511499",
    "name": "qdrant_demo_search based project help needed.",
    "messages": "I have a project to develop a search engine based on an API. The data comes from an ERP system containing approximately 10,000 companies across various categories. I need to be able to search using different filters, such as category and city. I've designed it based on the qdrant_demo with neural search, but I'm currently facing a challenge.\n\nFrom what I've observed in the qdrant_demo, it uses fastembed to search within the \"description\" field added to the payload. However, I would like it to search within the vector that I send to the data. If a result is found there, it should display what's in the payload. This is where I'm stuck. If you have any guidance, what would be the next step? Do I need fastembed, or should I proceed with a normal collection? How do I search within the vector and not just the \"document\"?\n\nI have the code in python on Github, https://github.com/lvalics/Django-SearchEngine\n\n{\n    \"id_value\": 1820,\n    \"id_key\": \"companyID\",\n    \"id_value2\": \"company\",\n    \"id_key2\": \"type\",\n    \"collection_name\": \"1_SearchEngineGP\",\n    \"payload\": {\n        \"companyID\": 1820,\n        \"document\": \"Insert fastembed code here to describe in a concise manner.\",\n        \"name\": \"Company X\",\n        \"address\": \"Address\",\n        \"url\": \"Website\",\n        \"description\": \"Short description\",\n        \"publicContactPhone\": \"0257-555-555\",\n        \"category\": \"Cosmetics\",\n        \"type\": \"company\",\n        \"etc\": \"Additional data for easier display.\"\n    },\n    \"data\": \"Extensive company information spanning 20-30 pages, including all data from the payload, comprehensive description, products, document details, etc., in an unstructured format.\"\n}"
  },
  {
    "threadId": "1229397862652055563",
    "name": "Setting up multitenancy.",
    "messages": "I am reviewing a strategy for implementing multitenancy by partitioning data using a payload key, specifically using `group_id `as shown in an example from an article. \nhttps://qdrant.tech/documentation/guides/multiple-partitions/#calibrate-performance\nIs `group_id` a reserved keyword for this purpose, or can I use any arbitrary field name for partitioning? \nAdditionally, does this partitioning strategy follow the same principles as other payload indexes (which we use i.e. for filters)?  \nI want to ensure that the methodology I'm applying for payload partitioning is consistent with best practices for indexing and filtering data."
  },
  {
    "threadId": "1218547908128018522",
    "name": "Nested filtering over a list of dictionaries",
    "messages": "I have a detailed question about filtering over a nested object field. I can not write the full question here due to character limits, but it is most appreciated if yur read my question here:\nhttps://stackoverflow.com/questions/78171813/qdrant-filteration-using-nested-object-fields"
  },
  {
    "threadId": "1217712801431289967",
    "name": "how add the new data in the qdrant db",
    "messages": "Hi is anyone help me with this \"I stored the documents in the database without specifying an ID for each chunk. Now, I want to add a new chunk to the database while ensuring that all existing chunks are retained. If duplicates are encountered, I want to overwrite them. Otherwise, I just want to append the new chunks.\""
  },
  {
    "threadId": "1229379448424370216",
    "name": "Target Machine Actively refused it",
    "messages": "So i have just opened create a new app to Inject PDF Data to my old QDRANT Free Tier.\n\nI also connected it with QDRANT using new Qdrant API and QDRANT HOST.\nAs i send the request to QDRANT to inject my Data, i get a big error on my console.\nand at the end, it is saying :\n\nqdrant_client.http.exception.ResponseHandlingException:   [WinError 10061] No connection could be made because the target machine actively refused it"
  },
  {
    "threadId": "1227622491120341002",
    "name": "Error accepting connection: Too many open files (os error 24)",
    "messages": "i have a fresh install of qdrant via docker. I have 3 collections, collection 1 and collection 2 we added 15K vectors each. While adding vectors on the 3rd collection, it went down with following error while it reached around 2.1K vectors.\n\n2024-04-10T14:05:49.521672Z ERROR collection::update_handler: Failed to flush: Service runtime error: Failed to flush id_tracker mapping: Service runtime error: RocksDB flush_cf error: IO error: While open a file for appending: ./storage/collections/coll3/0/segments/8fb41a90-4841-4965-bfb8-710bed56d588/000277.sst: Too many open files \n2024-04-10T14:05:49.803562Z ERROR actix_server:ЁЯЙС Error accepting connection: Too many open files (os error 24)\n\nis https://qdrant.tech/documentation/guides/common-errors/ the only way to solve this issue or am i doing something incorrect? Any help will be appreciated."
  },
  {
    "threadId": "1218438812607320126",
    "name": "How do I retrieve data from Azure SQL database?",
    "messages": "I already have my sql azure database/openai/qdrant connected. I also created a function for chunking data (using Langchain TextSplitter) and embeddings.\nMy last missing key piece is to retrieve data from Azure SQL database. My data is currently a table. Do I have to use Azure Blob Storage to store a bunch of .txt files in Azure SQL database to work?"
  },
  {
    "threadId": "1218066042870567032",
    "name": "Code to update records with new filters",
    "messages": "I need to write some code for the following:\n\nFor a given collection, find all records that have the following filter \"source_id\", with a given value. Then, I need to update all the records found with another filter \"document_name\", with a certain value.\n\nIs this possible with the Python SDK? Could someone point me to the right code examples or documentation for this, please? I have found the code to do the filtering, but not sure how to update."
  },
  {
    "threadId": "1218242177419968623",
    "name": "Full text index with match any",
    "messages": "Can I use match any filter on a full text index?"
  },
  {
    "threadId": "1218129904520925184",
    "name": "Search Filters problem",
    "messages": "I try to search with filter by group_id but looks like the results don't contain exactly what I need. \nExpected: List of results contains exactly records that matched in filters\nI use Qdrant Docker local, version 1.8.1 via Python client"
  },
  {
    "threadId": "1218003346493804636",
    "name": "Search Not Working on Large Collection",
    "messages": "We have a Langchain/OpenAI-based RAG solution (Python) using Qdrant as our vector db (thru AWS).  It has been in Beta with several clients for 2-3 months now.  We currently segregate each client by giving them a distinct collection.  Most clients have given excellent reviews and are very happy with testing results.  But one client recently said that they were uploading new documents and are not able to get answers (they basically get \"I don't know the answer\" to everything, which is how our prompt is configured).\n\nI decided to load their documents (approx. 40-50K small PDFs) into a new (temporary) collection earlier today.  I can search the vectors (thru web collection console) for the specific file name  that we are wanting to query.  It is definitely present in the collection.  But if I ask a specific question for a topic I see in the vector \"page content\", I get \"I don't know\" answer -- for EVERYTHING.   Again, all other client collections are working (near) perfectly when we ask questions.\n\nNow, I loaded the one single document that has our desired information (vs. 40-50K docs) into yet another temporary collection.  Same test, same questions -- WORKS PERFECTLY NOW!\n\nSo the first temporary collection has about 70K points.  The second temporary collection (single doc) has 11 points.\n\nMy only conclusion (based on above tests) is that collection size somehow is in play here.  Same document in both collections -- but when it is surrounded by 70K *other* docs, my Q&A search (from langchain) is not yielding an answer -- only when it is by itself.\n\nThoughts?  Thanks!    (Cluster was upgraded to 1.8.1)"
  },
  {
    "threadId": "1218191660635394068",
    "name": "read timeout",
    "messages": "What is the best way to troubleshoot ReadTimeout: The read operation timed out?\nI am running into these when i run batch jobs to add points into the index even as i use wait=False."
  },
  {
    "threadId": "1218167843737047181",
    "name": "Upgrade issue  (1.6.1 -> 1.8.1)",
    "messages": "Hello there!\nToday we decide to update our instances (we have 4 clusters)  via helm chart, from 0.6.1 (1.6.1) to 0.8.1 (1.8.1).\nWe have updated 3 clusters successfully, but our 4th cluster, with ~65kk vectors collection, had an issue.\nOur setup is very simple - 3-node cluster, 6 shards, replica factor=3.\nOne pod checks readiness probe, bot other two are not. Also in the output of than pods we got \n```\nтФВ 2024-03-15T10:30:57.534111Z  WARN collection::collection_manager::collection_updater: Update operation declined: No point with id 9429411762745619 found \nтФВ 2024-03-15T10:58:15.101991Z  WARN collection::collection_manager::collection_updater: Update operation declined: No point with id 10696049129088377 found\nтФВ 2024-03-15T10:58:15.110792Z  WARN collection::collection_manager::collection_updater: Update operation declined: No point with id 10696049129088377 found\n```\nBut there are INFO messages about successful collection recovery, which means (i think) that the data is persistent.\n\nShould we try to delete data from this two pods and try to resync\\replicate? Or mb there are some workarounds about this?\n\nI couldn't find any duplicated posts, so sorry if there is the same queston and thank you in advance тЭдя╕П"
  },
  {
    "threadId": "1217848044595183656",
    "name": "Connecting a retriever with langchain without adding new documents",
    "messages": "Hi,\nI'm building a RAG with groq, langchain and qdrant which is working finde so far. Now I have issues to connect qdrant (cloud) to a retriever without adding new documents. I just found a workaround where I can override existing data in the cloud. My working code is as follows: \n\n`url = \"......\"\n\nvectorstore = Qdrant.from_documents(\n    documents=splits,\n    embedding=embeddings,\n    url=url,\n    prefer_grpc=True,\n    api_key=api_key,\n    collection_name=\"qdrant_db\",\n    force_recreate=True)\n\nretriever = vectorstore.as_retriever()`\n\nPlease give me an advice."
  },
  {
    "threadId": "1218021197304823941",
    "name": "How to set alarms for cluster indicators",
    "messages": "I want to collect cluster indicators to Prometheus and set alarms"
  },
  {
    "threadId": "1217870587767296171",
    "name": "I have 10 collections",
    "messages": "How to query mutliple collections at a same time?"
  },
  {
    "threadId": "1217870295193489449",
    "name": "Keyword based search",
    "messages": "Hi , Is there a possibility of doing keywords based search along with vector search in qdrant?"
  },
  {
    "threadId": "1217372923388497920",
    "name": "Wrong input: Can\\'t create collection with name wave1. Collection data already exists at",
    "messages": "even after deleting i am getting the same issue may i know why this issue is comming?"
  },
  {
    "threadId": "1205595649433936026",
    "name": "how do I get qdrant to listen to ipv6?",
    "messages": "when I try to curl qdrant by ipv6 url I get **Connection refused**\n\nSo I checked netstat and i noticed that qdrant listen only ipv4\n\ntcp        0      0 0.0.0.0:6333            0.0.0.0:*               LISTEN      off (0.00/0/0)\n\nHow I can fix it?"
  },
  {
    "threadId": "1217500236906233856",
    "name": "Performance issue with Qdrant vector search APIs",
    "messages": "Setting:\nI am trying to check qdrant search speed at scale and stored everything on disk.\nPre-uploaded 1 millions vectors (with size = (512, )), all vectors were saved on disk\nQdrant podтАЩs resource:\nCPU limit: 700m \nMemory limit: 1Gb\n\nBehaviour:\nCreate a new vector with size (512, )\nDo search over 1 millions vectors in the Qdrant.\nSearching speed was very slow, it is about 45-60 sec\n\nExperiments:\nChanging CPU limit to 700m or 1000m--> The searching speed was improved, itтАЩs about 0.2-0.3 sec\nChanging memory limit to 2Gb  or 3Gb--> The searching speed was improved, itтАЩs about 0.3-0.4 sec\nCreate a new Qdrant pod (service) with initial CPU limit = 1000m, Memory limit = 3Gb --> The searching speed is also slow (about 45-50 sec)\n\nInstance speed . \n```\nInstance speed:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      40 bits physical, 48 bits virtual\nCPU(s):                             4\nOn-line CPU(s) list:                0-3\nThread(s) per core:                 1\nCore(s) per socket:                 4\nSocket(s):                          1\nNUMA node(s):                       1\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz\nStepping:                           6\nCPU MHz:                            2599.998\nBogoMIPS:                           5199.99\nVirtualization:                     VT-x\nHypervisor vendor:                  KVM\nVirtualization type:                full\nL1d cache:                          128 KiB\nL1i cache:                          128 KiB\nL2 cache:                           16 MiB\nNUMA node0 CPU(s):                  0-3\nMemory                              32Gb \n```\nAny help will be appreciated."
  },
  {
    "threadId": "1217680056483053628",
    "name": "Qdrant Cloud and Multilingual Full Text Index",
    "messages": "I'm using Qdrant Cloud right now and wasn't sure how to enable multiling-japanese for full text index.\nwhere do I put it if I'm using the console to add the index?\n```\nPUT /collections/{collection_name}/index\n{\n    \"field_name\": \"name_of_the_field_to_index\",\n    \"field_schema\": {\n        \"type\": \"text\",\n        \"tokenizer\": \"word\",\n        \"min_token_len\": 2,\n        \"max_token_len\": 20,\n        \"lowercase\": true\n    }\n}\n```"
  },
  {
    "threadId": "1216771443388321872",
    "name": "BM25   loading all documents into memory for indexing",
    "messages": "Hi, I am currently utilizing hybrid search with the Langchain and Qdrant database. However, when using the BM25 retriever, all documents are loaded into memory, causing potential issues. Is there any approach within Qdrant to mitigate this, or if not, how can we effectively manage memory usage while maintaining the hybrid search functionality with Qdrant?"
  },
  {
    "threadId": "1217568997479026789",
    "name": "Enable Hybrid Search when creating Vector Store",
    "messages": "When setting up my vector database I didn't use `enable_hybrid=True,` is there a way to do this after the fact, or would I need to rebuild the entire vector db from scratch?"
  },
  {
    "threadId": "1217429844485476413",
    "name": "Snapshots stored in S3",
    "messages": "Hi! I am starting to use Qdrant in the company I work and I am trying to set up a backup/snapshot mechanism but I am not able to find how (if possible) can I make the snapshots to be stored in S3. Can someone help me with it? Thanks!"
  },
  {
    "threadId": "1205106269875212308",
    "name": "Difference between optimizers_config.indexing_threshold and hnsw_config.full_scan_threshold",
    "messages": "I understand that if the size of all the points that fit the search filter are within full_scan_threshold, then an exact search is preferred by the query planner. And indexing_threshold is basically waiting till we have enough vectors to index. Please help me in understanding the case where the indexing threshold is larger than my full scan threshold? Will I not get any of the non indexed points in search results?"
  },
  {
    "threadId": "1217463329316933642",
    "name": "ERROR qdrant::startup: Panic occurred in file /qdrant/lib/collection/src/shards/replica_set/mod.rs",
    "messages": "I've deloyed my qdrant collection in azure, it was working untill last week now one of container failed, I dont know what is error here, is it related to qdrant update?  or related to azure? When I deployed qdrant, it didnt have a collection, but now is it having depadancy? how to solve it? thanks in advance"
  },
  {
    "threadId": "1217238748371353691",
    "name": "Querying with Sparse Vectors",
    "messages": "Hello guys, I'm new to sparse vectors. I'm trying to follow this tutorial for sparse vector search https://qdrant.tech/articles/sparse-vectors/\nI am following the tutorial step by step, however, it seems as if no matter what the query is, I get the same search results.. could it be something wrong with the implementation? could it be related to my documents?"
  },
  {
    "threadId": "1215836464101855232",
    "name": "Python client configuration for self-signed server",
    "messages": "How can I make the python client ignore failed SSL verification? I'm currently getting the below since I'm using self-signed cert with the qdrant service:\n\n`qdrant_client.http.exceptions.ResponseHandlingException: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate (_ssl.c:1002)`"
  },
  {
    "threadId": "1217204270160543834",
    "name": "Json formatting error",
    "messages": "Hi folks - trying out using qdrant for the first time and running into some problems upserting points to a created collection. Its supposed to be a Json formatting error but can't figure out how to get around this at all.  Here's my code for the following: \n\nHelp from the community would be super helpful!\n\n#making qdrant points from embeddings\npoints_updated = [\n    PointStruct(\n        id=idx,\n        vector=data.embedding,\n        payload={\"text\": main_file_text_list[idx],\n                 \"main_speaker\": main_file_speaker_list[idx]},\n    )\n    for idx, data in enumerate(result.data)\n]\n#creating a new collection for CDVol1\nvolume1_collection = qdrant_client.recreate_collection(\n   collection_name = \"vol1_collection\",\n   vectors_config=VectorParams(size=1536, distance=Distance.COSINE)\n)\n\nqdrant_client.upsert(\"vol1_collection\", points_updated)\n\nError: \n    raise UnexpectedResponse.for_response(response)\nqdrant_client.http.exceptions.UnexpectedResponse: Unexpected Response: 400 (Bad Request)\nRaw response content:\nb'{\"status\":{\"error\":\"Format error in JSON body: expected value at line 1 column 6594639\"},\"time\":0.0}'"
  },
  {
    "threadId": "1217143482129580043",
    "name": "onloading and offloading payload indices",
    "messages": "Hey guys, I have a collection having 1 lakh payload indices having 1000 vectors in each indices on an average. How come onloading and offloading works here, if i am interacting with only one index. How can I see the created index on disk. I set on_disk as true. And also little confused about segmentation. Can anyone please help me out. I am a ML developer using qdrant in our platform."
  },
  {
    "threadId": "1217158871936925777",
    "name": "Installation of Qdrant database",
    "messages": "Hi Team i want to install qdrant dtabase on my test server for POC.\nI am having only clinet libaries.\nKindly share the steps for same. I am new to installation of Qdrant vector databsae\n@qdrant"
  },
  {
    "threadId": "1217137168007561341",
    "name": "Payload index creation to all custom shards",
    "messages": "I am using custom shards, how can I create payload indexes that apply to all (I'm using the same payload keys across all of them)\n\nIndex creation:\n```\nlet indexes = vec![\n        (\"scraper\", FieldType::Keyword),\n        (\"ad_id\", FieldType::Keyword),\n        (\"photo_id\", FieldType::Keyword),\n        (\"sepomex_id\", FieldType::Keyword),\n    ];\n\n    for (field_name, field_type) in indexes {\n        match client\n            .create_field_index_blocking(COLLECTION_NAME, field_name, field_type, None, None)\n            .await\n        {\n            Ok(_) => {}\n            Err(e) => {\n                println!(\"Failed to create index for {}: {}\", field_name, e);\n            }\n        }\n    }\n```\n\nSetup shards config:\n```\nshard_number: Some(1),\nsharding_method: Some(ShardingMethod::Custom.into()),\n```"
  },
  {
    "threadId": "1217131087201828976",
    "name": "Is it possible to push uint8 vector to collection?",
    "messages": "Hi, I pushed SIFT1B dataset to Qdrant by python client using uint8 np.array. And suddenly noticed that after search vectors qdrant returns some float vector and computing cos dist on (uint8, float). \n\nIs it ok?"
  },
  {
    "threadId": "1227988085208252456",
    "name": "qdrant \"The read operation timed out error\" i get error when i try storing long pdf in qdrant.",
    "messages": ""
  },
  {
    "threadId": "1178539206772015184",
    "name": "Keyword boosting/weighting in filter",
    "messages": "I am filtering my vectors with a list of keywords in full-text index. Importance of all keywords are not same. Is there a way to express this in search query?\nI will assign weight to keywords/filters and search result will be scored accordingly.\n\nAny advice on implementing this is highly appreciated."
  },
  {
    "threadId": "1217103716885336114",
    "name": "Having one vector column for multiple text columns on Qdrant",
    "messages": "I  have a products table that has a lot of columns, which from these, the following ones are important for our search:\n\nTitle 1 to Title 6 (title in 6 different languages)\nBrand name (in 6 different languages)\nCategory name (in 6 different languages)\nProduct attributes like size, color, etc. (in 6 different languages)\nWe are planning on using qdrant vector search to implement fast vector queries. But the problem is that all the data important for searching, are in different columns and I do not think (correct me if I am wrong) generating vector embeddings separately for all the columns is the best solution.\n\nI came up with the idea of mixing the columns together and generating separate collections; and I came up with this solution because the title, the category, brand and attrs columns are essentially the same just in different langs.\n\nAlso I use the \"BAAI/bge-m3\" model which is a multilingual text embedding model that supports more than 100 langs.\n\nSo, in short, I created different collections for different languages, and for each collection I have a vector column containing the vector for the combined text of title, brand, color, and category in each language and when searched, because we already know which language the website is, we will search in that specific language collection.\n\nNow, the question is, is this a valid method? What are the pros and cons of this method? I know for sure that when combined, I can not give different weights to different parts of this vector. For example one combined text of title, category, color, and brand may look like this:\n\n\"Koala patterned hoodie children blue Bubito\"\n\nor Something like:\n\n\"Striped t-shirt men navy blue Zara\"\n\nNow, user may search \"blue hoodie for men\", but due to the un-weighted structure of the combined vector, it will not retrieve the best results.\n\nI may be wrong and this may be one of the best results, but please tell me more about the pros and cons of this method, and if you can, give me a better idea."
  },
  {
    "threadId": "1214926363652263936",
    "name": "Best Practice for recommendation",
    "messages": "I am currently building a product recommendation system for an e-commerce shop.\n\nI have access to a gigantic JSON with extensive information about the products which include \"marketing texts\" (description, highlights, ...) but also technical aspects (size, weight, price, id, AEN, productNO, ...).\n\nI will be using my system within an ai chatbot using OpenAI Embeddings.\n\nI want to be able to:\n- Make queries based on \"marketing texts\" (\"What's the best helmet for a mountainbike tour?\")\n- Make queries based on technical aspects (\"What's the product with the productNO xxx?\", \"I am looking for a bike made of aluminium/carbon?\"\n\nWould love to get your advice about this project. Thank you!"
  },
  {
    "threadId": "1215972814860058654",
    "name": "is it possible retrive chunks from qdrant db to give the BM25",
    "messages": "I am developing the documentation chatbot using hybrid search. I am utilizing LangChain and Vector DB to store the embeddings. Currently, I am storing the data in the S3 bucket and retrieving the documents from S3. I then segment the documents into chunks and store these chunks into the Quandrant DB.\n\nFor example:\n\n1. Initially, there are 10 documents in the S3 bucket. Once the data load is complete, it will be removed from S3. The documents are segmented into chunks and stored in the Vector DB. These chunks are then utilized by the BM25 retriever.\n\n2. When a new document is uploaded to the S3 bucket, it is loaded, segmented into chunks, and stored in the Vector DB. However, this time, the BM25 retriever receives only the chunks of the new document instead of all 10 documents.\n\nHow can I handle this? Is there an option in Quandrant to retrieve chunks directly from the Vector DB?"
  },
  {
    "threadId": "1227164361844326442",
    "name": "Type of vectors in response (with_vector)",
    "messages": "Hi\nI have a collection with some quantization configs. I wanted to know if I enable `with_vector` parameter in my search/scroll, I will get quantized version of my vectors or the real ones?"
  },
  {
    "threadId": "1214923871103361034",
    "name": "Sparse vectors after creation of the Collection",
    "messages": "Hello,\nI might migrate from OpenSearch to Qdrant. \nI'm really interested into hybrid search and see that since v1.7.0 it's possible. However I don't have time to properly implement it right now. We mainly want to have a first version working without hybrid.\n\nIs it possible to first create a collection with only embedding vectors and then in 6 month add sparse vectors to all documents to perform hybrid search ? :) \n\nAlso bonus question: do you know if it's possible to load a dump of opensearch into a Qdrant databse ? I guess with a lot of manual code only !"
  },
  {
    "threadId": "1183026730127077457",
    "name": "How to add sparse vector to existing collection",
    "messages": "Hello! Great to see that sparse vectors are supported from 1.7! I was wondering how I would go about adding sparse vectors to an additional collection (with unnamed vectors)? I mean in terms of 1) updating the collection parameters and 2) inserting the additional sparse vectors. Any advice or recommendation? Thanks!"
  },
  {
    "threadId": "1226880175522971709",
    "name": "Add Metadata while bulk upsert",
    "messages": "I have my code as below to upsert in batch of 250:\n\n`import qdrant_client.http.models as qmodels\nids=[]\nvectors=[]\npayloads=[]\nbatch=250\nself.client.upsert(\n                        collection_name=self.COLLECTION_NAME,\n                        points=qmodels.Batch(\n                            ids = ids,\n                            vectors=vectors,\n                            payloads=payloads\n                        ),\n                    )`\n\nMy payload looks as follows:\n\n`payload = {\n            \"text\": content,\n            \"url\": url,\n        }`\n\nWith the approach i am not able to add the url to the metadata.\n\nIs there a way to add metadata while bulk upsert."
  },
  {
    "threadId": "1216261245204824187",
    "name": "Stream removed error",
    "messages": "I am trying to upload 15861 points to a qdrant collection. Vector is of dimension 1586 (text-embedding-3-small).\n\n```client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY, prefer_grpc=True, timeout=60)```\n```client.upload_points(collection_name=QDRANT_COLLECTION_NAME, points=batch, wait=True)```\n\nThese are the logs:\n```\nE0307 17:33:29.464721000 8045076160 hpack_parser.cc:999]               Error parsing 'content-type' metadata: invalid value\nWARNING:root:Batch upload failed 3 times. Retrying...\n```\n\nError:\n```\n_InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n    status = StatusCode.UNKNOWN\n    details = \"Stream removed\"\n    debug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"Stream removed\", grpc_status:2, created_time:\"2024-03-07T17:33:29.466312+05:30\"}\"\n>\n```\n\nThis happens after the uploading is going on for around 10 minutes. Around 10000 points get uploaded but it fails after that.\n\nCan you please help?"
  },
  {
    "threadId": "1216093612392054904",
    "name": "order_by & Range Index",
    "messages": "Hello, \n\nI'm trying to use \"order_by\" on an integer index: \n\n```json\nPOST collections/****/points/scroll\n{\n  \"limit\": 10,\n  \"order_by\": {\n    \"key\": \"published_at\",\n    \"direction\": \"desc\"\n  }\n}\n\n```\n\nWhich is an integer index:\n\n```json\n\n\"published_at\": {\n  \"data_type\": \"integer\",\n  \"points\": 580\n}\n\n```\n\nBut I receive this error message: \n\n```json\n\n{\n  \"error\": \"Bad request: No range index for `order_by` key: published_at. Please create one to use `order_by`. Integer, float, and datetime payloads can have range indexes, see https://qdrant.tech/documentation/concepts/indexing/#payload-index.\"\n}\n\n```\n\nIn the list of existing Indexes, there's no such thing as an \"Range index\" so I'm not sure how I'm supposed to correctly use \"order_by\".\n\nWhat am I supposed to do?"
  },
  {
    "threadId": "1215764446538895401",
    "name": "OpenAI Embedding Rate Limit Error",
    "messages": "I am loading up vectors to my qdrant database using the langchain OpenAIEmbeddings.     When I try to upload and create embeddings for a 40MB text dense document (its regulations for the IRS and the Armed Services)...  with a chunk size of 1,000 - I am getting OpenAI Rate Limit Errors.       Is there anything I can do to control how quickly the embedding request are made?       I'm happy to share code I'm just not sure how to add it here.   I get my qdrant object from:   from langchain_community.vectorstores import Qdrant."
  },
  {
    "threadId": "1215763678402580500",
    "name": "Best practices for the \"match any\" filter",
    "messages": "QDrant has a \"match any\" filter to narrow down the point search space:\n\nFrom the docs (https://qdrant.tech/documentation/concepts/filtering/#match-any)\n\n> In case you want to check if the stored value is one of multiple values, you can use the Match Any condition. Match Any works as a logical OR for the given values. It can also be described as a IN operator.\n```\n{\n  \"key\": \"color\",\n  \"match\": {\n    \"any\": [\"black\", \"yellow\"]\n  }\n}\n```\n\nIs there any idea as to how big the \"match any\" field can realistically be before we start to notice significant slow-downs in performance?\ni.e\n```\n{\n  \"key\": \"fileName\",\n  \"match\": {\n    \"any\": [\"investmentMemo.pdf\", \"letter.docx\"... 1000+ more]\n  }\n}\n```\n\nThank you!"
  },
  {
    "threadId": "1214607203877519400",
    "name": "do we have best practices document for qdrant distributed setup in Azure AKS?we suspect 250Mi.",
    "messages": "Do we have best practice documents for qdrant distributed setup in Azure AKS?\nwe are expecting 250Mi records of collection.\nAnd we need sharding as well."
  },
  {
    "threadId": "1215640377504178227",
    "name": "Validation errors for PointStruct",
    "messages": "After updating the qdrant and pydantic and run code i have which extract the string from a df and then encode them and store them in qdrant collection with pointstruct i get the following error and the code i have been running it daily for around month but suddenly it's not working any more!"
  },
  {
    "threadId": "1206597766999056435",
    "name": "Async errors with Qdrant Cloud, LangChain and Chainlit - Getting \"QdrantLocal cannot interoperate..\"",
    "messages": "I am using a Chainlit and Langchain to create a RAG chatbot. When I use a local Qdrant DB as the vector store, it works fine. However when I try to switch to Qdrant Cloud I get the error:\n\n\"QdrantLocal cannot interoperate with sync and async clients\"\n\nI tried to use some of the async features as described here: https://python.langchain.com/docs/modules/data_connection/vectorstores/#asynchronous-operations\n\nBut the examples all relate to creating a vector store from documents. I just want to use my existing collection in Qdrant cloud, but can't figure out how to do that in an async way. Here's the relevant functionтАФany pointers?\n\n```python\n@cl.on_chat_start\nasync def on_chat_start():\n\n    client = QdrantClient(\n                    url=\"https://620342be-1e5e-401c-98da-42bcaddaed57.us-east4-0.gcp.cloud.qdrant.io:6333\",\n                    api_key=os.environ['qdrant_apikey'],\n                    grpc_port=6334,\n                    prefer_grpc=True\n                )\n\n    vectorstore = Qdrant(\n        client=client,\n        collection_name=collection,\n        embeddings=embeddings,\n    )\n    docs_retriever = vectorstore.as_retriever()\n    message_history = ChatMessageHistory()\n\n    memory = ConversationBufferMemory(\n        memory_key=\"chat_history\",\n        output_key=\"answer\",\n        chat_memory=message_history,\n        return_messages=True,\n    )\n\n    # Create a chain that uses the Chroma vector store\n    chain = ConversationalRetrievalChain.from_llm(\n        ChatOpenAI(model_name=\"gpt-4\", temperature=0, streaming=True),\n        chain_type=\"stuff\",\n        retriever=docs_retriever,\n        memory=memory,\n        return_source_documents=True,\n    )\n\n    cl.user_session.set(\"chain\", chain)   \n```"
  },
  {
    "threadId": "1215214427771633664",
    "name": "Haystack Qdrant Integration Merge Request",
    "messages": "Hello,\n\nI work with Haystack library.\nI've read that Qdrant maintains the Qdrant integration in the library. (See: https://qdrant.tech/documentation/concepts/storage/#configuring-memmap-storage)\nI've created a merge request about `on_disk` parameter and `payload indexing`. So I was wondering how I could notify Qdrant team ? :)\nSee: https://github.com/deepset-ai/haystack-core-integrations/pull/553\n\nHave a nice day !"
  },
  {
    "threadId": "1168371539125731418",
    "name": "Questions about multi-tenancy.",
    "messages": "We are considering to use Qdrant as our backend vector db to support our SaaS.\nAnd I got some question about the multi-tenancy. We may have many users to use it. how can we implement this multi-tenancy with Qdrant?  I see there are lines in the doc, users can use different collections. But I wonder there is not RBAC to support multi-tenancy? If one user for one collection, how many users can we support? Is there any such info? Thanks."
  },
  {
    "threadId": "1214958671742902272",
    "name": "Check if index exists",
    "messages": "I'm working on multi-tenancy. I read the docs here <https://qdrant.tech/documentation/guides/multiple-partitions/#calibrate-performance> and I had a question concerning the payload index creation (`create_payload_index` or `PUT /collections/{collection_name}/index`)\n\nDoes calling it re-create it from scratch or nothing happens if it already exists ? Because I'm integrating this function in Haystack (LLM Orchestration) and I was thinking where to put this code (index creation), if I could run it each time we create our collection object or if it should be done only once. \n\nI'm scared of big resource usage (compute/ram) like at each reboot of my app for re-creating all index. Is there a way maybe to check if an index exist already and if not, create it ?\nWould be cool if there is a way in python to check if index exist like: `if not client.index_exists(collection_name, index_name): create_payload_index(...)`. Then each time I initiate my connection to my collection I could check if the index exists or create it if not (like if we changed our app settings to index a new field !)."
  },
  {
    "threadId": "1214941501205909544",
    "name": "Error 500 when I restore snapshot",
    "messages": "I try to restore a snapshot in my qdrant but when I do this with CURL I've got a HTTPS 500 error.\n\ncURL command :\n```bash\ncurl -X POST 'https://address/collections/collection_v1/snapshots/upload?priority=snapshot' \\\n    -H 'Content-Type:multipart/form-data' \\\n    -F 'snapshot=/tmp/collection_v1.snapshot'\n```\n\nServer response :\n```json\n{\n  \"status\": {\n    \"error\": \"Service internal error: File IO error: failed to iterate over archive\"\n  },\n  \"time\": 0.000910312\n}\n```\n\nQdrant logs :\n```\n2024-03-06T13:58:31.944221Z  INFO actix_web::middleware::logger: 10.244.0.58 \"POST /collections/collection_v1/snapshots/upload?priority=snapshot HTTP/1.1\" 500 111 \"-\" \"curl/7.81.0\" 0.002149\n\n2024-03-06T13:58:43.942979Z DEBUG storage::content_manager::snapshots::recover: Downloading snapshot from file:///qdrant/snapshots/collection_v1/aae62979-3c01-45cf-a1dd-050b3c86acd1 to /qdrant/./snapshots/tmp/download-xCx68Q\n\n2024-03-06T13:58:43.943006Z DEBUG storage::content_manager::snapshots::recover: Snapshot downloaded to /qdrant/snapshots/collection_v1/aae62979-3c01-45cf-a1dd-050b3c86acd1\n\n2024-03-06T13:58:43.943072Z DEBUG storage::content_manager::snapshots::recover: Recovering collection collection_v1 from snapshot /qdrant/snapshots/collection_v1/aae62979-3c01-45cf-a1dd-050b3c86acd1\n\n2024-03-06T13:58:43.943082Z DEBUG storage::content_manager::snapshots::recover: Unpacking snapshot to /qdrant/./storage/tmp/col-collection_v1-recovery-KBKctY\n\n2024-03-06T13:58:43.943619Z  WARN qdrant::actix::helpers: error processing request: File IO error: failed to iterate over archive\n```\n\nThanks ЁЯШД"
  },
  {
    "threadId": "1214938871247413318",
    "name": "1.8 update & drop in RAM usage?",
    "messages": "Hello, \n\nI just made the 1.8 update and I noticed a massive drop in RAM usage.\n\nIs that an expected behavior?"
  },
  {
    "threadId": "1214930740949688401",
    "name": "gRPC set up for qdrant doesn't work - invalid api key",
    "messages": "We have a helm installation of qdrant (via Terrform) and we are trying to access the gRPC interface. We made the following change to our helm:\n```      \"ingress.hosts[0].paths[1].path\"        = \"/grpc\"\n      \"ingress.hosts[0].paths[1].pathType\"    = \"Prefix\"\n      \"ingress.hosts[0].paths[1].servicePort\" = \"6334\"\n```\nOn accessing /grpc it shows \"invalid apikey\". We are passing the API key and the Qdrant UI works fine when we enter it. We have also allowed 6334 port in our SGs. Any other config needed to be added?"
  },
  {
    "threadId": "1214899100701691946",
    "name": "Renaming vectors",
    "messages": "Hey Qdrant team, \n\nI tried using the recommended method for updating collections here: https://qdrant.tech/documentation/concepts/collections/#update-vector-parameters, but it seems that it only supports changing configuration of the vectors. `update_collection` does not seem to support renaming the vectors themselves. \n\nIt seems the only solution would be to make a new collection, and upload the points to the new collection with a different name.\n\nIs this true? Or am I missing something?\n\nWe are running Qdrant 1.7.3, for context.\n\nThanks for the advice,\n\nRob"
  },
  {
    "threadId": "1212956097896521748",
    "name": "ResponseHandlingException when perform async search",
    "messages": "I created a collection using following approach.\n```\nself.vectordb.recreate_collection(\n        collection_name=self.collection_name,\n        embedding_size=self.embedding_size,\n        optimizers_config=models.OptimizersConfigDiff(default_segment_number=2,memmap_threshold=20000),\n        )\n```\n\nand perform search using following method:\n```\nrequest_dict = {\n            \"collection_name\":collection_name,\n            \"query_vector\":text_embeddings,\n            \"query_filter\":models.Filter(\n                must=[\n                    models.FieldCondition(\n                        key=key, match=models.MatchValue(value=value)\n                    ),\n                ]\n            ),\n            \"search_params\":models.SearchParams(exact=True),\n            \"limit\":search_limit,\n            \"with_payload\":True,\n        }\n        if client_option == \"async\":\n            results = await self.async_client.search(\n                **request_dict\n            )\n'''\n\n'''\ntasks = [retrieve_context(embedding_model,row[search_column],collection_name,top_k, index) for index, row in data.iterrows()]\n    responses = await asyncio.gather(*tasks, return_exceptions=True)\n```\n\nBut among 8845 request that i sent, about 1500 of them are failed with ResponseHandlingExcept without any more info, why is this happening?\n```\n  File \"/home/jackson/.conda/envs/llms_env/lib/python3.10/site-packages/qdrant_client/http/api_client.py\", line 163, in send\n    response = await self.middleware(request, self.send_inner)\n  File \"/home/jackson/.conda/envs/llms_env/lib/python3.10/site-packages/qdrant_client/http/api_client.py\", line 195, in __call__\n    return await call_next(request)\n  File \"/home/jackson/.conda/envs/llms_env/lib/python3.10/site-packages/qdrant_client/http/api_client.py\", line 175, in send_inner\n    raise ResponseHandlingException(e)\nqdrant_client.http.exceptions.ResponseHandlingException\n```"
  },
  {
    "threadId": "1214855609522978816",
    "name": "Indexing stuck at 32 Million vectors",
    "messages": "Hi guys!\nI am currently experiencing an issue where the indexing process is stuck at 32,525,284 out of a total of 75,369,756 vectors, which is approximately 43.15%. I attempted to resolve this problem by restarting the pod, but unfortunately, it did not help.\n\nMy setup includes a k8s instance without any replicas, and I am using a GKE e2-highmem-8 instance with 6 cores and 54G of RAM.\n\nIn an attempt to find a solution, I came across this ticket: [qdrant/qdrant/issues/2793](https://github.com/qdrant/qdrant/issues/2793). Following the information provided in the ticket, I tried to update the `indexing_threshold` using the following command:\n\n```bash\ncurl -X \"PATCH\" --location \"localhost/collections/prod_collection\" \\\n--header \"Content-Type: application/json\" \\\n--data \"{ \\\"optimizers_config\\\": { \\\"indexing_threshold\\\": 20000 } }\"\n\n{\"result\":true,\"status\":\"ok\",\"time\":0.096209278}%\n```\n\nI have attached screenshots showing the volume utilization and CPU usage time and memory consumption for the qdrant instance after updating the `indexing_threshold`. The volume started to increase as well as the memory and CPU usage.\n\nBelow is my current configuration:\n\nI would greatly appreciate any assistance in resolving this issue. Thank you!"
  },
  {
    "threadId": "1214633323373011074",
    "name": "Stack qdrant selfhosted",
    "messages": "I created this qdrant Stack for docker swarm, but the qdrant-web-ui service keeps restarting, does anyone know the cause?"
  },
  {
    "threadId": "1212579795842498600",
    "name": "Encounter forbidden when downloading snapshot",
    "messages": "Hi,\n\nI'm currently trying to copy a collection in a cluster to another one (both clusters are on Qdrant Clout) by snapthot download/upload.\n\nHowever, when I try to download snapshot from cluster via UI (like attached screenshot), it reports below.\n```json\n{\"error\":\"forbidden\"}\n```\nI have already put my API Key when I enter the UI for this cluster.\nWhat I'm doing wrong...?"
  },
  {
    "threadId": "1211997826876178464",
    "name": "LlamaIndex",
    "messages": "I have LlamaIndex some code running on a server that's designed to index documents into an existing Qdrant collection. Here's the code:\n\n```\nclient = qdrant_client.QdrantClient(host=QDRANT_HOST,\n                                    grpc_port=QDRANT_GRPC_PORT,\n                                    prefer_grpc=True,\n                                    api_key=QDRANT_API_KEY)\nvector_store = QdrantVectorStore(client=client,\n                                  collection_name=collection_name,\n                                  batch_size=20)\nindex = VectorStoreIndex.from_vector_store(vector_store=vector_store,\n                                           service_context=service_context)\nfor document in documents:\n    try:\n        log.info(f\"SETUP: Source {source_id} Updating document\")\n        index.update_ref_doc(\n            document,\n            update_kwargs={\n                \"delete_kwargs\": {\n                    \"delete_from_docstore\": True\n                }\n            },\n        )\n    except Exception as err:\n        log.info(f\"SETUP: Source {source_id} Error: {err}\")\n        log.info(f\"SETUP: Source {source_id} Update failed, trying insert\")\n        index.insert(document)\n```\n\nThis code performs well when processing documents one at a time. However, it encounters issues under multiple concurrent requests. Some requests fail with the error: `Wrong input: Collection <collection name> already exists!`. Consequently, both `index.update_ref_doc` in the `try` block and `index.insert(document)` in the exception handler block fail.\n\nCan anyone offer some advice on this? Is Qdrant not capable of handling concurrent insertions?"
  },
  {
    "threadId": "1213593748898971708",
    "name": "Build failed with docker",
    "messages": "Hiii, I am using docker in mac m1 8GB to build qdrant using command \n``` docker build . --tag=qdrant/qdrant\n```\nbut each time it fails with the error:"
  },
  {
    "threadId": "1214181428770770971",
    "name": "Semantic search with key = metadata and value = list of files",
    "messages": "I want to search with filter. Now the thing is i have metadata and it contains list of filenames. so let's say i want to search from 2 documents so my filter condition should be key = \"filename\" value = [\"file1\", \"file2\"]\n\nCan we do something like this? Appreciate you guys inputs!"
  },
  {
    "threadId": "1214192230676570112",
    "name": "Exact search on a field with full text index",
    "messages": "We have 2 separate use-cases when searching through our database. One requires a filter with exact match on a payload field, the other requires a full text search on the same field.\n\nIf we enable full text search index, we are afraid it will include more results than intended for the exact match scenario.\n\nIs there a way to enforce an exact match when a full-text index is present?"
  },
  {
    "threadId": "1214152402643591179",
    "name": "not able to get more than first instance of the string in metadata",
    "messages": "I was trying out the code using \"texts = get_chunks(raw_text)\n\nvectorstore.add_texts(texts, metadatas=\"abcd\")\" but i would always get metadata as \"a\" here that is the first letter. How to resolve this?"
  },
  {
    "threadId": "1214101844977254410",
    "name": "Payload Index to filter",
    "messages": "Hi I have a question, when using the API to create a payload index, it will create an independent payload index, meanwhile modify the hnsw graph to support filtering, is it right?"
  },
  {
    "threadId": "1214124603450851328",
    "name": "Read timeout - Scroll - Python Client",
    "messages": "Hello, \n\nI'm facing a \"Read Timeout error\" using the python client and the .scroll endpoint.\n\nThe other endpoints seem to work as expected (.search, .searcg_batch)\n\nAny idea of what could be wrong?\n\nNote: the problem seems to be related to my collection as when I try with a different and smaller one, the problem doesn't occur."
  },
  {
    "threadId": "1213861506161180722",
    "name": "Updating collection replication factor.",
    "messages": "Hi! I have a collection distributed across several shards. Now the development has come to a point where we need to maintain multiple replicas of those shards. I have two and a half questions regarding that:\n1. Is it possible to change replication factor of the collection using update collection parameters API method? (In the main docs it is stated, that replication factor could only be specified during collection creation, while API docs contain replication factor in PATCH collections/{collection_name} method examples) \n2. If not, is there a built-in way to copy one collections content to another (differently named for example) collection, or it should be done with one's own code?\n2.5. If replication factor can only be specified during collection creation, is there by any chance a close release that changes that? (meaning, should we invest resources into recreating collections, or should we wait for a next release?:))\nThanks for your help in advance!"
  },
  {
    "threadId": "1213461591899766795",
    "name": "First Time Query is too slow compared to consecutive queries using the python qdrant_client package",
    "messages": "Self-Hosted Qdrant Specs \n-----------------------------\nInstance - t3.xlarge\nRam = 16 gb\ncpu = 4 vcpu \nTotal collections = 10 \nTotal Vectors = 1 million\n-------------------------------\nCollection Config on which it is tested\n-------------------------------\n```\n{\n    \"result\": {\n        \"status\": \"green\",\n        \"optimizer_status\": \"ok\",\n        \"vectors_count\": 682063,\n        \"indexed_vectors_count\": 681563,\n        \"points_count\": 682063,\n        \"segments_count\": 5,\n        \"config\": {\n            \"params\": {\n                \"vectors\": {\n                    \"size\": 384,\n                    \"distance\": \"Cosine\"\n                },\n                \"shard_number\": 1,\n                \"replication_factor\": 1,\n                \"write_consistency_factor\": 1,\n                \"on_disk_payload\": true\n            },\n            \"hnsw_config\": {\n                \"m\": 16,\n                \"ef_construct\": 100,\n                \"full_scan_threshold\": 10000,\n                \"max_indexing_threads\": 0,\n                \"on_disk\": false\n            },\n            \"optimizer_config\": {\n                \"deleted_threshold\": 0.2,\n                \"vacuum_min_vector_number\": 1000,\n                \"default_segment_number\": 0,\n                \"max_segment_size\": null,\n                \"memmap_threshold\": 1000000,\n                \"indexing_threshold\": 20000,\n                \"flush_interval_sec\": 5,\n                \"max_optimization_threads\": 1\n            },\n            \"wal_config\": {\n                \"wal_capacity_mb\": 32,\n                \"wal_segments_ahead\": 0\n            },\n            \"quantization_config\": null\n        },\n        \"payload_schema\": {}\n        }\n    },\n    \"status\": \"ok\",\n    \"time\": 0.000033929\n}\n```\n-------------------------------"
  },
  {
    "threadId": "1213231758141689966",
    "name": "Always the same results?",
    "messages": "I have loaded ten vectors represented by different images. When I upload any one of the images and query the collection I get the same results. I would expect it to return the same uploaded image/vector (it does not find it) and different images and scores.  Is it just not enough data? or something else?\n\nresults = [ScoredPoint(id='a4dcdcec-df47-490b-8bdd-1b5fe0a654b9', version=4, score=0.9999998, payload={'image_token': 'efdeec2d09c1', 'img': '47f1f19bdbe4_frame288.jpg'}, vector=None, shard_key=None), ScoredPoint(id='6cfbbc76-4d84-479e-9e70-ca3add012109', version=6, score=0.9999998, payload={'image_token': '873391703e67', 'img': '47f1f19bdbe4_frame144.jpg'}, vector=None, shard_key=None), ScoredPoint(id='090f5c90-ab89-4ee5-b51f-3a94a5f356e3', version=7, score=0.9999998, payload={'image_token': '836e4bb7d7a9', 'img': '47f1f19bdbe4_frame96.jpg'}, vector=None, shard_key=None)]"
  },
  {
    "threadId": "1162322206181822514",
    "name": "Proper workflow for javascript > python searching for images?",
    "messages": "I have successfully indexed several million objects in qdrant using the python client. Each object has filterable attributes, TextIndexPrams and two sets of embeddings, one for text and one for an image using a dino-vits16 transformer.  The issue i'm having is integrating the qdrant searching into a webui, specifically svelte which is pure javascript/typescript.\n\nMy workflow is as follows:\nPerson uploads an image > server creates embeddings > returns results closest to the uploaded image\nPerson enters in text > server creates embeddings for text > returns results closest to the vectorized text field which is stored in qdrant\n\nI THINK I need a JS example of how to pass search params/image to a python client(API), python then creates an embedding and querys qdrant, then it returns the results to the client. Is this the correct way or is there a better way?"
  },
  {
    "threadId": "1213074649097838632",
    "name": "Can't initialize consensus: Failed to initialize Consensus for new Raft state: Failed to create time",
    "messages": "Hi, I am trying to raise the Qdrant cluster. And slaves nodes are crushing.\n\nqdrant v1.7.4\n\nStart first node with ipv6 URI\n```\n./qdrant --uri $QDRANT_LEADER_URI --config-path $QDRANT_CONFIG\n```\nresult: https://pastebin.com/PeHcWfGx\n\nThen I start second node\n```\n./qdrant --bootstrap $QDRANT_LEADER_URI --config-path $QDRANT_CONFIG\n```\nAnd I get \n```\nERROR qdrant::startup: Panic backtrace: \n.\n.\n.\nPanic occurred in file src/main.rs at line 307: Can't initialize consensus: Failed to initialize Consensus for new Raft state: Failed to create timeout channel: transport error   \n```\nFull log: https://pastebin.com/xiRmGdLW\n\nConfigs\nfirst node: https://pastebin.com/rj63ryEd\nsecond node: https://pastebin.com/GPbPqeCt\n\nTelnet Access:\nsecond -> first 6335 OK\nsecond -> first 6334 OK\n\nHow can I fix this problem? Thank you!"
  },
  {
    "threadId": "1212835145518686208",
    "name": "Memmap_threshold for quantization config",
    "messages": "I am using scalar quantization as below and I believe that means original vectors are on disk (read by memmap)  ```   \"optimizers_config\": {\n        \"memmap_threshold\": 20000\n    },\n    \"quantization_config\": {\n        \"scalar\": {\n            \"type\": \"int8\",\n            \"always_ram\": true\n        }\n    }```  I see a big perf issue every time when a pod restarted. The disk IOs is maximized and latency is terrible for 30 minutes. I think this is because it takes that long for file system buffer to warm up with the 300 GB vector data.  So I am thinking put all original vectors in memory too. The questions are: 1. How can I configure that (remove the memmap threhold.  2. What does it mean for the live system? Will it be effective immediately or need to wait for next restart?"
  },
  {
    "threadId": "1212818709278756875",
    "name": "Dense vector not found",
    "messages": "Hey all, I am making what I think is a simple query using the Python client and supplying a list of floats as the query vector and am getting the following error:\n\nValueError: Dense vector  is not found in the collection\n\nfrom\n\n# it must be dense vector\nif name not in self.vectors:\n                raise ValueError(f\"Dense vector {name} is not found in the collection\")\n            vectors = self.vectors[name]\n            distance = self.get_vector_params(name).distance\n\nCan I not supply a list of floats? If I set the score threshold low enough shouldn't I get something/anything back?"
  },
  {
    "threadId": "1212704417317527553",
    "name": "Hello everyone, anyone knows this error at start time?",
    "messages": "ERROR qdrant::startup: Panic occurred in file /qdrant/lib/collection/src/shards/replica_set/mod.rs at line 246: Failed to load local shard \"./storage/collections/collection_name/0\": Service internal error: RocksDB open error: IO error: While open a file for random read: ./storage/collections/collection_name/0/segments/1ad50fec-e4fb-44e9-bd7b-994f4b615558/payload_index/001738.sst: Input/output error"
  },
  {
    "threadId": "1212400017319665664",
    "name": "transport error on `collection_info` Rust client after idling for a period of time",
    "messages": "I'm running into an issue with the Rust client (v1.7.0), and my Qdrant cloud cluster (v1.7.4). Generally, everything works very well, but after a few minutes of not querying Qdrant, when we try to call `.collection_info`, we get this error:\n\n```status: Unknown, message: \\\"transport error\\\", details: [], metadata: MetadataMap { headers: {} }\\n\\nCaused by:\\n    0: transport error\\n    1: connection error: connection reset\\n    2: connection reset```\n\nIf I call the service a second time after this failure, the operation works. This is how my client is initialized.\n```\nQdrantClientConfig::from_url(...)\n            .with_api_key(...)\n            .with_timeout(Duration::from_secs(20))\n            .build()\n```\n\nI suspect this might have to do with the `keep_alive_while_idle` setting, which is true by default."
  },
  {
    "threadId": "1211194915921465364",
    "name": "Client Match (number of returning result)",
    "messages": "Hi there, \nI want to ask about somthing in qdrant client search \nI have a collection with 31K points in it and when I do searching I expects to get the matching result of the 31K points as (id,payload and scores) but what happens is that I get only 1700 (this number changes when i change the input text i am using to match) result of the matching points and do not know where is the rest.\nHere is some paramters I am using in search:\nтАвlimit = 31000 (the number of the points in the collection)\nтАвoffset=0\nтАвscore_threshold= 0 (and I also tried to set it as nigative)\nтАв and using no filters \n\nnotes:\nтАвI am using client on the collection saved on the local \nтАвThe collection has 3 vectors with size 1024\nтАвUsing cosin similarity \nтАвI am using LLM for embedding the text before saving to collection and for embedding the text used in the match"
  },
  {
    "threadId": "1222813667016310784",
    "name": "Nested vector embeddings within named vector.",
    "messages": "Hi team I am trying to build RAG with Qdrant wherein I have multiple named vectors stored for a point. However for each point there are multiple lines of english sentences describing the point which I want to make searchable if any user asks a question and if it matches with the point description. I was planning to break the sentences into individual sentence and store the individual embeddings of the sentence rather than storing the embedding of the whole paragraph.\nIs it possible to do this? Or is there any better way to approach the problem?"
  },
  {
    "threadId": "1212096007785939014",
    "name": "dspy.Ollama(model='mistral')?",
    "messages": "Hello, I was trying the beginner tutorial from dspy Github. Can we use open-source models like Mistral 7b through Ollama instead of GPT 3.5?"
  },
  {
    "threadId": "1212011335035125771",
    "name": "How improve my model semantic search on medical felid",
    "messages": "I want to build a medical transformer but i have tried multiple sentence-encoder but they are so bad, \ni test normal llms but they are not strong also. \nthe problem i want to solve is that there is some medical codes like \narco  => a long medical description 1 \narxo  => a long medical description 2\naxvo  => a long medical description 3\nwhat are things i can try to improve the model mapping for my own domain!\nthanks in advance for any help or advice."
  },
  {
    "threadId": "1212008017688662037",
    "name": "crash",
    "messages": "Hi my qdrant has been operating fabulously but its now crashing -  maybe something to do with qty / size there is no traffic to it. \n\nThere are 2m vectors in one collection with some extra payload and 384 dim vector\n\nMaybe i understand the product wrong - does it have to store everything in memory? I was hoping to scale this out to 200m vectors as only the most recent will really be searched regulalry \n\nAny pointers on what to do ? Where to look - have checked the website .\n\nThe docker container stops with an unhelpful message"
  },
  {
    "threadId": "1211752349660618864",
    "name": "Close local client",
    "messages": "Hey all, I instantiated a local Python client using the path variable in a notebook and it seems it is still being accessed, I would like to close it but when I try to instantiate a client with that same path again I get the following error, what is the most straightforward way to close the client?\n\nError:\nStorage folder /home/ubuntu/... is already accessed by another instance of Qdrant client. If you require concurrent access, use Qdrant server instead."
  },
  {
    "threadId": "1211708035374907503",
    "name": "Change of Payload value type",
    "messages": "Can someone know if we can change easly the type of a payload values for a lot a values ?"
  },
  {
    "threadId": "1221818766397145128",
    "name": "Recover Collection from Snapshot URL",
    "messages": "Dear Qdrant Experts,\n\nI'm trying to duplicate a collection by creating a snapshot and recovering that snapshot into a new collection. I have tried recovering into an existing collection as well as providing a new collection name. I'm using the python client (1.8.0) and Qdrant Cloud. Specifically, I'm calling the `recover_snapshot` method like this:\n\n```\nclient.recover_snapshot(\ncollection_name='target_collection_name',              location=qdrant_url+\"/collections/\"+source_collection_name+\"/snapshots/\"+snapshot_name\n                       )\n```\nthis returns\n\n```\nUnexpectedResponse: Unexpected Response: 500 (Internal Server Error)\nRaw response content:\nb'{\"status\":{\"error\":\"Service internal error: Http request error: error sending request for url (\n```\n\nServer Log\n```\n2024-03-25T13:36:53.877126Z INFO actix_web::middleware::logger: 10.172.1.2 \"PUT /collections/target_collection_name/snapshots/recover?wait=true HTTP/1.1\" 500 273 \"-\" \"python-httpx/0.27.0\" 129.388682\n2024-03-25T13:36:53.876662Z WARN qdrant::actix::helpers: error processing request: Http request error: error sending request for url (qdrant_url/collections/source_collection/snapshots/snapshot_name): error trying to connect: tcp connect error: Connection timed out (os error 110)\n```\n\nmight this be an authentication issue?"
  },
  {
    "threadId": "1219584709802328074",
    "name": "spark",
    "messages": "Hi All.\n\nI'm trying to write a spark-frame into qdrant. Below is the error I'm facing:\n\nAlso adding code the snippet I'm using:\n\nBe great if someone could help with the same."
  },
  {
    "threadId": "1211570304774578177",
    "name": "Questions on security and multitenancy",
    "messages": "I read through the qdrant docs on security and multitenancy:\nhttps://qdrant.tech/documentation/guides/multiple-partitions/\nhttps://qdrant.tech/documentation/guides/security/\n\nLet me know if my understanding is correct-\n1. Api-key config or enabling TLS can only be done at cluster level and cannot be done at collection level\n2. For collection isolation using partition by payload approach, we need to make sure that we are not exposing raw query endpoints to users."
  },
  {
    "threadId": "1201638617311883454",
    "name": "qdrant-js rest client on AWS lambda - error dynamic require of assert is not supported",
    "messages": "Hi, I am trying to use qdrant-js rest client on AWS Lambda, however I get the following error when importing the package:\n\n2024-01-28T11:04:26.920Z undefined ERROR Uncaught Exception {\"errorType\":\"Error\",\"errorMessage\":\"Dynamic require of \\\"assert\\\" is not supported\",\"stack\":[\"Error: Dynamic require of \\\"assert\\\" is not supported\",\" at file:///var/task/index.mjs:13:9\",\" at node_modules/undici/lib/client.js (file:///var/task/index.mjs:6348:18)\",\" at __require2 (file:///var/task/index.mjs:16:50)\",\" at node_modules/undici/index.js (file:///var/task/index.mjs:16337:18)\",\" at __require2 (file:///var/task/index.mjs:16:50)\",\" at file:///var/task/index.mjs:16676:29\"]}\n\nLooks like it might be an issue with undici dependency. Any idea how to get this to work?"
  },
  {
    "threadId": "1202117642345775135",
    "name": "Adding a key to json payload field",
    "messages": "is it possible to add a new key to an existing payload with json field?\n\nfor example payload is\n```json\n{\"name\": \"abcd\"\n\"data\" : {\"place\" :\"A\" , \"time\": 123}\n}\n```\nto \n```json\n{\"name\": \"abcd\"\n\"data\" : {\"place\" :\"A\" , \"time\": 123, \"value\" : \"red\"}\n}\n```\n\nSo I have a payload like this\n```json\n{\"payload\" : {\"property1\" : 123, \"property2\": {\"a\" : \"a\", \"b\" : \"b\"}}} \n```\nDo I send a request like this to update?\n`POST /collections/{collection_name}/points/payload`\n```json\n{\n    \"payload\": {\n        \"property2.c\":\"c\" ,\n    },\n    \"points\":[\"123\", \"123\"]\n}\n```\nWhen I tried this, it created a new field with \"property2.c\" : \"c\""
  },
  {
    "threadId": "1211295801129570384",
    "name": "issue with collection updates",
    "messages": "Hi, I would like to know how to solve an issue regarding the update of a collection. Forgive me if my language won't be so technical, I'm trying to learn and improve ЁЯЩВ\n\nThis is the specific explanation of what happens:\n\nBefore running the code, the vectors and points count of my collection is 17. I add two chunks to the knowledge base, I run the code and the console output prints this:\n\nCollection Info: status=<CollectionStatus.GREEN: 'green'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=19 indexed_vectors_count=0 points_count=19 segments_count=2 \n\nSo everything looks good, the vectors and points counts is 19 because of the 2 chunks added. \nThen I started to test and after the first message I refreshed the \"Collection Info\" webpage and checked the number of vectors and points count and the number was 36 (looks like the 19 of the beginning plus the 17 of the previous version). Anyway all the questions had the right answer, also the question related to the two new chunks added to knowledge base.\n\nAfter other few messages, I asked again a question related to new chunks and the bot didn't respond correctly, as in the same question of 3 previous messages. I refreshed again the \"Collection Info\" webpage and checked the number of vectors and points count and the number was 17, so the same number of vectors of the collection before running the code. I continued to test, but both counts were 17 for the rest of the session.\n\nthis is just an example, in other sessions the \"behaviour\" of the collection was different, but the end was the same\nThere is something wrong regarding the creation of the collection and its updates. The issue could be related to a not update version of dependencies, because the code stopped to work 10 days ago after Langchain-related dependencies' updates and I installed the previous versions of the dependencies to have the bot active and working properly. In the next message I'm going to provide the code and the list of dependencies"
  },
  {
    "threadId": "1210621963471552552",
    "name": "Is there any way to see the last collection point?",
    "messages": "Can we do indexing and see last collection point or id of last point?\n\nAlso I want to search if the perticluar ID is already in collection or not how do we check it without query vector?\n\n        #if condition applied  - change\n        filter_condition = models.Filter(\n        must=[\n                models.FieldCondition(\n                    key=id\n                )\n            ]\n        )\n        result_description = client.search(\n            collection_name=collection_name,\n            query_filter=filter_condition\n        )\nsomething like this?\n\nThanks in advance ЁЯЩВ"
  },
  {
    "threadId": "1210588662308409445",
    "name": "qdrant cloud service timeout under 10/s concurrency",
    "messages": "Hello,\n\nI am using Qdrant cloud service, and my collection contains approximately 200,000 rows of data. I have written a Java server-side service to access the collection in Qdrant cloud, with about 10 requests per second.\n\nI've noticed that after the service went live, under the 10/s concurrency, the response time from the Qdrant client API gradually increases until it reaches a 60-second timeout (about 5 minutes to reach the timeout state). After the concurrent requests stop, the time for a single request gradually recovers to within 1 second.\n\nHowever, I understand that Qdrant should be very capable of handling concurrent requests. I am wondering if there is something wrong with my usage.\n\nmy code is attached. Any help is appreciated!"
  },
  {
    "threadId": "1208901064322195526",
    "name": "Optimising for high limit",
    "messages": "Hi all,\n\nWe're currently using Qdrant Cloud for search. Since we have quite a lot of filters (and found Qdrant quite slow with filtering in our tests), we decided to request all records & scores and filter on our backend instead.\n\nWe have 50,000 records in our database, using ~985MB RAM (capacity on our plan is 2GB), ~0.25vCores (very rarely hits the max of 0.5vCores), 1.2GB Disk (capacity is 8GB). \n\nThis means we use a limit of 50,000 to search. We only request scores and record ids (not payload). We're getting a latency of anywhere between 0.3s to 3s when requesting a limit like this. \n\nWe're looking into reworking our search logic so as not not need to request so many records, but just want to check - is a latency like this expected? If so, are there any optimisations we can do, Qdrant side?\n\nCluster ID: 0171fb30-24e4-4154-a641-578353f416aa\n\nConfig is as follows:\n{\n  \"params\": {\n    \"vectors\": {\n      \"size\": 768,\n      \"distance\": \"Cosine\"\n    },\n    \"shard_number\": 1,\n    \"replication_factor\": 1,\n    \"write_consistency_factor\": 1,\n    \"on_disk_payload\": true\n  },\n  \"hnsw_config\": {\n    \"m\": 16,\n    \"ef_construct\": 100,\n    \"full_scan_threshold\": 10000,\n    \"max_indexing_threads\": 0,\n    \"on_disk\": false\n  },\n  \"optimizer_config\": {\n    \"deleted_threshold\": 0.2,\n    \"vacuum_min_vector_number\": 1000,\n    \"default_segment_number\": 0,\n    \"max_segment_size\": null,\n    \"memmap_threshold\": null,\n    \"indexing_threshold\": 20000,\n    \"flush_interval_sec\": 5,\n    \"max_optimization_threads\": 1\n  },\n  \"wal_config\": {\n    \"wal_capacity_mb\": 32,\n    \"wal_segments_ahead\": 0\n  },\n  \"quantization_config\": null\n}"
  },
  {
    "threadId": "1210532626654429195",
    "name": "Feature request: in Qdrant cloud console, show breakdown of RAM and disk usage per index",
    "messages": "as title"
  },
  {
    "threadId": "1210442818326233088",
    "name": "Help needed to build Qdrant from source",
    "messages": "Greetings! I have a special use case that requires me build Qdrant from source insude a docker container but run into issues.\nI have posted the issue on GitHub (https://github.com/qdrant/qdrant/issues/3675) but wonder if someone could also help me here. Many thanks!"
  },
  {
    "threadId": "1208056482600325130",
    "name": "Custom Sharding Question",
    "messages": "Reading this article with interest - https://discord.com/channels/907569970500743200/1072823321521041478/1206904664528785408 we will soon need to implement functionality where a collection will need to be split per client by client_id. With custom sharding, is there only one shard per specified custom shard? Or can there be multiple? i.e. If there's a custom shard for client=123 and this customer has 10x the data of the others, will this be split into sub-shards? This is an issue we have with our data where there are lots of smaller clients and some clients up to 100 times bigger than these."
  },
  {
    "threadId": "1209442379099148328",
    "name": "Text search over all the key, value pairs in the collection.",
    "messages": "Hi is there a possibility or API which allows to search for a value over all the keys in the payload? Say suppose I want to search for name, but I dont know under which key it would be mapped to, but I want all the results or points if the given name is there in any of the payload values."
  },
  {
    "threadId": "1210133070263488593",
    "name": "getting upsert errors",
    "messages": "#upsert\n\nHi - I am running the following code but something is wrong and I get an error. See below code and error\n\nI have checked the points vector. ItтАЩs created correctly and has an id and vector in it. Empty payload. \n\nI am not a professional coder, so any help will be much appreciated ЁЯЩП\n\nThanks \n\nтАФтАФ \n\nCode: \n    # Create point_data using PointStruct\n    point_data = PointStruct(\n        id=filename,  \n        vector=summary_vector.tolist(),\n        payload={\n            # ... other metadata\n        } \n    )\n\n    \n    # Upsert the data into the QDRANT collection\n    client.upsert(\n        collection_name=\"transcripts\",\n        points=[point_data] \n    )\n\nError: \nUnexpectedResponse                        Traceback (most recent call last)\n<ipython-input-151-d8fbbc9d0c5f> in <cell line: 26>()\n     42 \n     43     # Upsert the data into the QDRANT collection\n---> 44     client.upsert(\n     45         collection_name=\"transcripts\",\n     46         points=[point_data]\n\n5 frames\n/usr/local/lib/python3.10/dist-packages/qdrant_client/http/api_client.py in send(self, request, type_)\n     95             except ValidationError as e:\n     96                 raise ResponseHandlingException(e)\n---> 97         raise UnexpectedResponse.for_response(response)\n     98 \n     99     def send_inner(self, request: Request) -> Response:\n\nUnexpectedResponse: Unexpected Response: 400 (Bad Request)\nRaw response content:\nb'{\"status\":{\"error\":\"Format error in JSON body: data did not match any variant of untagged enum PointInsertOperations\"},\"time\":0.0}'"
  },
  {
    "threadId": "1209841091709509643",
    "name": "spark connector",
    "messages": "Does anyone notice the ID generation bug when using Spark connector? \nI'm trying to load vectors with IDs to Qdrant collection, the problem is that the ID field filled with the generated ID and not with the ID I provided"
  },
  {
    "threadId": "1209686696044789801",
    "name": "Delete N by metadata value",
    "messages": "Hi, I wrote a helper function to delete N entries with a given metadata value. Here:\n\ndef delete_database_entry(db_path, metadata_name: str, metadata_value):\n    extra_args = _compute_all_extra_args({})\n    client = qdrant_client.QdrantClient(**extra_args)\n    result = client.delete(db_path,\n                  points_selector=rest.Filter(must=[rest.FieldCondition(key=metadata_name, match=rest.MatchValue(value=metadata_value))]),\n                  wait=True,\n                           )\n\nIt returns success, but no deletion happens. It does not delete the entries. Have I misunderstood the usage of filters here?? \n\nAny help is welcome. Thanks!"
  },
  {
    "threadId": "1209834052027420672",
    "name": "Deleting point is not removing vectors.",
    "messages": "Hi. Whenever I'm deleting a point, I'm expecting it to remove respective vector as well but currently it shows points count as 0 and vector count as 284. How can I delete a point with it's respective vector?\n\n->ids = [data.id for data in result[0] if data is not None]\nlogger.info(ids)\nclient = qdrant_client.QdrantClient(url=self.host, port=self.port)\nself.client.delete(collection_name = self.collection_name, points_selector = ids)"
  },
  {
    "threadId": "1209766851886710785",
    "name": "Search within Nested key value",
    "messages": "Hi \nclient.scroll(\n    collection_name=\"{collection_name}\",\n    scroll_filter=models.Filter(\n        must=[\n            models.FieldCondition(\n                key=\"city\",\n                match=models.MatchValue(value=\"London\"),\n            ),\n\ni want to search for all the points within key \"metadata\" but in that key i have a subkey called \"file\" and then value : \"abc.txt\"\ndoes qdrant allows nested searching?"
  },
  {
    "threadId": "1209567626683093062",
    "name": "Python v Rust client",
    "messages": "Do you have any tests on how much better is the Rust client performance over Python's? These results can help people decide whether it is worth spending time migrating to Rust or not."
  },
  {
    "threadId": "1209544567003680829",
    "name": "MatchAny not working properly on python client.",
    "messages": "I'm trying to retrieve some points based on their id using the MatchAny function but seems toe always return empty list. Here is a small example of a code that i'm positive it should work:\n\n``\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.http.models import (\n    FieldCondition,\n    Filter,\n    MatchAny,\n)\ncollection_name = \"VectorStore\"\n\nclient = QdrantClient(host=\"localhost\", port=6333)\nresults = client.scroll(collection_name=collection_name, scroll_filter=Filter(should=None, must=None, must_not=None))\nids = [result.id for result in results[0]]\nclient.scroll(collection_name=collection_name, scroll_filter=Filter(should=None, must=[FieldCondition(key='id', match=MatchAny(any=ids))], must_not=None))\n``"
  },
  {
    "threadId": "1209507491746942976",
    "name": "Qdrant client error : json_with_bigint_1 = require(\"json-with-bigint\");",
    "messages": "Hi , i'm getting an unexpected error today , I didn't change anything in code , client ver 1.7 , how to fix it ?\n\n /home/node/node_modules/@qdrant/openapi-typescript-fetch/dist/cjs/fetcher.js:5  \nтФВ const json_with_bigint_1 = require(\"json-with-bigint\");  \nтФВ Error [ERR_REQUIRE_ESM]: require() of ES Module /home/node/node_modules/json-with-bigint/json-with-bigint.js from /home/node/node_modules/@qdrant/openapi-typescript-fetch/dist/cjs/fetcher.js not supported.      \nтФВ Instead change the require of json-with-bigint.js in /home/node/node_modules/@qdrant/openapi-typescript-fetch/dist/cjs/fetcher.js to a dynamic import() which is available in all CommonJS modules.    \nтФВ     at Object.<anonymous> (/home/node/node_modules/@qdrant/openapi-typescript-fetch/dist/cjs/fetcher.js:5:28) {                                                                                                                                                    \nтФВ Node.js v20.11.1 ```"
  },
  {
    "threadId": "1209325111866302504",
    "name": "Paginated with filters",
    "messages": "Is there a way to do pagination by filtering by payload fields? Like the search offset. I was about to use the scroll method, but it does not receive integers (number of documents to ignore) in the offset"
  },
  {
    "threadId": "1209188435252740096",
    "name": "Qdrant with Azure Storage collection",
    "messages": "I've a qdrant collection, I need to use qdrant collection and qdrant with my web service, \n\nfor qdrant collection do I need to dockerize it first then I need to add in storage account?(Blob), \nor I can directly add qdrant collection to Azure Storage account? \nor I can use it in Azure containerapp service (Note I've to store data as well)\n\nWill Qdrant collection be accessed within the webservice from Azure storage account? collection can be updated with real time data? \n\nlink: https://learn.microsoft.com/en-us/cli/azure/containerapp/service/qdrant?view=azure-cli-latest  (this suggest only container app service)\n\nlink:https://github.com/Azure-Samples/qdrant-azure (this suggests kubernatives service only)\n\nI cant figure out how to integrate qdrant and qdrant collection with Azure."
  },
  {
    "threadId": "1208727693114220576",
    "name": "Retrieve information regarding points",
    "messages": "Hi guys, I'm using qdrant to build a document retrieval system. So i need to create a api which will return me list of all the points. I did the following \nself.client.get_collection(collection_name = self.collection_name)\nthis returns ->\n('status', <CollectionStatus.GREEN: 'green'>)\n('optimizer_status', <OptimizersStatusOneOf.OK: 'ok'>)\n('vectors_count', 136)\n('indexed_vectors_count', 0)\n('points_count', 136)\n('segments_count', 1)\n\nI want to get detailed information of the points in that collection like document, metadata, ids etc. How can i do that? Thank you."
  },
  {
    "threadId": "1209262133288767518",
    "name": "`Recovering collection` takes almost two hours on every start",
    "messages": "Hi, I'm not sure what's going on. My collection is 916 MB on disk. After it runs (after two hours), this is the status:\n```json\n{\"result\":{\"status\":\"green\",\"optimizer_status\":\"ok\",\"vectors_count\":456861,\"indexed_vectors_count\":456861,\"points_count\":456861,\"segments_count\":2,\"config\":{\"params\":{\"vectors\":{\"size\":384,\"distance\":\"Cosine\",\"hnsw_config\":{\"m\":16},\"on_disk\":true},\"shard_number\":1,\"replication_factor\":1,\"write_consistency_factor\":1,\"on_disk_payload\":true},\"hnsw_config\":{\"m\":16,\"ef_construct\":100,\"full_scan_threshold\":10000,\"max_indexing_threads\":0,\"on_disk\":false},\"optimizer_config\":{\"deleted_threshold\":0.2,\"vacuum_min_vector_number\":1000,\"default_segment_number\":0,\"max_segment_size\":null,\"memmap_threshold\":null,\"indexing_threshold\":20000,\"flush_interval_sec\":5,\"max_optimization_threads\":1},\"wal_config\":{\"wal_capacity_mb\":32,\"wal_segments_ahead\":0},\"quantization_config\":null},\"payload_schema\":{}},\"status\":\"ok\",\"time\":0.000043093}\n```\n(I can't seem to remove the specific `hnsw_config` for the `vectors` - any ideas how to do that?)\nI tried with `on_disk: true` for `vectors` and without - same thing. Why is this taking 2 hours? What can I do to debug/fix this? Qdrant is the latest version run in Docker on Ubuntu 23.10."
  },
  {
    "threadId": "1219768260376006748",
    "name": "Cluster fails to scale down after it was manually scaled up",
    "messages": "I manually scaled up the qdrant cluster to use 3 nodes because I had to seed some data, but after I finished that task, every attempt of mine to scale it down it fails. Specifically, I get the error `Sorry, something went wrong.ЁЯШв We are working on it. Try to reload a page`, but no matter how many times I try, it always fails. Any ideas?"
  },
  {
    "threadId": "1209016979520757762",
    "name": "Multiple VDB or Metadata Filtering",
    "messages": "Hello, I am using Qdrant in my LLM application and I am looking for advice.\n\nI am using Qdrant as RAG and I have to differentiate between different categories of RAG (e.g.: Products/Pages/Articles).\n\nI have to ideas in mind on how to do this:\n\n1. Build Multiple collections and just query the one I need\n\n2. Use one central VDB and filter based on metadata (products, pages, articles)(https://qdrant.tech/documentation/concepts/filtering/#must) \n\nWhat are your thoughts on this, am I missing out on other solutions?\n\nWhat are the pros/cons of these in terms of\n- speed\n- quality\n- other aspects?\n\nUse-case:\nMultiple mini-projects with each 4 collections and around 50-500 points each.\n1536 - Cosine"
  },
  {
    "threadId": "1219117664723669113",
    "name": "Downgrading to 1.7.1 would render the issue",
    "messages": "hello, I have encountered that downgrading qdrant version from 1.8.1 to 1.7.4 makes the following error in k8s environment:\n\n```\n2024-03-18T05:16:11.096079Z ERROR qdrant::startup: Panic occurred in file lib/collection/src/wal.rs at line 154: Can't deserialize entry, probably corrupted WAL on version mismatch: Syntax(\"data did not match any variant of untagged enum CollectionUpdateOperations\")\n```\n\nAre there any issues from someone who have encountered like this?"
  },
  {
    "threadId": "1219278025351499776",
    "name": "Monitor collection in memory",
    "messages": "How can I check how much memory a collection is using?"
  },
  {
    "threadId": "1219521717949435915",
    "name": "Qdrant-spark",
    "messages": "Hi All. I'm trying to write data into qdrant from spark. My spark version is 3.2.1 and my qdrant-spark connector version is 2.1.0. \n\nI keep running into a JAVA runtime mismatch error."
  },
  {
    "threadId": "1208004218309574736",
    "name": "No connection could be made because the target machine actively refused it",
    "messages": "I have docker container which has qdrant snapshot as well as collection, When I'm running the container in remote desktop it shows this error while connecting to my collection.\n\nthe client is being created."
  },
  {
    "threadId": "1176276642755252224",
    "name": "Error when using api key on self hosted db",
    "messages": "```def generateEmbeddings():\n    model_name = \"C:/Users/sashi/PycharmProjects/legalrag/rag/bge-base-en-v1.5\"\n    encode_kwargs = {'normalize_embeddings': True}  # set True to compute cosine similarity\n    model_norm = HuggingFaceBgeEmbeddings(\n        model_name=model_name,\n        model_kwargs={'device': 'cpu'},\n        encode_kwargs=encode_kwargs\n    )\n\n    vectordb = Qdrant.from_documents(documents=data, url=url,embedding=model_norm,prefer_grpc=True,  port=6333, api_key=\"adfs**********nj\", https=False, collection_name=\"aipg\")\n    return vectordb\n\nvectordb = generateEmbeddings()\nretriever = vectordb.as_retriever()```"
  },
  {
    "threadId": "1207719031608713297",
    "name": "collection is optimising for more than 24 hours",
    "messages": "Hello!\n\nMy qdrant collection is optimising for more than 24 hours, I have checked the telemetry, merge optimisation is going on.\n\n```\"optimizations\": {\n    \"status\": \"ok\",\n    \"optimizations\": {\n      \"count\": 0,\n      \"last_responded\": \"2024-02-14T03:58:43.910Z\"\n    },\n    \"log\": [\n      {\n        \"name\": \"merge\",\n        \"segment_ids\": [\n          13395926331535385000,\n          16472707112722290000,\n          13379875295337552000,\n          12646532230853243000\n        ],\n        \"status\": \"optimizing\",\n        \"start_at\": \"2024-02-14T11:49:37.780289375Z\",\n        \"end_at\": null\n      }\n    ]\n  }```\n \n```\"config\": {\n      \"params\": {\n        \"vectors\": {\n          \"size\": 150,\n          \"distance\": \"Cosine\"\n        },\n        \"shard_number\": 1,\n        \"replication_factor\": 1,\n        \"write_consistency_factor\": 1,\n        \"on_disk_payload\": true\n      },\n      \"hnsw_config\": {\n        \"m\": 100,\n        \"ef_construct\": 200,\n        \"full_scan_threshold\": 1000000,\n        \"max_indexing_threads\": 0,\n        \"on_disk\": true\n      },\n      \"optimizer_config\": {\n        \"deleted_threshold\": 0.2,\n        \"vacuum_min_vector_number\": 1000,\n        \"default_segment_number\": 0,\n        \"max_segment_size\": null,\n        \"memmap_threshold\": 1000000,\n        \"indexing_threshold\": 1000000,\n        \"flush_interval_sec\": 5,\n        \"max_optimization_threads\": 2\n      },\n      \"wal_config\": {\n        \"wal_capacity_mb\": 32,\n        \"wal_segments_ahead\": 0\n      },\n      \"quantization_config\": null\n    }```\n\nShould I change anything to make it faster or leave it as it is?"
  },
  {
    "threadId": "1207989261144559669",
    "name": "How to optimize insert rate from a continuous data stream",
    "messages": "This question relates to the article I linked in the community-blogposts channel: https://discord.com/channels/907569970500743200/1193339960502272100/1207984151362408489\n\nI set up a demo where a Kafka consumer continuously reads embeddings (and the document payload) from a Kafka topic and ingests them into Qdrant cloud.\n\nFor small chunks of text (like product descriptions) this works fine, but for larger chunks (e.g 4000 chars) it can take a while to ingest, and I've even had the Kafka consumer time out because qdrant ingestion exceeded the default time limit.\n\nI know its more common to upload data in bulk, and I've seen the optimizations suggested here: https://qdrant.tech/documentation/tutorials/bulk-upload/\n\nBut are there any other optimizations specific to ingesting from a data stream that would apply here?\n\nContext: Someone on LinkedIn asked (in relation to the article) \"what streaming insert rate into qdrant can we expect?\" but there are so many variables, its difficult to answer.\n\nhttps://www.linkedin.com/feed/update/urn:li:activity:7163206559823601666?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7163206559823601666%2C7163317435142430720%29&dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287163317435142430720%2Curn%3Ali%3Aactivity%3A7163206559823601666%29"
  },
  {
    "threadId": "1186718269453914174",
    "name": "Error in filter",
    "messages": "Hi all when I attempt to run a query with this filter\n```\nquery_filter=models.Filter(\n                    must_not=[\n                        models.FieldCondition(key=payload_var_name, match=models.MatchAny(value=list_of_values))\n                    ]\n                )\n        )\n```\n I get the following error\n```\nValidationError: 2 validation errors for MatchAny\nany\n  Field required [type=missing, input_value={'value': ['.NET Application Architect']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.5/v/missing\nvalue\n  Extra inputs are not permitted [type=extra_forbidden, input_value=['.NET Application Architect'], \ninput_type=list]\n    For further information visit https://errors.pydantic.dev/2.5/v/extra_forbidden\n```\nim just showing output with a list of size 1 but the code errors with longer lists as well.\nAny advice would be helpfull. Thank you!"
  },
  {
    "threadId": "1207747460160819210",
    "name": "Search Limits",
    "messages": "Hi i want to learn what are the limits for search. I have a usecase where i need to do an extensive search. I was using milvus which has a limit for 16k entities. i want to know if qdrant has any such limits while searching. I am not able to get this info online in documentation. can someone help me out to get this info.\nThanks."
  },
  {
    "threadId": "1207352892869320754",
    "name": "Payload points are Zero and not reflecting schema",
    "messages": "I created a collection, defined a schema, and upserted data into Qdrant. While the data is stored and payloads are visible, points associated with each payload item are zero. This suggests the data doesn't conform to the originally defined schema. Can you help me to fix it?"
  },
  {
    "threadId": "1206352929913577502",
    "name": "Creating indexes at the group level - multitenancy",
    "messages": "Hello, had a question regarding indexes and multitenancy. From the multitenancy section in the docs it describes creating an index for a fieldname `group_id` at the collection level. Is my understanding correct here? If so, how can we create an index for a specific group? If i had two groups in my collection (`group1` and `group2`), how can i create an index for just `group1`?\n\nThank you!"
  },
  {
    "threadId": "1207661943427436544",
    "name": "Scale down Memory",
    "messages": "Hi I would like to scale down my memory usage from 8 GB to 4GB because I noticed that I only use 3.5 GB. \nWhere can I choose another Package where you scale down RAM and CPU?\n\nThis is a followup to this discussion (sorry my reply there was quite late ЁЯШз) https://discord.com/channels/907569970500743200/1192806108436369469"
  },
  {
    "threadId": "1207330957321969664",
    "name": "Feature similar to `collapse` in ElasticSearch?",
    "messages": "Hi, is there a feature in Qdrant that lets you return \"diverse\" matches? \nUse case:\nWe have a job index where job description vectors are stored along with metadata about the jobs.\nWe have a bunch of jobs with similar title and company, but a different location/salary. When performing a similarity search, we are getting back seemingly duplicate jobs, because the job descriptions in these jobs have a lot of common text. \nThis is possible despite using filters.\n\nIs there a way we can group results based on a key (like `collapse` in ES), or is there a Qdrant way to handle this sort of a problem?"
  },
  {
    "threadId": "1207696623795179650",
    "name": "Precision in quaterion",
    "messages": "I have been fine-tuning embedding models with quaterion and the metrics I get are MUCH worse then running SentenceTransformer.fit. \nRunning pair-wise with subgroups and MNRL assymetric loss.\nI am getting single-digit precision w/ Quaterion vs 90%+ w/ SentenceTansformer fit (with of course a ton more compute)\nAre these expecyted results?"
  },
  {
    "threadId": "1207622728421478410",
    "name": "Incorrect data on Qdrant Trust Center?",
    "messages": "Looking at https://qdrant.to/trust-center As subprocessors Amazon Web Services is mentioned as Cloud infrastructure. It says:\nData location: United States\n\nIs this correct info? Isn't this dependent on your AWS region or is there other data being processed when using Qdrant managed cloud?"
  },
  {
    "threadId": "1207352673381257237",
    "name": "Improve precision",
    "messages": "I have a uploaded a documentation of a product, this Documentation consists of alot of pages, and I broke down each page to chunks by header.\nI have severl version of a product 1.1,1.2.1.3 etc...\n\nWhen I request to see the last feature updated, Im not retrieaving the actual last version and its features.\nor for example I try to find in which version was 'XYZ\" functionally was added, it can return documentation for some version but the version I need(the version is just an example)\n\nhow can Improve the information I retrieve, or apply any solution such iterataing through chunk with a certian logic?"
  },
  {
    "threadId": "1207607970247217192",
    "name": "Indexing in batch or streaming",
    "messages": "Hi all!\n\nI have a question regarding indexing. I understand that points are added to segments in the collection as they come, and that the optimizer when needed rebuilds new segments from scratch.\n\nAlso when indexing, we have the option of index as we upsert, or deactivate indexing, batch upsert and then activate indexing.\n\nMy question is, is there a difference in how the index is created when we \"stream\" points for indexing vs \"batch\" indexing? When I say a difference I mean, is the index constructed the same, or does the resulting index depend on the order of the upserts? If not, is there a preferred way to index?"
  },
  {
    "threadId": "1207188454056923146",
    "name": "qdrant persist",
    "messages": "I want help in saving the qdrant vectors to disk"
  },
  {
    "threadId": "1207228715042471967",
    "name": "Require authentication in dashboard",
    "messages": "We have to deploy Qdrant to production with Kubernetes, but we don't want anonymous users to have access to the dashboard, and we want our developers to have access to the dashbord.\n\nWhat can we do?"
  },
  {
    "threadId": "1207101061500895255",
    "name": "What are best practices for working with embeddings in Qdrant",
    "messages": "What are the best practices for ensuring that the same embedding generation process with a Large Language Model (LLM) is used for both storing vector embeddings in Qdrant and querying data from the Qdrant database, based on a prompt? Adding information of the LLM to the payload?"
  },
  {
    "threadId": "1207215774339694602",
    "name": "I cannot add new data to the collection I created before",
    "messages": "Hello, when I want to add new data to my collection I created before, it deletes my old data and adds new ones. \nThis is my code"
  },
  {
    "threadId": "1206944901153947698",
    "name": "Qdrant Kubernetes Operator",
    "messages": "Hello Qdrant Community,\nHow to setup Qdrant Kubernetes Operator. I emailed support and they told me I needed to contact them here."
  },
  {
    "threadId": "1207004623563653140",
    "name": "Scroll with no limit",
    "messages": "Is there a way to scroll with no limit using Python or Rust clients?"
  },
  {
    "threadId": "1204840435860906055",
    "name": "Binary Quantization Resulting in Negligible Latency Improvement",
    "messages": "I am performing requests to a cloud instance running in AWS US East-1 with 64GB RAM and 8 vCPU with 3 nodes. The cluster is under negligible load (~10% of CPU / ~10 rps) and has 30% of its memory available. Removing all load does not materially change the latency of my query.\n\nI have a batch recommend query that is part of my critical path wherein I am providing on the order of 250-300 requests in the batch (with 1 positive point id in each request). A keyword filter is applied to the payload (which is indexed). I also specify a limit and similarity threshold. For the query I am testing against the candidate set of points after applying the payload filter is on the order of 10,000. I am using the `lookup_from` option as the points provided in the request are in a different collection from the collection I seek recommendations from. I am only requesting the payload (which I understand is stored on disk) and not the vectors; however, when I test requesting neither the payload nor the vector, the decrease in latency is marginal, so the fetching of payloads does not appear to be a material contributor to the overall latency.\n\nIt should be noted that I have tested this query also using batch search rather than recommend to control for the influence of `lookup_from`, and there appeared to be no material difference between the two.\n\nThis request consistently takes roughly 1.5 seconds to complete. This is wall clock time from before performing the request to receiving the response. This latency is the observed latency when performing it against collections that do not have quantization enabled and that store 1536 dimension ada embeddings using cosine similarity.\n\nMore details follow as it exceeds the max message size..."
  },
  {
    "threadId": "1206488291134939147",
    "name": "How to delete or remove or drop already create vector index, i want create a fresh index.",
    "messages": "How to delete or remove or drop already create vector index, i want create a fresh index."
  },
  {
    "threadId": "1206950341468233798",
    "name": "how to control the number of results from DB to RAG to avoid token error?",
    "messages": "Currently, I am using the below code \n\n   # Create Qdrant vector store\n            embedding_function = HuggingFaceEmbeddings(model_name=\"sentence-t5-base\")\n            qdrant_vec_store = Qdrant(client, self.collection_name, embedding_function, content_payload_key= self.payload_key)\n\n            # Create RetrievalQA\n            rag = VectorDBQA.from_chain_type(llm=llm, chain_type=\"stuff\", vectorstore=qdrant_vec_store, return_source_documents=true)\n\nbut this setup is giving a token error from LLM and I want to reduce the number of  similar text outputs coming from qdrant_vec_store, suppose currently its 3 points then I want to reduce it to 1, does anyone know any parameter to control the input so LLM will not give token error ?"
  },
  {
    "threadId": "1206931098911776769",
    "name": "Uploading Qdrant collection in Azure",
    "messages": "I've Qdrant collection in docker, I'm using it locally. I want to upload it on Azure, Do i need to upload only the collection or do I need to upload image of qdrant and collection within the image of qdrant which is in docker to the Azure? What are the other ways we can upload the collection and use it using qdrant in Azure. How many containers do I need to create? 1) Project, 2)Qdrant image?"
  },
  {
    "threadId": "1217777421181255731",
    "name": "Qdrant version check",
    "messages": "Quick question, is there a way to get the qdrant version running in the cluster via the HTTP API? Thanks!"
  },
  {
    "threadId": "1216415454785048676",
    "name": "how to update payload array object key value ?",
    "messages": "hello , I wanted to set_payload on basis of some filters...i only want to update object in an array field, hence i am giving payload : { key[].title=value}, but instead og updating in key array object its actually creating a new separate key named 'key[].title'.  Is there any method to update an object's key value inside an array in payload of qdrant vector database"
  },
  {
    "threadId": "1204582677689278524",
    "name": "Delete points and vectors based on Payload Id.",
    "messages": "So, doing a curl on GET /collections/<collection name>/points/{id}\n\nGives me The Image I attached. \n\nNow, my payload has a field called file_id which tells the points which file the vectors belong to. I am creating a service where I want to delete vectors and points based on which points have payload that have the file_id that I mention. Say I want to delete all points and vectors that have \"testFile2.txt\" in their file_id payload. I need help with this because I am unable to figure out a way to do this."
  },
  {
    "threadId": "1206268739461324831",
    "name": "Preprocess text before vectorization",
    "messages": "I am using qdrant for pdf Q&A. Long story short, results are bad. \n\nNow I'm wondering If it is important to preprocess the text before vectorization?\n\n1) Remove stopwords\n2) Lemmatize"
  },
  {
    "threadId": "1204803464123912223",
    "name": "Keeping track of Points ID",
    "messages": "Im using python, having a list, I want to run that list and insert into the collection.\nhowever, I need to know which point ID is the last to avoid override, so 2 questions:\n1. how to get last point ID?\n2. after some time there might be gaps in point ID(due to point deletion) is there a way to find those gaps in ID to fill them?\n\nThank you"
  },
  {
    "threadId": "1205896419332849694",
    "name": "Max recommended  number of Vectors to delete",
    "messages": "We're doing it on an API call, so anything that takes longer than 30s.\n\nIs it 1000, 10,000, etc"
  },
  {
    "threadId": "1205809764148453386",
    "name": "Not sure how to load qdrant cloud collection to build langchain based RAG",
    "messages": "I have a collection named \"faq_collection\" on qdrant cloud, now I want to build llama-based RAG using qdrant. Now the problem I am facing, below is the code I need to run to implement RAG \n\"\nfaq_chain = VectorDBQA.from_chain_type(\nllm=llm,\nvectorstore=doc_store,\nverbose=True\n)\"\n\nwhen I am building doc_store with qdrant, I want to use my already loaded collection from the cloud, but the code I am getting everywhere is using local data like this \" Qdrant.from_texts(text, embeddings, host = api_url, api_key= api_key)\" where \"text\" is local list. Can someone please tell me how I can build a \"doc_store\" that actually load data from the cloud and can be used in the RAG model \n\nA few key pages I followed \nhttps://qdrant.tech/articles/langchain-integration/\nhttps://qdrant.tech/documentation/frameworks/langchain/\nhttps://python.langchain.com/docs/integrations/retrievers/self_query/qdrant_self_query \nhttps://python.langchain.com/docs/integrations/vectorstores/qdrant"
  },
  {
    "threadId": "1204526307489349683",
    "name": "Understanding Payload Memory Usage",
    "messages": "I am  confused about payload storage.\n\n\nWith `on_disk_payload: true`, we max out at a stable 20GB of memory.\n\nWith `on_disk_payload: false`, we crash during ingestion, using over 90GB of memory.\n\nhttps://github.com/qdrant/qdrant/assets/3887682/1bc2bdcd-6979-4ed9-8c93-880fee9a0ac5\n\nBut our payloads should be very small\n\n```\n{\n  \"field_name\": \"v\",\n  \"field_schema\": \"bool\"\n}\n\n{\n  \"field_name\": \"s\",\n  \"field_schema\": \"integer\"\n}\n```\n\nWe are storing an integer and a boolean. \n\nFor 64 bit integers, we should only need about 500MB of memory for all 66M records, and even less for the boolean.\nEven if we need to store the `field_name` independently, that should still only be like 2GB across the board?\n\nFinding it very hard to capacity plan, we keep running experiments and being surprised - the amount of memory consumed seems wildly out of proportion to the size of our actual payloads"
  },
  {
    "threadId": "1205335152603959316",
    "name": "Format error in JSON body: Data did not match any variant of untagged enum VectorStruct",
    "messages": "attached is one of the image nodes I was trying to insert with Llama Index, the difference is I don't know OpenCLIP's embedding size so this could maybe be that? the ID is a `ulid` and yeah. Error messages are too vague."
  },
  {
    "threadId": "1205541158931931166",
    "name": "How to forcely remove a peer in crash loop",
    "messages": "Hi,  I have a pod enters crashloop, I want to remove it from the cluster but it complains there are still shard on it, How I can forcefully remove the peer?"
  },
  {
    "threadId": "1192806108436369469",
    "name": "Scale down Memory",
    "messages": "Hi I would like to scale down my memory usage from 8 GB to 4GB because I noticed that I only use 3.5 GB. How can I do that?"
  },
  {
    "threadId": "1199786609009246289",
    "name": "Can't search sparse vectors with Batch search",
    "messages": "when I search with sparse separately then I am getting desired output:\noutput = await client.search(\ncollection_name=collection_name,\nquery_vector=models.NamedSparseVector(\n            name=\"text\",\n            vector=models.SparseVector(\n                indices=query_indices,\n                values=query_values,\n            ),),\nquery_filter=filter,\n    limit=1,\n    with_payload=True\n)\n-> \n[ScoredPoint(id=7359, version=0, score=16.72979736328125, payload={'model_type': 'test_10k_0'}, vector=None, shard_key=None)]\n\nbut with Batcg Search it is failing with error:\n  return model.vector, model.name\n else:\n         raise ValueError(f\"invalid NamedVectorStruct model: {model}\")\n\nValueError: invalid NamedVectorStruct model: name='text' vector=SparseVector(indices=[1029, 2001, 2002, 2010, 2018, 2020, 2032, 2040, 2253, 2332, 2338, 2343, 2365, 2563, 3159, 3364, 3410, 3656, 3910, 4074, 4300, 5708, 6384, 7150, 7664, 8925, 9895, 10662, 13084, 13402, 15993], values=[0.1526612937450409, 1.2225030660629272, 0.93537437915802, 0.5707768201828003, 0.06368067860603333, 0.22014950215816498, 0.07290615886449814, 0.04111705720424652, 0.3879556655883789, 0.5963300466537476, 0.07041019946336746, 0.088597372174263, 0.1444776952266693, 0.1480744183063507, 0.40767455101013184, 1.0532915592193604, 0.5268365144729614, 0.12913961708545685, 1.8436529636383057, 0.08647909760475159, 3.043473958969116, 0.3512234389781952, 0.11374594271183014, 0.06685321033000946, 0.17790648341178894, 0.0639503225684166, 0.018158389255404472, 0.24518823623657227, 0.4965633749961853, 3.1616880893707275, 0.46298572421073914])\n\n\nneed support ASAP"
  },
  {
    "threadId": "1169521948355727361",
    "name": "Qdrant vectors from collection",
    "messages": "Hi, I am new to Qdrant , can anyone suggest how can retrieve the vectors from collection and view it"
  },
  {
    "threadId": "1205201180758646864",
    "name": "Vectors not visible localhost dashboard",
    "messages": "This is following the basic onboarding to create a test_collection and insert some vectors in.\nIs this a bug in the UI?\n\n```\ncurl -L -X PUT 'http://localhost:6333/collections/test_collection/points?wait=true' \\\\n    -H 'Content-Type: application/json' \\\\n    --data-raw '{\\n        \"points\": [\\n          {\"id\": 1, \"vector\": [0.05, 0.61, 0.76, 0.74], \"payload\": {\"city\": \"Berlin\"}},\\n          {\"id\": 2, \"vector\": [0.19, 0.81, 0.75, 0.11], \"payload\": {\"city\": [\"Berlin\", \"London\"] }},\\n          {\"id\": 3, \"vector\": [0.36, 0.55, 0.47, 0.94], \"payload\": {\"city\": [\"Berlin\", \"Moscow\"] }},\\n          {\"id\": 4, \"vector\": [0.18, 0.01, 0.85, 0.80], \"payload\": {\"city\": [\"London\", \"Moscow\"] }},\\n          {\"id\": 5, \"vector\": [0.24, 0.18, 0.22, 0.44], \"payload\": {\"count\": [0] }},\\n          {\"id\": 6, \"vector\": [0.35, 0.08, 0.11, 0.44]}\\n        ]\\n    }'\n```\n\n```\n  qdrant:\n    image: qdrant/qdrant\n    container_name: qdrant\n    ports:\n      - \"6333:6333\"\n``"
  },
  {
    "threadId": "1205292116327268382",
    "name": "Update payload",
    "messages": "Is there a way to update a vector's payload without entirely overwriting it?\n\nSay I have the following payload and I only want to update the count value or add an element to sizes list. Is it possible to achieve this or is it planned for future releases?\n\n{\n    \"count\": 10,\n    \"sizes\": [35, 36, 38]\n}"
  },
  {
    "threadId": "1205122271488770068",
    "name": "Distributed Deployment for Qdrant using helm not working (v1.7.4)",
    "messages": "Hi! I have installed qdrant in my EKS cluster using helm. I have enabled distributed deployment with 3 replicas. However, when I uninstall the helm release and try to reset the distributed deployment replicas, the connections still persist. I get the following result even after reinstalling the helm release with just one replica and disabled distributed deployment:\n \nCommand: GET /cluster\nResult:\n```\n{\n  \"result\": {\n    \"status\": \"enabled\",\n    \"peer_id\": 6807553274081970,\n    \"peers\": {\n      \"7447274613747405\": {\n        \"uri\": \"http://qdrant-2.qdrant-headless:6335/\"\n      },\n      \"6807553274081970\": {\n        \"uri\": \"http://qdrant-0.qdrant-headless:6335/\"\n      },\n      \"163649704023659\": {\n        \"uri\": \"http://qdrant-1.qdrant-headless:6335/\"\n      }\n    },\n    \"raft_info\": {\n      \"term\": 18874,\n      \"commit\": 140,\n      \"pending_operations\": 0,\n      \"leader\": 0,\n      \"role\": \"Candidate\",\n      \"is_voter\": true\n    },\n    \"consensus_thread_status\": {\n      \"consensus_thread_status\": \"working\",\n      \"last_update\": \"2024-02-08T12:02:38.626007041Z\"\n    },\n    \"message_send_failures\": {\n      \"http://qdrant-2.qdrant-headless:6335/\": {\n        \"count\": 18052,\n        \"latest_error\": \"Error in closure supplied to transport channel pool: status: Unavailable, message: \\\"Failed to connect to http://qdrant-2.qdrant-headless:6335/, error: transport error\\\", details: [], metadata: MetadataMap { headers: {} }\"\n      },\n      \"http://qdrant-1.qdrant-headless:6335/\": {\n        \"count\": 18052,\n        \"latest_error\": \"Error in closure supplied to transport channel pool: status: Unavailable, message: \\\"Failed to connect to http://qdrant-1.qdrant-headless:6335/, error: transport error\\\", details: [], metadata: MetadataMap { headers: {} }\"\n      }\n    }\n  },\n  \"status\": \"ok\",\n  \"time\": 0.000015419\n}\n```\nCould really use some help to reset the distributed deployment and manage it via helm"
  },
  {
    "threadId": "1205144067781165178",
    "name": "Does the UI work with collections of multiple vectors?",
    "messages": "Hi! I have a collection with two types of vectors, each one with a different vector size.\n\nI am trying to visualize the points using the UI, but it is not displaying any points.\n\nIf I run \n```{\n  \"limit\": 500,\n  \"vector_name\": \"text\"\n}```\nI do not see any vector displayed.\n\nIf I run\n```{\n  \"limit\": 500,\n  \"vector_name\": \"image\"\n}```\nI get this error: `Visualization Unsuccessful, error: No vector found with name image` \nBut there are points of both vector types."
  },
  {
    "threadId": "1205166982480273489",
    "name": "What would happen if I try to create the same Payload Index twice?",
    "messages": "I have a situation where  I don't know if a Payload Index was created for a collection or not. So if I trigger this multiple times, will Qdrant safely ignore subsequent calls? https://qdrant.tech/documentation/concepts/indexing/#payload-index"
  },
  {
    "threadId": "1204901127242514452",
    "name": "Partse SearchResponse",
    "messages": "Hi\n\nthe search collection provides this response\n\nSearch Response: SearchResponse { result: [ScoredPoint { id: Some(PointId { point_id_options: Some(Num(8454145)) }), payload: {}, score: 0.3756859, version: 17, vectors: None, shard_key: None }, ScoredPoint { id: Some(PointId { point_id_options: Some(Num(8224770)) }), payload: {}, score: 0.367375, version: 16, vectors: None, shard_key: None }, ScoredPoint { id: Some(PointId { point_id_options: Some(Num(9207810)) }), payload: {}, score: 0.3604974, version: 18, vectors: None, shard_key: None }, ScoredPoint { id: Some(PointId { point_id_options: Some(Num(6782987)) }), payload: {}, score: 0.20516166, version: 12, vectors: None, shard_key: None }, ScoredPoint { id: Some(PointId { point_id_options: Some(Num(6914094)) }), payload: {}, score: 0.19635947, version: 13, vectors: None, shard_key: None }], time: 0.011056175 }\n\nhow do i parse this in rust and the id to find the original text of the documents?\n\nThanks"
  },
  {
    "threadId": "1187228597866467358",
    "name": "Does Qdrant Cloud / AWS Marketplace support Japan Region?",
    "messages": "As the title says"
  },
  {
    "threadId": "1205060392888438817",
    "name": "Search only via payload key",
    "messages": "Hi is it possible to perform text search over the collection with only the payload details. I dont have any vector embeddings over which I want to perform search but rather with the payload details only.\nIs it achieved by scroll method?"
  },
  {
    "threadId": "1204424714089529364",
    "name": "Filtering for recommendations",
    "messages": "Hi all, we've been using Qdrant very happily for 6 months now. One of our use cases is for part of a recommendation engine. We embed items and then recommend them to users. Part of the algorithm of what to show next involves cosine similarity, which we implement with Qdrant search. There are about 50-100k different items/vectors and the key is that users should not see items that were recommended to them in the past again. Right now we are implementing this with a must_not filter on the IDs of all seen items, which we are storing separately. However this doesn't seem very scalable (?) once we reach >1k previously seen items per user, which we are getting to quickly for some users. Any thoughts/recommendations for this use case? Can Qdrant handle >1k or even >10k must_not filters by id? Should we handle this by post-processing instead -> having qdrant return hundreds of results and then do post-processing to remove already-seen ones with a bloom filter or Redis or some other efficient algorithm?"
  },
  {
    "threadId": "1204947591746293790",
    "name": "Search by point id",
    "messages": "Is there a way to directly search by point id (with any python or rust client) instead of passing the entire vector?"
  },
  {
    "threadId": "1204905005061247026",
    "name": "Exact search support",
    "messages": "Hi, does qdrant support exact search on similarity? For smaller amount of candidate vectors, exact search performs actually better approximate KNN. Ideally I should be able to say < 1m vectors, scan all vectors instead of doing HNSW. Is that something supported?"
  },
  {
    "threadId": "1204793957528375357",
    "name": "Updating Metadata of points ids",
    "messages": "Is it possible to update metadata? I have a list of points ids with a metadata key named source and I want to modify the value of that key."
  },
  {
    "threadId": "1204445616311111690",
    "name": "Updating points in a multitenant collection",
    "messages": "We are using a single collection partitioned by filters ( multitenancy from the qdrant documentation). Since the collection is used in production, how do I update all points in a partition (for example partitioned on a payload field called group_id) without downtime? I have a solution with alias, where I'll create a new collection with the updates and atomically delete and update alias. But this seems too much overhead for a simple update operation. Is there a recommended way to do this?"
  },
  {
    "threadId": "1204737699475492904",
    "name": "Adding sparse vectors in parallel of an existing dense vectors collection",
    "messages": "We have an existing dense vectors collection and want to test hybrid mode. To do that we could recreate the collection and reindex, however we are wondering if there is a method to create the sparse vectors part based on the existing data, ie without recreating/deleting the previously generated vectors."
  },
  {
    "threadId": "1202986503714771044",
    "name": "Trying to create a collection",
    "messages": "Trying to create a collection, I already have 1 collection that I one created, trying to create another one and getting \"ResponseHandlingException: The read operation timed out\"\nThe code is simple- \n\nimport qdrant_client\nimport os\nfrom qdrant_client import models, QdrantClient\nfrom sentence_transformers import SentenceTransformer\nclient = qdrant_client.QdrantClient(\n    os.getenv('QDRANT_HOST'),\n    api_key=os.getenv('QDRANT_API_KEY')\n)\nclient.get_collection('xxx') #to check if it works- I get that the collection exists\nencoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n#create collection (or database) on a cluster in QDRANT\nvectors_config = qdrant_client.http.models.VectorParams(\n    size = 1536, # OpenAI uses 1,536 dimensions for embeddings\n    distance = qdrant_client.http.models.Distance.COSINE)\n\n\ncollection_name = \"maintest\"\nclient.create_collection(\n    collection_name=collection_name,\n    vectors_config=vectors_config\n)\nand the error of time out occurs, please assist"
  },
  {
    "threadId": "1204337568431341608",
    "name": "Enhancing UI Access with ReadOnlyAPIKey",
    "messages": "We've configured a ReadOnlyAPIKey to securely explore our database. \n\nHowever, the endpoints accessible with this read-only API key are limited, preventing us from clicking on the collection from the UI to explore payloads from the dashboard. While we can access the Console and query GET method endpoints without issue.\n\nI'm considering whether to create a new issue or pull request (PR) to propose this feature enhancement. Is there a specific reason why the UI cannot be accessed with a read-only API key?"
  },
  {
    "threadId": "1204136565769769021",
    "name": "Search partially in a collection based on metadata",
    "messages": "Hi. I have a collection created from several documents. I'm using metadata to identify each point with its  original file. Would it be possible to do a search only on points that match a value on that meta? Thanks"
  },
  {
    "threadId": "1204101462469124107",
    "name": "How often does telemetry run on Qdrant?",
    "messages": "I am having an issue on my Qdrant cluster where 7GB of outbound network traffic is happening every hour on the dot. I am trying to rule out possible causes for this. What is Qdrant's schedule for running telemetry. Thank you for your time!"
  },
  {
    "threadId": "1202825832289140766",
    "name": "Qdrant Memory usage high",
    "messages": "Hi folks I'm using Qdrant Cloud and these are my settings. Looking at the Qdrant Cloud Dashboard, I'm already using over 4GB of ram.\nI only have 40K vectors/points saved taking up only 1GB of disk space.\nCan someone please help explain why it uses this much memory?\n\n```\n{\n  \"params\": {\n    \"vectors\": {\n      \"size\": 768,\n      \"distance\": \"Cosine\"\n    },\n    \"shard_number\": 1,\n    \"replication_factor\": 1,\n    \"write_consistency_factor\": 1,\n    \"on_disk_payload\": true\n  },\n  \"hnsw_config\": {\n    \"m\": 16,\n    \"ef_construct\": 100,\n    \"full_scan_threshold\": 10000,\n    \"max_indexing_threads\": 0,\n    \"on_disk\": false\n  },\n  \"optimizer_config\": {\n    \"deleted_threshold\": 0.2,\n    \"vacuum_min_vector_number\": 1000,\n    \"default_segment_number\": 0,\n    \"max_segment_size\": null,\n    \"memmap_threshold\": null,\n    \"indexing_threshold\": 20000,\n    \"flush_interval_sec\": 5,\n    \"max_optimization_threads\": 1\n  },\n  \"wal_config\": {\n    \"wal_capacity_mb\": 32,\n    \"wal_segments_ahead\": 0\n  },\n  \"quantization_config\": null\n}\n```"
  },
  {
    "threadId": "1204076505056149605",
    "name": "{\"status\":{\"error\":\"Payload error: JSON payload (1733575144 bytes) is larger than allowed",
    "messages": "What is the solution, can batch processing solve the issue? my total file size with embeddings is of 1.30 gb"
  },
  {
    "threadId": "1204067201951207434",
    "name": "Unexpected Response:400(Bad Request)",
    "messages": "I have image embeddings and would like to upload them to qdrant collection but getting below errr. I have tried using \"upsert\" and \"upload_points\" but neither of them workded. Below is the error :"
  },
  {
    "threadId": "1203695854146555954",
    "name": "miles / kms [Geo-distance query] filter ?",
    "messages": "Does Qdrant supports miles/kms filter given latitude and longitude ?"
  },
  {
    "threadId": "1200074159728894023",
    "name": "Payload storage",
    "messages": "Hi,\nI assume that payload is stored in a classic relational db table, hence the ability to build index.\nI have a large collection of 1M+ vectors with 12 payload field each. So in theory 12M+ payload elements.\nI would like to understand how is this data stored. \nIs it all in one table?\nIs each payload element stored as 2 lines (one key, one value) or do you create columns for each payload key?\nDo you create new payload table for each collection or everything is stuffed in the same table for all collection?\n\nAsking because we ultimately abandoned Chroma for this exact reason (all metadata are stored as 2 lines in one table for all collections, so even when creating a covering index it still is slow with this amount of data). We've migrated to another solution but tempted to give Qdrant a try. Our total data size is about 75Gb."
  },
  {
    "threadId": "1203411516821475329",
    "name": "qdrant_client.http.exceptions.ResponseHandlingException: timed out #general-help",
    "messages": "Hey, guys\n\n\nI am running qdrant with `docker compose`\n\ndocker compose configuration is straight forward, freshly built\n\n```  qdrant:\n    image: qdrant/qdrant\n    volumes:\n      - ${DATA_PATH}/qdrant:/qdrant/data\n    restart: unless-stopped\n```\n\nthis error occurs in non-stable fashion.\n\nI connect  from sibling container:\n\n`client = QdrantClient(\"qdrant\")`\n\nerror occurs when\n\n```client.create_collection(\n            collection_name=client_id,\n            vectors_config=VectorParams(size=256, distance=Distance.COSINE),)\n```\n\nor \n\n```client.search(\n        collection_name=client_id, query_vector=emb, limit=1)```\n\nfull traceback is attached as txt\n\n\nthank you!"
  },
  {
    "threadId": "1203971559509721098",
    "name": "LlamaIndex + Qdrant: Keep getting this error \"The write operation timed out\"",
    "messages": "I've been using LlamaIndex along with Qdrant, hosted on Qdrant's cloud. Initially, I started with the free tier cluster but later upgraded to a production-grade cluster. Despite this, I continue to face the issue described below.\n\nI'm working with a large JSON file (417 kB), and the indexing step consistently fails with this error:\n\nERROR - Unexpected err=ResponseHandlingException(WriteTimeout('The write operation timed out')), type(err)=<class 'qdrant_client.http.exceptions.ResponseHandlingException'>\n\nThe LlamaIndex team confirmed that this is a Qdrant error.\n\nI can eliminate the error by adjusting the batch size (the default is 100):\n\nvector_store = QdrantVectorStore(client=client,\n                                  collection_name=collection_name,\n                                  batch_size=20)\n\nHowever, this adjustment slows down the process. With this batch size, approximately 35-40 calls are made to Qdrant from the LlamaIndex app for the indexing step.\n\nGiven this, I have two questions:\n\n1. Is there a standard method to determine the appropriate batch size based on the document size? In other words, is there a best practice or a formula I could use?\n2. Besides changing the batch size, is there another way to complete the operation without experiencing a timeout?"
  },
  {
    "threadId": "1203899737472897124",
    "name": "Search filters or multiple collections?",
    "messages": "Hi,\nWe have a single collection with 1M+ vectors and the search works very well.\nWe currently have a filter on a single field on 99% of our queries. This field's cardinality is around 5000 different values.\nWe were thinking that we could reindex this data by creating 5000+ collections, ie one per value of that field, and therefore do not use a filter.\nDo you think this would help gain some performance for the search query, or the overhead of switching collection from the previous query would be equal to the filter overhead?"
  },
  {
    "threadId": "1203967194132652092",
    "name": "How to use existing Collection in Cloud Cluster?",
    "messages": "In wide terms, how to create an vector store instance which uses my exisitng collection in cloud rather than creating new everytime.\n#sorry for wrong englush"
  },
  {
    "threadId": "1214527844907225098",
    "name": "Recover deleted cluster?",
    "messages": "Hi all,\n\nI am using cloud version and deleted my cluster by mistake. Is there any way to recover? Thanks for any help. \n\nUrl of my cluster is below:\nhttps://46cdd47d-c6da-427c-b78c-7bcb03f1b397.ap-northeast-1-0.aws.cloud.qdrant.io:6333/"
  },
  {
    "threadId": "1214148692563599360",
    "name": "default keep-alive timeout for idle connections",
    "messages": "Is there any default keep-alive timeout set in the actix web server for idle connections, I couldn't find it in the server settings in the codebase, we're using self-hosted qdrant behind an Application Load Balancer and we see random 502s being thrown by the load balancer, this might be happening due the idle timeout of qdrant http server being lower than the idle timeout set in the load balancer."
  },
  {
    "threadId": "1214520345818832917",
    "name": "Data Visualization",
    "messages": "I'm using qdrant's vector visualization tool to gain insights into my dataset. Is there somewhere I can get a mathematical explanation of how the qdrant projection is done? What is the technique used?"
  },
  {
    "threadId": "1203261976113979402",
    "name": "Best way to store multiple documents",
    "messages": "Hi guys,\nI'm developing a bot to help my employees with various documents of my company and I'm evaluating Qdrant for achieving that.\n\nIn my scenario I have 3 different types of documents:\n-technical documents (almost 1000 PDFs)\n-rule books (almost 200 PDFs)\n-FAQ (almost 15 PDFs)\n\nI need a single bot that can answer questions about all these 3 topics.\n\nWhat is the best option in your opinion?\nSaving ALL the documents in a single Collection (1215 PDFs)  or create 3 different collections, one for each document type (technical, rules, FAQ)?"
  },
  {
    "threadId": "1202703438866284544",
    "name": "Vector Params for fast-bge-small-en are not specified in config",
    "messages": "Just trying to query the vector database\n\n```\nembeddings = vector_client.query(\n                collection_name=vector_store.collection_name,\n                query_text=key,\n                query_filter=Filter(\n                    must=[\n                        FieldCondition(\n                            key=key,\n                            match=MatchText(\n                                text=value,\n                            ),\n                        ),\n                    ],\n                ),\n            )\n```\n\nthere's nothing in there yet, I just need to, when there is, query for things that are there, will this work? Full error is\n```bash\n  File \"/Users/zachhandley/Documents/GitHub/Twin-Protocol/api/.venv/lib/python3.11/site-packages/qdrant_client/http/api/points_api.py\", line 1039, in search_points\n    return await self._build_for_search_points(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/zachhandley/Documents/GitHub/Twin-Protocol/api/.venv/lib/python3.11/site-packages/qdrant_client/http/api_client.py\", line 146, in request\n    return await self.send(request, type_)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/zachhandley/Documents/GitHub/Twin-Protocol/api/.venv/lib/python3.11/site-packages/qdrant_client/http/api_client.py\", line 169, in send\n    raise UnexpectedResponse.for_response(response)\nqdrant_client.http.exceptions.UnexpectedResponse: Unexpected Response: 400 (Bad Request)\nRaw response content:\nb'{\"status\":{\"error\":\"Wrong input: Vector params for fast-bge-small-en are not specified in config\"},\"time\":0.000758098}'\n```"
  },
  {
    "threadId": "1202910248382373900",
    "name": "Deploying on Azure Container Apps",
    "messages": "Hi!\nI found this repo that provides a template for deploying Qdrant on Azure Container Apps: https://github.com/Azure-Samples/qdrant-azure/tree/main/Azure-Container-Apps\nHowever, there's also the add-ons public preview where it's explicitly said that add-ons are not production ready and should only be used for testing:  https://learn.microsoft.com/en-us/azure/container-apps/services\nHence, my question is: do you consider deploying on Azure Container Apps production ready?\nThanks!"
  },
  {
    "threadId": "1203004743849480282",
    "name": "Filter method with multivector collection",
    "messages": "I want to create a system where I'll give summary embedding first, itll return the payload, after that based on payload id I want to give description embedding with filter that is for description search itll only search within the ids that were in result of summary payload"
  },
  {
    "threadId": "1202990463997452380",
    "name": "PointStruct error with payload",
    "messages": "I've payload like this,\n\npayload = jira_tickets_df[['itqueue', 'service','subservice']].to_dict(orient=\"records\")\npayload[:3]\n\n\n[{'itqueue': 'Service Desk',\n  'service': 'User Accounts',\n  'subservice': 'User Accounts'},\n {'itqueue': 'Central Systems',\n  'service': 'Email and Calendar',\n  'subservice': 'Email and Calendar'},\n {'itqueue': 'Service Desk',\n  'service': 'User Accounts',\n  'subservice': 'User Accounts'}]\n\n\nI want to upsert the payload in collection \nclient.upsert(\n    collection_name='Qdrant collection1',\n    points=[\n        rest.PointStruct(\n            id=k,\n            vector={\n                'summary': v['embeddings_summary'],\n                'description': v['embeddings_description'],\n            },\npayloads = payload\n            \n        )\n        for k, v in jira_tickets_df.iterrows()\n    ],\n)\n\nit is giving me ValidationError: 1 validation error for PointStruct\npayloads\n  Extra inputs are not permitted [type=extra_forbidden, input_value=[{'itqueue': 'Service Des...: 'Email and Calendar'}], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.5/v/extra_forbidden\n\nthis error, I cant find the solution how to add payload,\nthanks"
  },
  {
    "threadId": "1202914604448223263",
    "name": "Recommendation api not following filter rules?",
    "messages": "I'm trying to apply a recommendation filter to return results within a specific set of points. So I'm doing this: \ndata = {\"positive\": [query_embed], \"negative\": negative_filters, \"strategy\": \"best_score\", \"limit\": limit, \"with_payload\": True, \"filter\": {\"must\": [{\"has_id\": point_filters}]}}\nresponse = s.post(qdrant_url+f'{use_index}/points/recommend', json=data, headers=headers)\nThe idea is to get the recommended results within the point_filters set. \nBut I find that the result is empty. On debugging a little, this is because my limit is set to 20. Setting my limit to 50000 for eg. returns expected results. My understanding of FILTER was that it will return LIMIT number of results from within the FILTER set. Am I wrong in my understanding?"
  },
  {
    "threadId": "1197257043472560169",
    "name": "vector upload failure",
    "messages": "i use vector db benchmark repo to benchmark multiple databases. for now i use qdrant. i have 27 000 000 vectors of size 768. i use qdrant single node with no ram limit and server machine on aws with 128GiB memory and 16 vCPU on aws ec2. as per calculator 124 GiB should be enough ram for this dset. i run it via docker compose up. But after uploading first 10 millions of vectors i get `multiprocessing.pool.MaybeEncodingError: Error sending result: '<multiprocessing.pool.ExceptionWithTraceback object at 0x7feebb3c7c40>'. Reason: 'TypeError(\"cannot pickle '_thread.RLock' object\")'` error on client vm. client vm is (edit) 8 ~~16~~ vCPU 16GiB memory. when the failure happens the database is accessible via url and it's indexing (yellow status). I use m-32 and construction-ef 256. default upload batch size.\nAre there any better parameters (m, construction ef, upload batch size) i should use for such a large dataset?\nrepo link: https://github.com/qdrant/vector-db-benchmark"
  },
  {
    "threadId": "1202581240868245515",
    "name": "Does Qdrant have a basic demo UI that I can reuse to test similarity search?",
    "messages": "I am building a super basic continuous ingestion pipeline and I want to put a similarity search UI at the end of it so that people trying it out can compare the recency of the results. It could be kind of a companion the Qdrant Llamaindex / recency example (https://github.com/qdrant/examples/tree/master/llama_index_recency) because it ensures the DB is continuously fed with the latest documents and decouples the doc loading from the embedding creation. \n\nIn examples like this, the similarity search is usually illustrated within the context of a Jupyter Notebook, but I was wondering if there was something standalone that i could reuse. Otherwise, I'll was going to build a simple UI in Streamlit.\n\nNote: I was poking around in the Dashboard for my cloud instance but I dont really understand what  the \"Find by similar ID\" field is for."
  },
  {
    "threadId": "1202551008572604467",
    "name": "Qdrant is using too much memory",
    "messages": "We used to have faiss as our primary vector store, but it was difficult to maintain for the huge number of clients we have. Hence we decided to switch to qdrant and currently using it from the aws marketplace. We are trying it out in our qa environment but we are noticing increased memory usage which doesn't correlate with the number of vectors present. This is putting doubt in qdrant as with the current memory usage, we'll not be able to scale (which is conflicting with what we see in qdrant blogs)\n\nOur cluster has a single collection with the following config\n\n```\n{\n  \"result\": {\n    \"status\": \"green\",\n    \"optimizer_status\": \"ok\",\n    \"vectors_count\": 20261,\n    \"indexed_vectors_count\": 15602,\n    \"points_count\": 20200,\n    \"segments_count\": 6,\n    \"config\": {\n      \"params\": {\n        \"vectors\": {\n          \"openai\": {\n            \"size\": 1536,\n            \"distance\": \"Cosine\"\n          }\n        },\n        \"shard_number\": 2,\n        \"replication_factor\": 1,\n        \"write_consistency_factor\": 1,\n        \"on_disk_payload\": true\n      },\n      \"hnsw_config\": {\n        \"m\": 0,\n        \"ef_construct\": 256,\n        \"full_scan_threshold\": 1000,\n        \"max_indexing_threads\": 0,\n        \"on_disk\": true,\n        \"payload_m\": 64\n      },\n      \"optimizer_config\": {\n        \"deleted_threshold\": 0.2,\n        \"vacuum_min_vector_number\": 1000,\n        \"default_segment_number\": 0,\n        \"max_segment_size\": null,\n        \"memmap_threshold\": 100000,\n        \"indexing_threshold\": 1000,\n        \"flush_interval_sec\": 5,\n        \"max_optimization_threads\": 1\n      },\n      \"wal_config\": {\n        \"wal_capacity_mb\": 32,\n        \"wal_segments_ahead\": 0\n      },\n      \"quantization_config\": null\n    },\n    \"payload_schema\": {\n      \"model_key\": {\n        \"data_type\": \"keyword\",\n        \"points\": 20200\n      }\n    }\n  },\n  \"status\": \"ok\",\n  \"time\": 0.0005857\n}\n```\nWe'd expect the total memory to be ~200 MB but it is taking a whopping 11 GB!!\n\nAny help would be appreciated."
  },
  {
    "threadId": "1202116283542278145",
    "name": "Upgrade from 1.1.3 to 1.7.4",
    "messages": "Hello, \n\nMy cluster is using version 1.1.3 and I have plenty of vector documents stored in there for my app. However there are memory issues that my cluster is experiencing and trying to update collection to use disk space instead of memory does not work on such version. I'm thinking of updating to latest 1.7.4 is there any precautions I can do to make sure migration goes well and no data will be lost?"
  },
  {
    "threadId": "1202255713074958457",
    "name": "are there anyways to convert snapshot as json or csv",
    "messages": "I want to edit the qdrant snapshots from my editor, are there any ways to convert it to json or csv?"
  },
  {
    "threadId": "1202309184373129217",
    "name": "Empty responses from async client",
    "messages": "Hey, I'm migrating a project from flask to fastapi and I've switched to using the async client instead of the synchronous one. \nThe `client.search()` method seems to just return an empty array with the async client ЁЯд╖\n\nHere's my async code:\n```\nfrom qdrant_client import AsyncQdrantClient\n\nclient = AsyncQdrantClient(\n    url=qdrant_cloud_url, \n    api_key=qdrant_api_key\n)\ncollection=\"my_collection\"\n\nsearch_result = await client.search(\n        collection_name=collection,\n        query_vector=embedding, \n        limit=8,\n        score_threshold=0.78\n    )\n```\n\n`search_result` returns an empty array [] even though the exact same query with the synchronous client returned the top 8 results. Is there something obvious I'm doing wrong?"
  },
  {
    "threadId": "1201954308745330689",
    "name": "Change cluster configuration to utilize low ram",
    "messages": "Hi there, \n\nI've received email notifications about my cluster running out of memory and I wonder what are the steps to reconfigure cluster to use less memory, but also make sure that configuration process will not result in data loss?\n\nThanks"
  },
  {
    "threadId": "1202260835544870992",
    "name": "Weird scores generated by Qdrant.",
    "messages": "I've added a few sentences to my vectorDB, and I'm encountering some unusual values. Firstly, when attempting to search for similar scores to this string '.'  ( a dot), I'm getting a score of 0.78, which is unexpected. Secondly, when using models.Distance.COSINE or Distance.Dot, the same value is being returned.\n\nRegarding the sentences being stored, why should there be a high score for a dot string '.'? Can someone clarify this issue?\n\nHere's the code snippet where I create the collection:\n\n```python\nqdrant.create_collection(\n    collection_name=collection_name,\n    vectors_config=models.VectorParams(size=768,distance=models.Distance.COSINE, on_disk=True),\n    optimizers_config=models.OptimizersConfigDiff(default_segment_number=2),\n    hnsw_config=models.HnswConfigDiff(on_disk=True, m=64, ef_construct=512),\n)\n```"
  },
  {
    "threadId": "1202231764307935262",
    "name": "Issue with Qdrant Indexing: Only Partial Indexing Completed for Large Collection",
    "messages": "I have a Qdrant collection containing around 19 million points/vectors. To optimize the upload process, I initially disabled indexing by setting the **indexing_threshold** parameter to 0. After uploading the points, I attempted to enable indexing by setting **indexing_threshold** to 20000.\n\nHowever, I encountered a problem тАУ only 1.8 million vectors were successfully indexed before the optimizer status turned green. Despite multiple attempts to disable and enable indexing, the issue persists. I'm now experiencing timeouts when attempting semantic search, similar to the ones before indexing, indicating that the indexing process did not complete successfully.\n\nWhat might be causing this issue?"
  },
  {
    "threadId": "1202105352171290685",
    "name": "Huge amounts of filter clauses: Are there limitations and performance concerns?",
    "messages": "In my application, content is organized into groups, and each user can be a member of virtually any number of groups, although for the purpose of this discussion, let's set a limit of 10K groups.\n\nEach group typically contains between 1K and 50K documents, with the majority having fewer than 5K. The overall number of documents in the system is approximately 250M. Importantly, each document is associated with only one group, and the payload includes a group ID, for example: {..., \"group_id\": 12345, ...}.\n\nWhen conducting a search on behalf of a user, our goal is to retrieve documents whose vectors closely match a given vector, but exclusively from groups to which the user belongs.\n\nSo if we will blindly construct a query to obtain the necessary data, the filter section of this query could potentially contain up to 10K clauses\nFor example in \"filter\".\"should\" array:\n\"should\": [\n        { \"key\": \"group_id\", \"match\": { \"value\": 111222 } },\n        { \"key\": \"group_id\", \"match\": { \"value\": 12346 } },\n        ... (10,000 more values here) ...,\n        { \"key\": \"group_id\", \"match\": { \"value\": 32145715 } }\n ]\nOr in \"key\".\"match\".\"any\" array:\n{\n  \"key\": \"group_id\",\n  \"match\": {\n    \"any\": [\"111222\", \"12346\", ...10K more values here..., \"32145715\"]\n  }\n}\n\nMy questions are:\n1. Will this even work, and are there any limitations regarding the number of clauses?\n2. What kind of performance penalty can I expect with this method?\n3. How does performance decline as the number of values increases? Is there a linear penalty in some worst-case scenario?"
  },
  {
    "threadId": "1212804325437341726",
    "name": "Highlight vectors with specific metadata field",
    "messages": "Hi, is there a way to highlight vectors with \"model\" : \"xy\" in the metadata in red for example, and the others just normal?\n\nTalking from the dashboard vizalize mode"
  },
  {
    "threadId": "1201746306637512745",
    "name": "Can not Creating New Shard Replica from Listener Node",
    "messages": "I executed create new shard replica as this curl request\n\n- 4978556989278284 = Listener Node\n- 3205037787010283 = New Follower Node\n- http://localhost:7000 = Listener Node Base URL\n```\ncurl --location 'http://localhost:7000/collections/cluster_collection/cluster' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"replicate_shard\": {\n        \"shard_id\": 0,\n        \"from_peer_id\": 4978556989278284,\n        \"to_peer_id\": 3205037787010283\n    }\n}'\n```\n\nI got this error. \n```json\n{\n    \"status\": {\n        \"error\": \"Bad request: Shard 0 is not active on peer 4978556989278284\"\n    },\n    \"time\": 0.038553139\n}\n```\n\nI expected the listener node can be the source for the backup, including transfer its shard to other node as I did above.\n\n\nAny one can explain this? Thanks"
  },
  {
    "threadId": "1202144035398221864",
    "name": "How can I search in two vectors at the same time",
    "messages": "Hi! How can I search in two vectors at the same time, for example\n\n```python\nclient.search(\n    collection_name=COLLECTION_NAME,\n    query_vector={\n        \"text\": text_vector,\n        \"image\": image_vector\n      },\n    limit=5,\n    with_vectors=False,\n    with_payload=True,\n)\n```\n\nI did not find such a query_vector option in the documentation"
  },
  {
    "threadId": "1212272443524386837",
    "name": "How to get all available filters for a given collection?",
    "messages": "Hi,\n\nCould you tell me if there's a method to retrieve all the available filters within a collection?\n\nFor instance, as demonstrated in this example - https://qdrant.tech/documentation/concepts/filtering/#:~:text=implemented%20in%20Qdrant.-,Suppose%20we%20have%20a%20set%20of%20points%20with%20the%20following%20payload%3A,-%5B, I'm interested in getting all the possible IDs.\n\nIs this achievable using the API?"
  },
  {
    "threadId": "1201682619490902026",
    "name": "Payload has zero points after restart",
    "messages": "We had a problem over the weekend where the qdrant cluster went down. We use persistent disks, and brought the cluster back up.\n\nHowever, one of our three payloads now has zero points!\n\n```\n \"payload_schema\": {\n      \"type\": {\n        \"data_type\": \"keyword\",\n        \"points\": 65541568\n      },\n      \"asset_uuid\": {\n        \"data_type\": \"keyword\",\n        \"points\": 65541568\n      },\n      \"source\": {\n        \"data_type\": \"keyword\",\n        \"points\": 0\n      }\n    }\n```\n\nI have tried deleting and recreating the payload inxex, nothing seems to happen. I have also tried tweaking optimization settings to get it to optimize again. \n\nI cant seem to force it to do anything - the cluster is always green, and this payload always has zero points :/"
  },
  {
    "threadId": "1201552422187839488",
    "name": "How API Key works?",
    "messages": "I have installed Qdrant on my Docker Desktop following the instructions at: https://qdrant.tech/documentation/quick-start. Then, I have set an API Key using the Dashboard. However, when I click Apply, nothing happens, I'm still able to access the dashboard with any authentication and I can use the Qdrant API without any API Key. I can also pass a random key and get no error. \n\nAm I missing something in the configuration?"
  },
  {
    "threadId": "1200283346853449798",
    "name": "Custom Ingestion Pipeline with Llama Index Error - Format error in JSON body: data did not match any",
    "messages": "I got this error after building a custom image ingestion pipeline with Llama Index. My image node is embedded with `\"ViT-L/14\"` so I'm not quite sure I understand what's going on. I am essentally adding metadata to the image and then trying to insert it into the index and getting `Raw response content: b'{\"status\":{\"error\":\"Format error in JSON body: data did not match any variant of untagged enum PointInsertOperations\"},\"time\":0.0}'`\n\nMy embedded image node I was trying to insert is attached as well as it was too big lmao\n\nthanks in advance!"
  },
  {
    "threadId": "1200603870666166402",
    "name": "list metadata type",
    "messages": "For each doc, if it has metadata type field of type list, how to best represent it in qdrant?\nIn our case, each doc can be shared with a list of users.\nDo I have to duplicate the vectors for each user in order to benefit from indexing on user_id? I see payload type does not have a list type yet."
  },
  {
    "threadId": "1201434154022211728",
    "name": "Indexing Required for Nested Filter",
    "messages": "In the provided documentation example:\n\n```[\n  {\n    \"id\": 1,\n    \"dinosaur\": \"t-rex\",\n    \"diet\": [\n      { \"food\": \"leaves\", \"likes\": false},\n      { \"food\": \"meat\", \"likes\": true}\n    ]\n  },\n  {\n    \"id\": 2,\n    \"dinosaur\": \"diplodocus\",\n    \"diet\": [\n      { \"food\": \"leaves\", \"likes\": true},\n      { \"food\": \"meat\", \"likes\": false}\n    ]\n  }\n]```\n\nDo I need to create an index for ```diet[].food``` and ```diet[].likes```, or should I create any other index? \n\nI am facing a similar use case and utilising a nested object filter, but the response speed is slow."
  },
  {
    "threadId": "1201430622716035082",
    "name": "adding a key to a json payload",
    "messages": "is it possible to add a new key to an existing payload with json field?\n\nfor example payload is\n```json\n{\"name\": \"abcd\"\n\"data\" : {\"place\" :\"A\" , \"time\": 123}\n}\n```\nto \n```json\n{\"name\": \"abcd\"\n\"data\" : {\"place\" :\"A\" , \"time\": 123, \"value\" : \"red\"}\n}\n```"
  },
  {
    "threadId": "1201236871099990026",
    "name": "AWS Instance and Disk Suggestions",
    "messages": "Hello, Currently, we employ the r5.8xlarge instance with 32 CPUs and 256GB RAM, utilizing EBS Volume Mount with a low IOPS of 5000. \n\nI'm seeking recommendations for the optimal instance and disk volume, irrespective of cloud provider, that can deliver high IOPS speed."
  },
  {
    "threadId": "1199505911513886720",
    "name": "OOMKill during indexing",
    "messages": "We are getting OOMKills towards the end of indexing (running on kubernetes, 12G memory limit per node, 8 nodes total), which is causing indexing to never truly finish.\n\nWe are storing on disk, with binary quantization in memory. Each node has 16 gigs of memory, so we have more than enough memory for search.\n\nWhat knobs are available to us for tuning memory consumption during indexing? Number of threads?"
  },
  {
    "threadId": "1201141303212253275",
    "name": "Payload Error",
    "messages": "Encountering this error:\nUnexpectedResponse: Unexpected Response: 400 (Bad Request)\nRaw response content:\nb'{\"status\":{\"error\":\"Payload error: JSON payload (46866880 bytes) is larger than allowed (limit: 33554432 bytes).\"},\"time\":0.0}'\n\nhelp me solve this."
  },
  {
    "threadId": "1185237752091000952",
    "name": "Timeout error",
    "messages": "Any ideas on how to troubleshoot this error? I am running a single instance of qdrant in docker with one collection and the status is green.\n`qdrant_1  | 2023-12-15T15:08:03.517036Z ERROR qdrant::tonic::logging: gRPC /qdrant.Points/SearchBatch unexpectedly failed with Internal error \"Service internal error: 1 of 1 read operations failed:\\n  Timeout error: Operation 'Search' timed out after 60 seconds\" 60.001907`"
  },
  {
    "threadId": "1200578115123028060",
    "name": "multiple vectors per doc",
    "messages": "Hi does qdrant support multiple vectors per doc? e.g. I have a doc, and chunk it to multiple pieces, they can share the same metadata of the doc without having to duplicate the data."
  },
  {
    "threadId": "1200535072332324914",
    "name": "Search operation increases RAM usage",
    "messages": "We have a qdrant deployment on EKS Cluster having 2 pods. Each having 55 GB RAM limit. \n\nAfter inserting 50M vectors with on-ram quantization config, the RAM usage on each pod is 27 and 29GB.  However, we noticed that querying  vectors (with search op) increases RAM usage persistently. \n\nAfter running 2000 search operations synchronously with random vectors, it seems like the RAM usage increased by 2GB and did not go back down-  which is too much. Could it be a memory leak or a default behavior?\n\nWould appreciate if someone can guide on how search mechanism works. I read on the docs that there is some caching on search for optimization purpose, but this is not sustainable as pods will eventually run out of memory after few-thousand queries.  \n\nLooking forward to a positive response.\n\nP.S: No more vectors have been added after the inital 50M."
  },
  {
    "threadId": "1200373992012394556",
    "name": "Payload Significantly Increases Response Time",
    "messages": "Hello,\n\nI'm currently using batch search API. This is my code:\n\n```python\nsearch_requests = [\n            rest.SearchRequest(\n                vector=vector,\n                filter=rest.Filter(\n                    must=[\n                        rest.FieldCondition(\n                            key=\"aID\",\n                            match=rest.MatchAny(any=filter_list),\n                        ),\n                    ],\n                ),\n                limit=limit,\n                score_threshold=score_threshold_dict[collection_name],\n                with_payload=rest.PayloadSelectorInclude(include=[\"aID\"]),\n                params=rest.SearchParams(**search_params.get(\"search_params\", {})),\n            )\n            for vector in vectors\n        ]\n\n        results = client.search_batch(\n            collection_name=collection_name, requests=search_requests\n        )\n```\n\nWithout including payload, my response time is ~9ms. With payload, it shoots upto ~300+ms.\n\nIs there something I'm doing wrong or something I can optimize?"
  },
  {
    "threadId": "1197820789454553209",
    "name": "Question on Resource Requirements",
    "messages": "We have 4 collections with over 4 million points in total. \n\n1. What would be the required memory if we are using the default config?\n\nWe had 12GB initially but the pod kept crashing due to OOM issue so we changed the collections config to\n\n\"vectors\": {\"\": {\"on_disk\": true}},\n  \"params\": {\n    \"on_disk_payload\": true\n  },\n \"hnsw_config\": {\"m\": 64,\n    \"ef_construct\": 512,\n    \"on_disk\": true}\n\n2. The collections' status is green now but the memory usage is hovering over 11.8GB still, is this excepted?"
  },
  {
    "threadId": "1200013420402053180",
    "name": "AppRunner",
    "messages": "Is it possible to use Qdrant on AWS AppRunner? In AppRunner, it is only possible to open one port, so I have done it with 6334 for the gRPC connection to work. I have tried in all possible ways, but I achieve connection from python. If I expose port 6333, I have access to the Dashboard. Does anyone have experience?"
  },
  {
    "threadId": "1200195768561184798",
    "name": "Optimization - max_indexing_threads and max_optimization_threads",
    "messages": "Hi, How does auto selection works for max_indexing_threads?\n\n```max_indexing_threads    \ninteger or null <uint> >= 0\nNumber of parallel threads used for background index building. If 0 - auto selection.```\n\nCurrently, in my collection, I've configured \"max_indexing_threads\" as 1000 and \"max_optimization_threads\" as 1. During collection optimization (yellow state), there is a noticeable spike in CPU and RAM usage. Could you please explain how modifying these configuration values will impact the utilization of RAM and CPU?"
  },
  {
    "threadId": "1200211399956234240",
    "name": "Do Qdrant charges for the requests?",
    "messages": "I want to know, if the qdrant cloud charges for request or not? If it do, then how much?"
  },
  {
    "threadId": "1163409654945153085",
    "name": "AI chatbot for your documentation",
    "messages": "Hey! My team is building an AI chatbot that can help people understand your developer documentation more easily and answer usersтАЩ questions. Would you be interested in checking out a demo?"
  },
  {
    "threadId": "1199824267022442597",
    "name": "Is backup the same as a snapshot?",
    "messages": "Can I create a snapshot from the Cloud console? I only found the backup creation option. If it's the same, can I download it?\nAnother thing is not clear - what does it mean, if I create it, will it be stored at your servers forever?\n\n```\nCreate a Backup\nAre you sure you want to create a backup for ''?\n\nBased on the selected cluster's cloud provider and region, the backup storage price is 0.13908 per GB per month (estimating 30.5 days a month).\n```\n\nThanks"
  },
  {
    "threadId": "1200049097005076490",
    "name": "I can't Delete Points by Metadata Filter",
    "messages": "Im using free tier in clouds\n\nI have created a COLLECTION with meta data...\nBut\nI can't \nDelete POINTS by using metadata filter \n\n\nSomething like this... \nWhat is wrong\n\n\n\n\n\n\n\n\ncurl -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer YOUR_API_KEY\" -d '{\n  \"filter\": {\n    \"should\": [\n      {\n        \"key\": \"topic_cct_id\",\n        \"match\": {\n          \"value\": \"488\"\n        }\n      }\n    ]\n  }\n}' \"https://fe207376a-56e7-4fe111c-a771-97d0jdjdjdbcbaa.europe-west3-0.gcp.cloud.qdrant.io:6333/collections/usa-laws/points/delete\""
  },
  {
    "threadId": "1197535126163312790",
    "name": "Combining full text search and semantic search in one collection",
    "messages": "Hello. I have a question about full text search. I created a collection with configuration:\nvectors_config = VectorParams(size=self.vector_dimensions, distance=Distance.COSINE)\n\nSemantic search works for me, but when I try to do a full text search it gives me an error: QdrantClient.search() missing 1 required positional argument: 'query_vector'\".\nI have 2 classes, one class implements semantic search, and the other implements full-text search, but both classes belong to the same collection. Will it work or do you need to create a separate collection with its own parameters for each type of search?"
  },
  {
    "threadId": "1199695741694791710",
    "name": "Time-out on search query:",
    "messages": "We have deployed qdrant  on EKS Cluster, which has 3 pods each of 27 GB RAM limit, and 300 GB EBS Storage. \n\nNumber of collection: 2\n1st collection vector count: 83 million\n2nd collection vector count: 2 million\n\nRAM limit for each pod: 27GB\nCurrent RAM Usage:  8GB RAM each pod\n\nCPU limit for each pod: 7 vCPU\ncurrent CPU Utilization: 200m each pod\n\nEBS on each pod: 300GB\nUtilized EBS Size: Around 120 GB for each pod attached EBS\n\nBut still unable to get results from search query result that we perform via python client and get a time out most of the time. and occasionally get the response and takes 8-12 sec.\n\nWill be glad if someone can helpout!!\n\nFurther sharing qdrant cluster config here as well"
  },
  {
    "threadId": "1199664452140798063",
    "name": "Rate Limit with Qdrant Cloud?",
    "messages": "I am currently using Qdrant Cloud with this setting\n\n16GB DISK\n 4GB RAM\n1 vCPU\n\nAnd I was wondering how many requests can I make to it per minute? I am making around 10 concurrent requests to it."
  },
  {
    "threadId": "1192565273174224906",
    "name": "Inquiry about Transitioning to Hybrid SaaS from Qdrant Cloud",
    "messages": "Dear Qdrant Support Team,\n\nI recently discovered that starting this month, you are offering support for Hybrid SaaS in a closed-beta format. I am very interested in transitioning from our current Qdrant Cloud setup to this new Hybrid SaaS model.\n\nCould you please inform me about the possibility of receiving support for this transition? Additionally, is there a need for me to sign up for a waitlist, or are there any specific steps I should follow to begin this process?\n\nI would like to take this opportunity to express my gratitude for the exceptional support and product you provide.\n\nLooking forward to your guidance.\n\nBest regards,"
  },
  {
    "threadId": "1199822562042056795",
    "name": "Rename Alias API?",
    "messages": "Hi all,\nI have a (hopefully) simple question regarding the alias renaming API. I was browsing the API docs and noticed there were 3 possible update operations specified under the POST /collections/aliases endpoint. After looking through the documentation, I found the body format for the `create_alias` and `delete_alias` actions, but not for the `rename_alias`. I assume it looks something like `actions: [ { rename_alias: {collection_name: ex, old_alias_name: old, new_alias_name: new } } ]` \nIf someone could confirm what this request body should look like, I would very much appreciate the clarification. If this information does in fact exist in the api/concept docs feel free to point me in the right direction!\nCheers"
  },
  {
    "threadId": "1199528447836434462",
    "name": "I'm getting 400 from upsert?",
    "messages": "It says the json is invalid but it it isn't. I tried all variations.\nIt occurs in all cases even using CURL command from the swagger page. https://ui.qdrant.tech/#/points/upsert_points \n\n'http://localhost:6333/collections/XXXXXXX/points?wait=true' \\\n\n-H 'accept: application/json'\n-H 'Content-Type: application/json'\n-d '{\n\"batch\": {\n\"ids\": [\n0,\n\"3fa85f64-5717-4562-b3fc-2c963f66afa6\"\n],\n\"vectors\": [\n[\n0, 1, 2, 3\n]\n],\n\"payloads\": [\n{\n\"additionalProp1\": {}\n},\n\"string\"\n]\n},\n\"shard_key\": \"string\"\n}'\n{\"status\":{\"error\":\"Format error in JSON body: data did not match any variant of untagged enum PointInsertOperations\"},\"time\":0.\n\nqdrant server built on macos 13.1 apple silicon. I tried both todays' main and tags/v1.7.3"
  },
  {
    "threadId": "1199706201269735494",
    "name": "Return Query ID When Searching?",
    "messages": "Hi, is it possibly to return the ID of the query vector in the result of `search`? \nWith batch querying we must trust that the returned results are in the same order as the passed IDs. Can we trust this?\nRegards\nSteve"
  },
  {
    "threadId": "1199635888481763418",
    "name": "pricing",
    "messages": "title: qdrant cloud pricing question\n\n- -in qdrant pricing page\nnumber of vectors: 500000\nvector dimension: 1536\n\nhttps://benchmark.vectorview.ai/vectordbs.html\nIn this document, it says qdrant Pricing (50k vectors @1536) - $9.\nIt is very different from the qdrant pricing page\n\nDid I enter the qdrant pricing calculator values correctly?"
  },
  {
    "threadId": "1199533293637476442",
    "name": "Manually Disconnect?",
    "messages": "I am currently using Async Qdrant Client\n```\nclient = AsyncQdrantClient(url=\"http://localhost:6333\")\n```\n\nI am instantiating this on every API call and was wondering if I need to manually disconnect from it once I'm done with my querying of vectors."
  },
  {
    "threadId": "1210282364190851112",
    "name": "points/scroll returns a 403 in python client, search call works fine",
    "messages": "I am using the qdrant python client 1.6.9.\n\nI have the following code:\n`qdrant_client   = QdrantClient(QDRANT_URL, port=QDRANT_PORT, api_key=QDRANT_API_KEY)\ncollection          = qdrant_client.get_collection(collection_name=qdrant_index)\npoints                 = qdrant_client.scroll(collection_name=qdrant_index, scroll_filter=filter, limit=1000, with_vectors=True)`\n\nThe problem is that the call to get_collection works fine, as does a call to search, but for some reason the call to scroll the points returns a 403.\n\nWhen I use the same API KEY in the console, the call to `POST collections/{collection}/points/scroll `works just fine.  \n\nSo I switched to using Python requests, and it also got back a 403 when calling the endpoint directly. \n\nAre you not allowed to scroll via the API?  Why is this?  Is there a way around this?"
  },
  {
    "threadId": "1199309395201380352",
    "name": "Upgrading and Moving to new cluster",
    "messages": "We created a new cluster of qdant and our old one was a single instance running on v1.4.0 and the new one is on v1.7.1\n\nwe want to move the data from the old one to new cluster. What would be the best way to do it?  I tried creating a snapshot and importing it but it did not work."
  },
  {
    "threadId": "1199300275257753600",
    "name": "Recommend api asking for vector name",
    "messages": "Dears,\nIm trying to run this query:\n\n`POST /collections/posts/points/recommend\n{\n  \"positive\": [4692733, 5022263],\n  \"strategy\": \"average_vector\",\n  \"limit\": 3\n}`\nbut im getting this error:\n`{\n  \"error\": \"Wrong input: Not existing vector name error: \"\n}`\n\nI read the docs, no mention for vector name here. I tried to add the vector name but I still get same issue.\nIm using v1.7.3 and here is my config\n`{\n    \"params\":{\n    \"vectors\":{\n    \"titleAndBodyEmbeddingsByOpenAI\":{\n    \"size\":1536\n    \"distance\":\"Cosine\"\n    \"quantization_config\":{\n    \"binary\":{\n    \"always_ram\":true\n    }\n    }\n    }\n    }\n    \"shard_number\":1\n    \"replication_factor\":1\n    \"write_consistency_factor\":1\n    \"on_disk_payload\":true\n    }\n    \"hnsw_config\":{\n    \"m\":16\n    \"ef_construct\":100\n    \"full_scan_threshold\":10000\n    \"max_indexing_threads\":0\n    \"on_disk\":false\n    }\n    \"optimizer_config\":{\n    \"deleted_threshold\":0.2\n    \"vacuum_min_vector_number\":1000\n    \"default_segment_number\":0\n    \"max_segment_size\":\n    NULL\n    \"memmap_threshold\":\n    NULL\n    \"indexing_threshold\":20000\n    \"flush_interval_sec\":5\n    \"max_optimization_threads\":1\n    }\n    \"wal_config\":{\n    \"wal_capacity_mb\":32\n    \"wal_segments_ahead\":0\n    }\n    \"quantization_config\":\n    NULL\n    }`"
  },
  {
    "threadId": "1199299414670442536",
    "name": "Snapshot when status is yellow vs green",
    "messages": "Hi there!\n\nMy Qdrant collection undergoes optimization every few hours, and I intend to capture snapshots on a daily basis.\n\nCould you please explain how Qdrant generates a snapshot when the collection status is yellow compared to when it's green? If I decide to create a snapshot during the yellow status, are there any potential issues I should be aware of?"
  },
  {
    "threadId": "1199060425027891330",
    "name": "Using NAS device for Qdrant Storage",
    "messages": "I've been running qdrant locally no issue on my M2 Mac when I use my local file system but I've been trying to set the qdrant storage folder path to my NAS and run into errors like the following:\n\n`qdrant: 2024-01-22T18:29:36.420972Z ERROR qdrant::tonic::logging: gRPC /qdrant.Collections/Create unexpectedly failed with Internal error \"Service internal error: RocksDB open error: IO error: While lock file: /Volumes/Public/vector_db/storage/collections/QDRANT_DEFAULT_COLLECTION/0/segments/8504c83e-3595-48dd-83ba-cd1705b5341e/LOCK: Operation not supported\" 0.211594 \nError 2:`\n\nIs this some sort of permissions issue?"
  },
  {
    "threadId": "1198963379688177695",
    "name": "Snapshot Creation is Slow",
    "messages": "Hi! I am trying a create snapshot of the collection, it's talking lot of time, right now I have around 25M+ points. \n\nI am using qdrant 1.7.3 using docker, and making CURL request.\nWhen I have called the create snapshot API, all other apis are very slow (search, count, collection info). \n\nRAM: 256 GB\nCPUv32\n\nAny suggestions to fix this issue, I want to create snapshot every day or every 3 days and don't want the search functionality to be slow while snapshot creation is going on."
  },
  {
    "threadId": "1198343697868259338",
    "name": "how can i implement a healthcheck in docker compose?",
    "messages": "I want to implement a healthcheck in docker compose but the base image does not have curl.\nideally i would just do this:\n```yaml\nqdrant-database:\n  image: qdrant/qdrant:v1.7.0\n  container_name: qdrant-database\n  environment:\n    - QDRANT__SERVICE__API_KEY=${QDRANT_API_KEY}\n  healthcheck:\n    test: [\"CMD\", \"curl\", \"-f\", \"http://qdrant-database:6333/healthz\"]\n    interval: 10s\n    timeout: 20s  # Increased timeout\n    retries: 5\n  depends_on:\n    condition: service_healthy\n  # ports:\n  #   - 6333:6333\n  #   - 6334:6334\n  volumes:\n    - qdrant_data:/qdrant/storage\n```"
  },
  {
    "threadId": "1198919621043048448",
    "name": "Cluster rebalancing",
    "messages": "Hi! We have a following case: qdrant cluster with two pods, a collection with shard_number=12. Those two pods get filled eventually. We add a third pod to the cluster. It does not rebalace, obviously. So my question is, do I understand correctly, that in \"move_shard\" method i can manually specify a third node as a receiver? Or, more broadly, what is the best manual rebalancing approach in a situation like ours?"
  },
  {
    "threadId": "1198703928250994728",
    "name": "Chaining Named Vector Searches",
    "messages": "I'm interested in searching by more than one named vector at the same time- for example, Document Embedding and Fragment Embedding in one query/filter/search.  Is this possible with Qdrant?  tks!"
  },
  {
    "threadId": "1198906530532167691",
    "name": "How to avoid running out of GPU memory using Quaterion - NLP Training",
    "messages": "I have trouble running my training script on a much bigger dataset than usual. Also this is the first time I am training via GPU. My training script is very similar to the nlp tutorial at https://quaterion.qdrant.tech/tutorials/nlp_tutorial.\n\nI don't quite understand how I can run out of GPU memory at this point. If all calculations are done batch-wise then something like this should not happen right? What could I try to avoid this?\n\nError:\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 7.75 GiB total capacity; 5.29 GiB already allocated; 341.25 MiB free; 7.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\nPredicting DataLoader 0:  43%|тЦИтЦИтЦИтЦИтЦО     | 5119/11889 [22:05<29:12,  3.86it/s]\n\nFull stacktrace in attachment."
  },
  {
    "threadId": "1198573656557764608",
    "name": "Error while implementing multitenancy!",
    "messages": "Hello, I am trying to implement multitenancy feature using the ts SDK, but it;s giving error and I am comepletely clueless about the error.\n\nThe code -  https://gist.github.com/DeadmanAbir/dda6661677f588bfc2ad134bf67698db\n\nwhile the error is - \n\n\nplease have a look and help to debug onwards..\n\n<@844295650400534599>"
  },
  {
    "threadId": "1198388713928020009",
    "name": "How to obtain all the vectors from collection?",
    "messages": "I just want to read all the vectors from a collection. This code is not working:\n\n```\n    dim = 1536 \n    embedded_query = [0] * dim \n    qdrant_client = get_qrant_client()\n\n    data = qdrant_client.search(\n        collection_name=coll_id,\n        query_vector=embedded_query,\n    )\n```\nI have the ValidationError which has a list of errors like \n\n```\nValidationError(model='SearchRequest', errors=[{'loc': ('vector',), 'msg': 'value is not a valid dict', 'type': 'type_error.dict'}, {'loc': ('vector', 0), 'msg': 'value is not a valid float', 'type': 'type_error.float'}, {'loc': ('vector', 1), 'msg': 'value is not a valid float', 'type': 'type_error.float'}, {'loc': ('vector', 2), 'msg': 'value is not a valid float', 'type': 'type_error.float'}, {'loc': ('vector', 3), 'msg': 'value is not a valid float', 'type': 'type_error.float'}, {'loc': ('vector', 4), 'msg': 'value is not a valid float', 'type': 'type_error.float'}, {'loc': ('vector', 5), 'msg': 'value is not a valid float', 'type': 'type_error.float'}, {'loc': ('vector', 6), 'msg': 'value is not a valid float', 'type': 'type_error.float'}, {'loc': ('vector', 7), 'msg': 'value is not a valid float', 'type': 'type_error.float'}, {'loc': ('vector', 8), 'msg': 'value is not a valid float', 'type': 'type_error.float'}, {'loc': ('vector', 9), 'msg': 'value is not a valid float', 'type': 'type_error.float'}, {'loc': ('vector', 10), 'msg': 'value is not a valid float', 'type': 'type_error.float'}, {'loc': ('vector', 11), 'msg': 'value is not a valid float', 'type': 'type_error.float'}, {'loc': ('vector', 12), 'msg': 'value is ...etc\n```"
  },
  {
    "threadId": "1183857994035896350",
    "name": "Upgrading helm cluster from 1.6.1 to 1.7.0 erroring",
    "messages": "Hi, I'm trying to upgrade a cluster of 4 from 1.7.0 from 1.6.1, and I seem to be getting an error in internal communication:\n\n```\n2023-12-11T19:31:53.142197Z ERROR qdrant::common::health: GetCommitIndex request failed: Error in closure supplied to transport channel pool: status: Unimplemented, message: \"\", details: [], metadata: MetadataMap { headers: {\"date\": \"Mon, 11 Dec 2023 19:31:53 GMT\", \"content-type\": \"application/grpc\", \"content-length\": \"0\"} }    \n2023-12-11T19:31:53.142321Z ERROR qdrant::common::health: GetCommitIndex request failed: Error in closure supplied to transport channel pool: status: Unimplemented, message: \"\", details: [], metadata: MetadataMap { headers: {\"date\": \"Mon, 11 Dec 2023 19:31:52 GMT\", \"content-type\": \"application/grpc\", \"content-length\": \"0\"} }    \n2023-12-11T19:31:53.142494Z ERROR qdrant::common::health: GetCommitIndex request failed: Error in closure supplied to transport channel pool: status: Unimplemented, message: \"\", details: [], metadata: MetadataMap { headers: {\"date\": \"Mon, 11 Dec 2023 19:31:52 GMT\", \"content-type\": \"application/grpc\", \"content-length\": \"0\"} }    \n2023-12-11T19:31:53.142504Z  WARN qdrant::common::health: Not enough cluster nodes responded to GetConsensusCommit request: required 2,\n                 responded 0 out of 4    \n```\n\nIt seems like perhaps there's a backwards incompatible change for a rolling upgrade? Or am I doing something wrong. For what its worth, I'm using the helm charts, so 0.6.1 to 0.7.0."
  },
  {
    "threadId": "1197855015386226688",
    "name": "Sementic Search",
    "messages": "Does Qdrant support sementic search ?"
  },
  {
    "threadId": "1197821564943609886",
    "name": "Sparse vector using qdrant client in colab?",
    "messages": "Am doing a test in colab and using the in memory option of the qdrant client for a sparse vector search. I get an error when I try to create the index like so: client.create_collection(\n    collection_name=\"sparse_test\",\n    sparse_vectors_config={\n        \"text\": models.SparseVectorParams(),\n    },\n)\n\nThe error says TypeError: QdrantClient.create_collection() missing 1 required positional argument: 'vectors_config'"
  },
  {
    "threadId": "1197550414963421236",
    "name": "Indexing is not working",
    "messages": "I have left my computer on over night and no indexes have been created. Am I doing anything wrong on the collection info settings?\n\n```json\n{\n  \"result\": {\n    \"status\": \"green\",\n    \"optimizer_status\": \"ok\",\n    \"vectors_count\": 22205,\n    \"indexed_vectors_count\": 0,\n    \"points_count\": 22205,\n    \"segments_count\": 8,\n    \"config\": {\n      \"params\": {\n        \"vectors\": {\n          \"size\": 384,\n          \"distance\": \"Cosine\"\n        },\n        \"shard_number\": 1,\n        \"replication_factor\": 1,\n        \"write_consistency_factor\": 1,\n        \"on_disk_payload\": true\n      },\n      \"hnsw_config\": {\n        \"m\": 16,\n        \"ef_construct\": 100,\n        \"full_scan_threshold\": 10000,\n        \"max_indexing_threads\": 0,\n        \"on_disk\": false\n      },\n      \"optimizer_config\": {\n        \"deleted_threshold\": 0.2,\n        \"vacuum_min_vector_number\": 1000,\n        \"default_segment_number\": 0,\n        \"max_segment_size\": null,\n        \"memmap_threshold\": null,\n        \"indexing_threshold\": 20000,\n        \"flush_interval_sec\": 5,\n        \"max_optimization_threads\": 1\n      },\n      \"wal_config\": {\n        \"wal_capacity_mb\": 32,\n        \"wal_segments_ahead\": 0\n      },\n      \"quantization_config\": null\n    },\n    \"payload_schema\": {}\n  },\n  \"status\": \"ok\",\n  \"time\": 0.000097637\n}\n```"
  },
  {
    "threadId": "1197111384635355187",
    "name": "Is there a way to search two indexes at the same time?",
    "messages": "Instead of search Index A and then Index B, is there a way to get a combined set of topN results?"
  },
  {
    "threadId": "1197471580721250406",
    "name": "Manage Vectorized Data",
    "messages": "I'm using python write application, I have 100 Chunks of text, that I have uploaded to the Vectore Space, I have it for example in a CSV with ID for each chunk, lets say chunk ID 33 Changes abit, how do I find this chunk in vectore space, delete it and replace with the new chunk ?\nInstead of vectorizing the whole CSV again and again. \nIm trying to achieve that just using python libraries only, Thanks"
  },
  {
    "threadId": "1197246567405596672",
    "name": "I am trying to restore several snapshots in a new cluster",
    "messages": "Following the tutorial:\nhttps://qdrant.tech/documentation/tutorials/create-snapshot/\n\nthe Downloading of snapshots works as expected, but the upload always fails with ` Waiting for consensus operation commit failed. Timeout set at: 10 seconds`\n\nThe snapshot files are in the size range of 60Mb - 140Mb (so I think they are not that big)\n\nThe cluster has been setup through the helm chart"
  },
  {
    "threadId": "1189281080822464562",
    "name": "TypeScript Build Error",
    "messages": "I'm getting the following error when building my typescript project:\n\n```node_modules/@qdrant/js-client-rest/dist/types/api-client.d.ts:4:59 - error TS1109: Expression expected.\n\n4 export type Client = ReturnType<typeof Fetcher.for<paths>>;```\n\nThere's a github issue for this as well: https://github.com/qdrant/qdrant-js/issues/48\n\nBut has anyone found a workaround for this?"
  },
  {
    "threadId": "1180481497707315260",
    "name": "Consensus operation timeout issue",
    "messages": "Due to message character limit, I opened this issue in qdrant-helm repo: https://github.com/qdrant/qdrant-helm/issues/100\n\nSummary: When the number of replicas are increased in distributed deployment, new collections cannot be created."
  },
  {
    "threadId": "1197191995471429733",
    "name": "Hello, I have an issue with method upload_collection.",
    "messages": "I have 40+ files with json lines where is text in form line=word with context as dict extracted from PDFs, and I upload each file processed by FastText to my collection with upload_collection. At the end after uploading all files to collection, I am not able to search files from first files. It looks like they dissappeared. \nIs there any problem with my setting or where is the issue? I also used filters to filter a specific result (file), but there is nothing. Could you please give me some advice how to solv it? Thanks"
  },
  {
    "threadId": "1197183293288878091",
    "name": "Azure Container app deplyment",
    "messages": "Hi!\n\nI have created an azure container app ande deployed qdrant 1.7.3 with public ingress and api key for protection. When I access it from the browser, everything works ok but when I try to connect to it through python client I get the error:\n\nqdrant_client.http.exceptions.ResponseHandlingException: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1002)\n\nDoes anybody have any recomandations about this issue?"
  },
  {
    "threadId": "1196881028736884746",
    "name": "Very slow indexing when collection has close to 50 million vectors",
    "messages": "Hello, we were trying to test the performance impact of having a large number of vectors in a single collection. Our vectors have 1536 dimensions. We store some metadata with each vector but the size is negligible. \n\nWe started pushing batches of 10 million vectors and noted how long it took for the cluster to finish indexing them all. Ingestion time is constant at around 2.5 hours for all batches. As for indexing, the first 3 batches took around 5 hours to index, the 4th took about 7 hours but the last batch of 10 million is taken over a day to index. Is the size of our collection large enough to cause this behavior? Or could we have a different problem in our cluster configuration?\n\nSome details about our cluster. We have 3 nodes running Qdrant 1.7.3 in AWS. We have configured it with the option to use disk storage and not just ram. Each node has 16GB of RAM and 1TB of disk storage.  We know we have plenty of disk space. The RAM usage is hovering at around 15GB for all 3 nodes.\n\nThanks in advance"
  },
  {
    "threadId": "1196482337505017916",
    "name": "UserWarning: `embeddings` should be an instance of `Embeddings`.Using `embeddings` as `embedding_fun",
    "messages": "I'm getting the following error when creating embeddings with langchain.\n\nUserWarning: `embeddings` should be an instance of `Embeddings`.Using `embeddings` as `embedding_function` which is deprecated\n\n\nI tried importing  OpenAIEmbeddings like \nfrom langchain_openai.embeddings import OpenAIEmbeddings\nand\nfrom langchain_openai import OpenAIEmbeddings\n\nI'm generating it like this. \n\nembeddings = OpenAIEmbeddings(openai_api_key=OPENAI_KEY)\n\n    vector_store = Qdrant(\n        client=client,\n        collection_name=collection_id,\n        embeddings=embeddings,\n    )\n\nCould you help me please fix this issue?"
  },
  {
    "threadId": "1197070028579799040",
    "name": "Qdrant docker container restart: Does it has some rollback or replay?",
    "messages": "Does Qdrant retrieve points and starts indexing them on container restarts when it has crashed?\nTo elaborate:\nI initiated indexing of some 143k points but due to RAM issues qdrant kept on crashing, however the client upsert method returned server disconnected responses for around 100k points.\nHowever when I was restarting the containers I could see in the docker logs that indexing started to happen automatically that is POST and PUT call on the same collection name.\nCan you help me explain how could thiss be possible? \nSince I stored all those 100k points as exception I should not be seeing those 100k points in the database but I was able to see them later."
  },
  {
    "threadId": "1195301520728666193",
    "name": "From 1M to 10M vectors",
    "messages": "Hello,\n\nI have been using Qdrant for 4 months now without any problems, and it works really well for what my company needs.\n\nCurrently, I am on a machine with 16 GB of RAM and 2 vCPUs, and I have about 1.4 million OpenAI embedding vectors of size 1536 with associated metadata.\n\nI would like to scale the database to 10 million vectors, but I feel that I have reached the limit of my current configuration because even when I increase the available RAM, it doesn't allow me to scale as much as I would like.\n\nWhat would be the solutions to enable me to scale up to 10 (or more?) million vectors?"
  },
  {
    "threadId": "1196837974034358437",
    "name": "New user can not switch to my account",
    "messages": "I invited a new user to my account. When she wants to change to my account she always get the following error. Who can help us with that? (mainly she needs right sto change the payment informations)"
  },
  {
    "threadId": "1196834167405682818",
    "name": "startswith() MatchText filtering?",
    "messages": "I wanted to confirm this condition does not yet exist in Qdrant? If that's the case, has anyone found a nice workaround? The only solution I can think of is matching on an array of (0, n) substrings for the target result, but I wanted to see if there's a better approach here. Thanks!"
  },
  {
    "threadId": "1196804218854588477",
    "name": "Is there a way to achieve what $elemMatch does in MongoDB when filtering payloads?",
    "messages": "We would like to filter our vectors according to multiple fields in a nested object. An example payload could look like the following:\n\n``` \n{\n  \"cities\": [\n    {\n      \"name\": \"Berlin\",\n      \"color\": \"red\"\n    },\n    {\n      \"name\": \"London\",\n      \"color\": \"blue\"\n    }\n  ]\n}\n```\n\nAn example query would be to try to find cities with `\"name\": \"Berlin\"` **AND** `\"color\": \"blue\"` which should filter OUT the given example above.\n\nWe have tried the following query:\n```\nPOST collections/test/points/scroll\n{\n  \"limit\": 10,\n  \"filter\": {\n    \"must\": [\n      {\n        \"key\": \"cities[].name\",\n        \"match\": {\n          \"value\": \"Berlin\"\n        }\n      },\n      {\n        \"key\": \"cities[].color\",\n        \"match\": {\n          \"value\": \"blue\"\n        }\n      }\n    ]\n  }\n}\n```\n\nwhich does not filter out this point due to both conditions matching different sub-elements.\n\nWe have also tried nested `must` conditions, but could not achieve what we wanted.\n\nI would appreciate some help ЁЯЩВ"
  },
  {
    "threadId": "1196696606012092459",
    "name": "Error when create new collection",
    "messages": "Hi teamя╝М I get below error msgя╝М could you give me some suggestionя╝Я thanks in advance~\nQdrant Versionя╝Ъ1.6.1\nqdrant-client versionя╝Ъ1.6.4\n\nerror messageя╝Ъ\nUnexpected Response: 400 (Bad Request)\nRaw response content:\nb'{\"status\":{\"error\":\"Deserialize error in query parameters: invalid digit found in string\"},\"time\":0.0}'"
  },
  {
    "threadId": "1196774673690656829",
    "name": "should filter with AND inside",
    "messages": "Any idea how I can do if (city=London & color=Red) OR (city=Paris & color =green)?\n\n```\nshould=[\n            models.FieldCondition(\n                key=\"city\",\n                match=models.MatchValue(value=\"London\"),\n            ),\n            models.FieldCondition(\n                key=\"color\",\n                match=models.MatchValue(value=\"red\"),\n            ),\n        ]\nOR\nshould=[\n            models.FieldCondition(\n                key=\"city\",\n                match=models.MatchValue(value=\"Paris\"),\n            ),\n            models.FieldCondition(\n                key=\"color\",\n                match=models.MatchValue(value=\"green\"),\n            ),\n        ]\n\n```"
  },
  {
    "threadId": "1194930260844757082",
    "name": "Does Qdrant automatically upsert on the `id` key?",
    "messages": "Whenever I run my ingestion pipeline, in order to avoid the same Vector Point being duplicated, I just need to make sure the `id` key is fixed correct?"
  },
  {
    "threadId": "1196520116360315022",
    "name": "thread safety - using the Python sdk",
    "messages": "In case I have several threads writing to the db, is each using a transaction or do I have to take care for the locking? I would like to prevent race conditions.\n\nthx"
  },
  {
    "threadId": "1195784612606115991",
    "name": "Sticky recommendation",
    "messages": "Hi, I'm experiencing a very strange search behavior. I  stored about 5M vectors of size 384 dimension. I use the recommend points API (https://qdrant.github.io/qdrant/redoc/index.html#tag/points/operation/recommend_points) to retrive recommendations of some points. But surprisingly, there are always a handful of 'sticky' points on the top of the result recommendation irrespective of the input point. How can this be the case? Does it mean that these points are the closeset to every other points in the entire population? Do I have a problem in the index? Thanks for any help."
  },
  {
    "threadId": "1196189624750579835",
    "name": "Data modeling for future filtering.",
    "messages": "I was wondering if the qdrant supports a filtering based on multiple ids, like if I have:\n\nclass RelatedKeyword(BaseModel):\n    concept: int\n    instance: int\n\nA text vector will include multiple related keywords, each keyword is distinguishable by those two ids, meaning concept can be the same but instance of the concept different.\n\nIn future would I be able to filter vectors which have exactly (concept,instance) or should I create a custom unique id for each of the variations and filter based on the set of unique ids?"
  },
  {
    "threadId": "1196195067229118626",
    "name": "Read-only API key in Qdrant Cloud",
    "messages": "Hi, I have noticed Qdrant has read-only API key support. Does it also work in Qdrant Cloud? I cannot create one in \"Data Access Control\" and I haven't found a way to modify the config.yaml on the Cloud. My use-case is that I want to create index from documentation, host it on Qdrant Cloud and then I want to open source my app chatting/querying the docs. I don't want to share my API key nor force users to set an API key."
  },
  {
    "threadId": "1156905543294783508",
    "name": "Filtering on empty vectors",
    "messages": "Hello here, following up on this old thread https://discord.com/channels/907569970500743200/928649303898075196/1148933787925893120 for which I did not manage to reproduce the bug. \n\nToday I still have some data inserted in my cluster with no vectors. I used to catch errors when making recommend requests for a point with no vectors but I would to use the batch endpoint now so that I improve performances especially when using filters. The issue is that if a batch has one point with no vector the whole batch wont return result.\n\nIs there any way to easily find those points with no vectors on a collection so that I can clean them and get rid of the errors while using the recommend endpoints ?\n\nThanks for reading and for the awesome tool as always !"
  },
  {
    "threadId": "1206574165306638357",
    "name": "How to compute similarity score between two documents?",
    "messages": "I've uploaded two documents into qdrant db, first document has 10 points and 2nd document has 10 points actually the content of both the files are same (for testing purpose) now i want to do similarity search based on those two sets of vectors. 10 points form doc 1 and 10 points form doc 2. how do we query in such a way that we get a similarity score of vectors for both doc 1 and doc 2. Can someone please help me here. Since the point metadata contains the file name and page number is there any workaround to find the similarity score between two documents"
  },
  {
    "threadId": "1206599244019138561",
    "name": "Trying to get a point by metadata unsuccsfully",
    "messages": "Trying to find/delete a point by -\nclient.scroll(\n    collection_name=\"Delivery\",\n    scroll_filter=models.Filter(\n        must=[\n            models.FieldCondition(\n                key=\"confluence_page_id\",\n                match=models.MatchValue(value=\"Page_18157069\"),\n            )\n        ]\n    ),\n)\n\nWhen I copy the point you will find:\n{\"metadata\":{\"confluence_page_id\":\"Page_18157069\",\"page_title\":[\"page title\"]},\"page_content\":\"Header: 5. Evaluation\\nContent: M be released.\\n\"}\n\nAnd I get None in return, can't understand why?"
  },
  {
    "threadId": "1206637653068619776",
    "name": "qdrant_client.upsert error for previously running code",
    "messages": "My code was all running fine and I was able to upload everything on qdrant server with below standard code, now without data and code change, its giving error \n\nhere is my code snippet \nqdrant_client = QdrantClient(\n    url=qdrant_url,\n    api_key=qdrant_key,\n)\nqdrant_client.upsert(\n    collection_name=faq_collection_name,\n    points=models.Batch(\n        ids=base_faq[\"id\"],\n        vectors=base_faq[\"embedding\"],\n        payloads=faq_payloads\n    )\n)\n\nand the error I am getting from nowhere \n\n----> 7     points=models.Batch(\n      8         ids=base_faq[\"id\"],\n      9         vectors=base_faq[\"embedding\"],\n     10         payloads=faq_payloads\n     11     )\n     12 )\n\nFile /opt/conda/lib/python3.10/site-packages/pydantic/main.py:341, in pydantic.main.BaseModel.__init__()\n\nValidationError: 51 validation errors for Batch\nids\n  value is not a valid list (type=type_error.list)\nvectors\n  value is not a valid list (type=type_error.list)\nvectors -> __key__\n  str type expected (type=type_error.str)\n\nI have a planned client demo tomorrow, can someone please help me if any version change or something happend ЁЯШж"
  },
  {
    "threadId": "1195847079646466078",
    "name": "Creating clusters of vectors",
    "messages": "Does qdrant support any clustering algorithms(such as DBScan)?"
  },
  {
    "threadId": "1194545281228865566",
    "name": "Pricing of using qdrant locally?",
    "messages": "Using qdrant from docker container is free or paid ? or does it have any limitations of usage?"
  },
  {
    "threadId": "1195440931470581870",
    "name": "RAG with Qdrant / danswer",
    "messages": "I read this quote of a danswer dev regarding qdrant support. I have two questions. Firstly can you comment on the two points? IMHO qdrant does support multiple vectors per doc? Secondly what RAG middleware would you choose or recommend to look at when evaluating RAG solutions with qdrant as the main vector db?\n \n> Qdrant didn't support:\n> \n> * multiple vectors per document\n> * custom scoring functions allowing us to do time related decay, learning from feedback etc."
  },
  {
    "threadId": "1195448563216433192",
    "name": "Strategy for food recipe recommender",
    "messages": "Hey! We would like to use qdrant recommender API for food recipes. A user tells what ingredients they prefer in a meal and what they would like to avoid. We then search for a recipe that matches the desired list. \n\nI have tried this approach тАФ the desired/undesired ingredients are embedded using openai embeddings тАФ incorporating recommender API. The results are soso, as I do not see a big difference in score between the recipes. \n\nI was wondering whether the approach is a good one or maybe recommender API is not the best choice. Appreciate any suggestionsЁЯСН"
  },
  {
    "threadId": "1194699848226717757",
    "name": "Need some help pushing back on biased vendor comparison from MongoDB",
    "messages": "We love working with Qdrant, but our MongoDB rep is starting to get my bosses ear about potentially being a better vector store than Qdrant. We feel the following is one-sided and biased, but not deep enough in Qdrant to know points to counter with. Qdrants recent release with hybrid search is definitely one point to push back on.\n\nPlease see attached image of claims sent to my boss.\n\nThe also shared this - MongoDB Atlas Vector Search Explained in 3 Minutes\nhttps://www.youtube.com/watch?v=DUxUW0Kg7JM\n\nSo, can anyone please help us point to similar or better functionality in Qdrant docs or relevant content somewhere"
  },
  {
    "threadId": "1195385547267510354",
    "name": "\"Implementation on Win Server 2019\"",
    "messages": "Hello everyone,\n\nI am currently working on implementing the Qdrant image on a server running the Windows Server 2019 operating system. I have tried several approaches, and the docker pull command is functional. However, I am encountering issues with the command \"docker pull qdrant/qdrant.\" Therefore, my general question is whether it is possible to implement Qdrant on the Windows Server 2019 operating system.\n\nThank you for your assistance."
  },
  {
    "threadId": "1195108080610971788",
    "name": "Upgrading Qdrant Cluster",
    "messages": "Is it safe to upgrade our qdrant cluster from 0.11.0 to 1.7.0? Also would the 1.7.0 cluster be compatible with the Python qdrant client version for 0.11.0?"
  },
  {
    "threadId": "1195036208326266980",
    "name": "RBAC support & multitenancy",
    "messages": "Just wondering if RBAC is supported. If not, any plan to add it?"
  },
  {
    "threadId": "1194503208081575967",
    "name": "Imbalanced disk IOs with scalar quantization and oversampling",
    "messages": "I have a 5 node cluster. The collection config is as below. I assume the oversampling and rescore will have to read disks to get the original vectors.  I can see search requests are evenly distributed among nodes. However, the disk ios are very imbalanced. There are two nodes have nearly 10K IOPS and the rest nodes are only in 10s.  The two nodes having high IOPS are having 10% more vectors. I wonder why this can cause such a high skew on IOPS \n\n    \"status\": \"green\",\n    \"optimizer_status\": \"ok\",\n    \"vectors_count\": 147150443,\n    \"indexed_vectors_count\": 147090522,\n    \"points_count\": 147102722,\n    \"segments_count\": 127,\n    \"config\": {\n      \"params\": {\n        \"vectors\": {\n          \"embedding\": {\n            \"size\": 768,\n            \"distance\": \"Dot\"\n          }\n        },\n        \"shard_number\": 16,\n        \"replication_factor\": 2,\n        \"write_consistency_factor\": 2,\n        \"on_disk_payload\": true\n      },\n      \"hnsw_config\": {\n        \"m\": 40,\n        \"ef_construct\": 999,\n        \"full_scan_threshold\": 10000,\n        \"max_indexing_threads\": 32,\n        \"on_disk\": false\n      },\n      \"optimizer_config\": {\n        \"deleted_threshold\": 0.2,\n        \"vacuum_min_vector_number\": 1000,\n        \"default_segment_number\": 2,\n        \"max_segment_size\": 4000000,\n        \"memmap_threshold\": 20000,\n        \"indexing_threshold\": 20000,\n        \"flush_interval_sec\": 5,\n        \"max_optimization_threads\": 1\n      },\n      \"wal_config\": {\n        \"wal_capacity_mb\": 32,\n        \"wal_segments_ahead\": 0\n      },\n      \"quantization_config\": {\n        \"scalar\": {\n          \"type\": \"int8\",\n          \"quantile\": 0.99,\n          \"always_ram\": true\n        }\n      }\n    },"
  },
  {
    "threadId": "1194974790352637963",
    "name": "Recover snapshot into on-disk index",
    "messages": "I wanted to play with the practice datasets shared by Qdrant.\nSince my qdrant container ram allocation is limited, I wanted to create an on-disk index to be able to fit the dataset in my docker instance.\nI created the collection like this:\n```PUT collections/wolt_food\n{\n  \"hnsw_config\": {\n    \"on_disk\": true\n  }\n}```\nRecovered the snapshot like:\n```PUT /collections/wolt_food/snapshots/recover\n{\n  \"location\": \"file:///qdrant/storage/wolt-clip-ViT-B-32-2446808438011867-2023-12-14-15-55-26.snapshot\"\n}```\nI ran into this error:\n```{\n  \"error\": \"Wrong input: Snapshot is not compatible with existing collection: Collection vectors: Multi({}) Snapshot Vectors: Single(VectorParams { size: 512, distance: Cosine, hnsw_config: None, quantization_config: None, on_disk: Some(true) })\"\n}```\n\nWhat is the correct way to make certain configuration changes before recovering a snapshot into a collection?"
  },
  {
    "threadId": "1194676142804828190",
    "name": "Batch upload sparse and dense vectors",
    "messages": "Hi, should I be able to do batch upsert sparse and dense vectors with this formatting?\n=\n```\nqdrant_client.upsert(\n    collection_name=QDRANT_COLLECTION,\n    points=models.Batch(\n        ids=ids[i : i + batch_size],\n        vectors=[{\"dense\":vectors[i+j],\n                  \"sparse\":{\n                      \"indices\":sparse_indices[i+j], \n                      \"weights\":sparse_weights[i+j]}\n                } for j in range(batch_size)],\n        payloads=payloads[i : i + batch_size],\n    ),\n)\n```\nThis is how the collection is setup: \n```\nqdrant_client.create_collection(\n        collection_name=QDRANT_COLLECTION,\n        vectors_config={\n            \"dense\":models.VectorParams(\n            size=768, distance=models.Distance.COSINE, on_disk=True\n        )},\n        sparse_vectors_config={\n        \"sparse\": models.SparseVectorParams(\n            index=models.SparseIndexParams(\n                on_disk=True,\n            )\n        )\n        },\n        shard_number=2,\n        optimizers_config=models.OptimizersConfigDiff(\n            indexing_threshold=0,\n        ),\n    )\n```"
  },
  {
    "threadId": "1194431086281887855",
    "name": "Is Qdrant designed to withstand operational use by a large number of users?",
    "messages": "When using Qdrabt in server mode, are CRUD operations thread-safe? Also, when the database is accessed by multiple people, does performance decrease as the number of users increases?"
  },
  {
    "threadId": "1194645424653471834",
    "name": "error : 1 validation error for NamedVector",
    "messages": "Hi,\nI just updated my Python `qdrant-client` version to 1.7.0 with `poetry add`. During the search with sparse vectors, I received an error `1 validation error for NamedVector, value is not a valid list (type=type_error.list)`. \nOne weird behaviour is that if I install the `qdrant-client` package with `pip`, I don't receive the error. And while I install with `poetry add`, the error occurs.\nDo you have any idea about how to solve this? Thank you in advance."
  },
  {
    "threadId": "1194555773229989968",
    "name": "Distributed deployment: Docker locally.",
    "messages": "I was wondering if anyone has used the distributed deployment. Was reading up the docs and am quite confused as to how to direct searches to a particular shard key. \n1. In the documentation read that search is sent across all shards, if so then how does sharding helps here in Qdrant? \n2. Say I have two shard keys test and prod and I want the searches should be directed to particular shard_key prod, how to achieve that?\n3. Is sharding here in Qdrant different as of shardings that happens say in MongoDB?\n4. At the time of upsert we can define the shard_key where we want it to be inserted, so how does it helps at the time of search? How will it know which shard_key to go to for searching?\n\nI am new to sharding and would appreciate answers and patience ЁЯЩВ"
  },
  {
    "threadId": "1194482273974755448",
    "name": "Qdrant use case: Large number of collections, metadata filtering and memory management",
    "messages": "Hi!\n\nWe, at Augmend (https://augmend.com/), are looking into Qdrant as a vectorDB solution for our product. Our platform, among other functionalities, allows one or more users to hold a video conference. During the conference, our system records content from various sources and converts it to embeddings.\n\nI'd like to learn more about Qdrant and see if it is a good fit with our system. Here are a couple of questions:\n\n- Ideally we would like each meeting to be encapsulated in an isolated collection of vectors, for various reasons, such as improved retrieval, access permissions etc. I was looking at one of the threads from earlier today that mentioned too many collections being an anti-pattern in Qdrant. I am curious to hear your thoughts about our use case.\n\n- As a follow up question, I am also curious to learn more about the metadata filtering approach that Qdrant takes. I could imagine emulating the isolation above by querying on metadata, but these queries tend to degrade performance. How is metadata filtering implemented? All I could  find on the subject is: https://qdrant.tech/articles/filtrable-hnsw/ but it is unclear if this is the currently used approach and how performant it is.\n\n- How does memory management work in Qdrant? Suppose we run our instance as a Docker container and each collection is memory-mapped. Suppose also that our data does not fully fit in the allocated memory. Is there a caching policy to swap out collections or segments in this case (and if so what is it)? How is this affected by the large number of collections that our use case requires? Will this guarantee that the instance never runs out of memory and fails?\n\nI am looking forwards to hearing your thoughts.\n\nThanks!"
  },
  {
    "threadId": "1193575070472343635",
    "name": "Single collection vs multiple",
    "messages": "I have data that is structured as a feed that has many documents, each of which has many chunks. I currently embed each feed, document, and chunks and store them in Pinecone namespaces (with structured metadata for the parent/child relationships).\n\nWhen migrating to Qdrant, should I have a single collection with feeds, documents, and chunks? Or should each be their own collection? I assumed separate collections but your docs are aggressive about suggesting a single collection.\n\nThanks!"
  },
  {
    "threadId": "1194409315814093000",
    "name": "Slow search queries in local mode",
    "messages": "Hi all!\nI am trying to run a few benchmarks in local mode (i.e. I defined the `path` parameter to be a path on my local file system) in order to better understand if Qdrant could be used for my use case. \nI have three different collections, with a different number of vectors stored: 10.000, 100.000 and 500.000.\nFor each collection, then, I run many vector searches with different values for the `limit` parameter.\n\nHere are the results for the first two collections:\n\n**Collection with 10k vectors**\n- limit = 5 --> 211 milliseconds\n- limit = 10 --> 182 milliseconds\n- limit = 100 --> 159 milliseconds\n- limit = 1000 --> 226 milliseconds\n- limit = 5000 --> 411 milliseconds\n- limit = 10000 --> 543 milliseconds\n\n**Collection with 100k vectors**\n- limit = 10 --> 897 milliseconds\n- limit = 100 --> 944 milliseconds\n- limit = 1000 --> 1013 milliseconds\n- limit = 10000 --> 1381 milliseconds\n- limit = 50000 --> 2859 milliseconds\n- limit = 75000 --> 3634 milliseconds\n- limit = 100000 --> 3546 milliseconds\n\nI'm quite new to the world of vector DBs, but the search times here seem rather bad to me. I also tried to modify the parameters for best performance as described here https://qdrant.tech/documentation/guides/optimize/, but I did not notice any noticeable difference. \nHence, I'm wondering whether the problem is related to the local mode (I noticed that it uses sqlite under the hood) or if I need to change some other parameters. Do you have any suggestions? Would you expect these times in local mode? \n\nI am using the \"sentence-transformers/all-MiniLM-L6-v2\" model from fastembed, with a vector size of 384 (note that  benchmark times does not include the embedding time).\n\nThe python client is the latest available (1.7.0).\n\nI'm using a linux-based machine with 4 cores, 20GB of RAM and 500GB of disk space (SSD).\n\nThanks a lot!"
  },
  {
    "threadId": "1194123276490903652",
    "name": "Regd Qdrant retrieval failures",
    "messages": "I have used optimizer indexing and indexed around 44232 chunks but size of chunk are not uniform, its starts from 70B to 3000 characters will this cause an issue? When a query vector is posted in chatbot its saying not source document, for most of the questions. Does this happen, any suggestions to improve the retrieval process?"
  },
  {
    "threadId": "1193147178210840586",
    "name": "Avoid duplicates inside a collection",
    "messages": "Are there checks in place that validates when uploading documents to a collection that no identical points are created ? If I try to add the same document to the same collection it just creates 2 points with the same content."
  },
  {
    "threadId": "1192747074765197362",
    "name": "Is there a config such like minimumShouldMatch in elasticsearch when using filter",
    "messages": "As title mentioned, I want to fulfill at least 2 clauses when filtering at sometimes, is there a choice when dealing with such cases?"
  },
  {
    "threadId": "1204385728306618398",
    "name": "Upgrading Qdrant",
    "messages": "Are there any risks while upgrading qdrant from v1.5.1 -> v1.7.4 ?"
  },
  {
    "threadId": "1204317142304956446",
    "name": "Does Qdrant Cloud also support Japan region in GCP?",
    "messages": "I couldn't find it in the dashboard.\n\nAlthough I did find Tokyo for AWS.\n\nSorry for cross-posting but just for reference:\nhttps://discord.com/channels/907569970500743200/1187228597866467358/1204316512257839116\n\nAh... never mind, I've found an answer for now:\nhttps://discord.com/channels/907569970500743200/1047555268499755152/1203611881663631420"
  },
  {
    "threadId": "1185239282739335248",
    "name": "Issue with Qdrant and qdrant-dotnet",
    "messages": "Dear Qdrant Team,\n\nI hope this message finds you well. I am reaching out for assistance as I am facing challenges adapting the following tutorial using the Qdrant-dotnet library : https://cookbook.openai.com/examples/vector_databases/qdrant/using_qdrant_for_embeddings_search. This tutorial outlines a method for adding points with two distinct vectors to a Qdrant collection using Python. My goal is to achieve the same operation using .Net with the Qdrant-dotnet library.\n\nSummary of the Issue:\nI am having difficulty figuring out how to add two vectors to the same PointStruct object with QdrantClient. Could you provide an example to help me create the collection and PointStruct with QdrantClient in the same way as outlined in the tutorial?\n\nQdrant Configuration:\nI am using Qdrant locally with Docker.\n\nConnection to Qdrant:\nI am using the qdrant/qdrant-dotnet library, available at this address: https://github.com/qdrant/qdrant-dotnet#readme\n\nDatabase Configuration:\nI would like to successfully create a collection identical to the one presented in the tutorial.\n\nI would appreciate your guidance on resolving these specific issues. Thank you in advance for your assistance.\n\nBest regards,"
  },
  {
    "threadId": "1192877178472370216",
    "name": "Cluster outage",
    "messages": "We are having an outage with our Qdrant cluster (running in AWS EKS, version 1.4.1). 2 nodes restarted for some reason and now the cluster is a bad state. What I mean by this is that raft info is not the same, the shard information is not consistent (some nodes report shard as being active with an active transfer, some report the same shard as dead).  There are also error like this one: \"error\": \"Service internal error: The replica set for shard 0 on peer 1160470838309247 has no active replica\". Many collections report a pending shard with shard transfer in progress but  those seem to be stuck. We tried to cancel one of them, kill the resulting dead shard and initiate a new copy but it does not seem to be working (no progress).\nAll nodes are accessible and not stuck.\nI have a couple of questions:\n    1) Is there a guide or a document with a set of troubleshooting steps?\n    2) Is there a configuration in place to prevent system from initiating shard transfer automatically when a node goes down (it's better to wait until node is back up rather than initiate a shard transfer which would not be needed after just few minutes)?\nThank you."
  },
  {
    "threadId": "1192601732421791886",
    "name": "consensus brain split after enable /readyz",
    "messages": "Hi, \nI am testing /readyz on 1.7.2 on a 5 node clusters. However, I noticed that first shutdown pod never enters the ready state after waiting for 45 minutes. The endpoint /readyz always returns 503. It seems to me that the rest 4 nodes are in a brain split state on consensus and cannot reach agreement. The /cluster endpoint on them showing different leader and commits.  Any idea what is the cause and how this can be fixed?"
  },
  {
    "threadId": "1192832028954067014",
    "name": "downtime during upgrading Qdrant version",
    "messages": "Hi,\nWe would like to upgrade our production clusters to v1.7, but there is always a downtime while upgrading. Do you have any functionality such as the collection alias to avoid the downtime while upgrading clusters?\nThanks."
  },
  {
    "threadId": "1192800083427733604",
    "name": "How to implement Qdrant database in Django?",
    "messages": "I am developing a back-end application with Django REST Framework. I will use Qdrant as vector database of my app. But I don't know the correct way of accessing Qdrant database in Django. Should I just initialize the client in views.py?"
  },
  {
    "threadId": "1192574104973357096",
    "name": "Question about RAM spike",
    "messages": "Hi there - I just added a collection of almost 10 million vectors of 2048 DIM to my 4 node cluster (32 GB). I am trying to keep RAM as low as possible so everything is on disk where it can be on disk and I am trying out binary quantization with always_ram=False. RAM usage still went up 40 GB when I indexed this collection (10 GB/node). Why would this be if my payload, hnsw_config are all set to on disk? Here is the collections config:\n{\n    \"result\": {\n        \"status\": \"green\",\n        \"optimizer_status\": \"ok\",\n        \"vectors_count\": 9805038,\n        \"indexed_vectors_count\": 9786322,\n        \"points_count\": 9805038,\n        \"segments_count\": 396,\n        \"config\": {\n            \"params\": {\n                \"vectors\": {\n                    \"size\": 2048,\n                    \"distance\": \"Cosine\"\n                },\n                \"shard_number\": 16,\n                \"replication_factor\": 1,\n                \"write_consistency_factor\": 1,\n                \"on_disk_payload\": true\n            },\n            \"hnsw_config\": {\n                \"m\": 16,\n                \"ef_construct\": 100,\n                \"full_scan_threshold\": 10000,\n                \"max_indexing_threads\": 0,\n                \"on_disk\": true\n            },\n            \"optimizer_config\": {\n                \"deleted_threshold\": 0.2,\n                \"vacuum_min_vector_number\": 1000,\n                \"default_segment_number\": 0,\n                \"max_segment_size\": 200000,\n                \"memmap_threshold\": 20000,\n                \"indexing_threshold\": 20000,\n                \"flush_interval_sec\": 5,\n                \"max_optimization_threads\": 1\n            },\n            \"wal_config\": {\n                \"wal_capacity_mb\": 32,\n                \"wal_segments_ahead\": 0\n            },\n            \"quantization_config\": {\n                \"binary\": {\n                    \"always_ram\": false\n                }\n            }\n        },\n        \"payload_schema\": {}\n    },\n    \"status\": \"ok\",\n    \"time\": 0.000814482\n}"
  },
  {
    "threadId": "1192718162362368080",
    "name": "How do I know how much memory Qdrant Cloud is currently using?",
    "messages": "When Qdrant Cloud's memory usage approaches 100%, the cluster becomes unresponsive. Therefore, I infer the memory usage from the number of points and an alert goes off when it exceeds 80%.\n\nI use a simple formula written on the official website, but this seems to have a large margin of error.\n`memory_size = number_of_active_vectors * vector_dimension * 4 bytes * 1.5`\n\nAre there any other techniques to know the current memory usage?"
  },
  {
    "threadId": "1192397337209876590",
    "name": "trying qdrant_demo got error on 'docker-compose -f docker-compose-local.yaml up'",
    "messages": "not sure why im getting this error\nerror log\n`[+] Building 1.8s (3/4)                                                                          docker:desktop-linux\n[+] Building 1.9s (4/4) FINISHED                                                                 docker:desktop-linux\n => [web internal] load .dockerignore                                                                            0.0s\n => => transferring context: 109B                                                                                0.0s\n => [web internal] load build definition from Dockerfile                                                         0.0s\n => => transferring dockerfile: 1.22kB                                                                           0.0s\n => ERROR [web internal] load metadata for docker.io/library/python:3.11-slim-bookworm                           1.8s\n => [web internal] load metadata for docker.io/library/node:20-bookworm-slim                                     0.0s\n------\n> [web internal] load metadata for docker.io/library/python:3.11-slim-bookworm:\n------\nfailed to solve: python:3.11-slim-bookworm: error getting credentials - err: exit status 1, out: `"
  },
  {
    "threadId": "1192244716935327815",
    "name": "How to recover a snapshot taken on another cluster?",
    "messages": "I'd like to take a snapshot built  on a different cluster and restore it on the current cluster. The motivation is to make index building offline without taking too much resource on the current cluster. I wonder if there is any guideline how to do this? Should I upload the shard snapshot first or I can directly call the recovery API?"
  },
  {
    "threadId": "1189824358638698546",
    "name": "Benchmarking Qdrant with limited resources",
    "messages": "Hi,\nCan we configure the max number of cores and storage memory that should be used by Qdrant.?\nIf yes, it would be really helpful if you guys mention the steps to do so. ЁЯШГ\nThanks."
  },
  {
    "threadId": "1192183186575339661",
    "name": "Query about Self-Hosting Qdrant with a 64GB Memory Dataset",
    "messages": "Dear Support Team,\n\nWe are considering switching to a self-hosted Qdrant setup, primarily to utilize our existing AWS credits. Previously, we have been using Qdrant Cloud, but now we are interested in deploying on our own. I have a couple of questions regarding this transition:\n\n1. Is it possible to directly migrate our data from Qdrant Cloud to our self-hosted environment on AWS?\n\n2. What is the recommended deployment approach for a dataset that fits within 64GB of memory? I'm considering different options such as:\n   - A single EC2 instance with 64GB of memory\n   - Four EC2 instances, each with 16GB of memory\n   - A Kubernetes setup (probably EKS, if we choose AWS)\n\nGiven our team's limited familiarity with Kubernetes and considering management costs, a single EC2 instance seems to be a feasible option. However, I am curious to know if this is a generally recommended practice or if there are better alternatives.\n\nAny guidance or best practices you can provide on this matter would be greatly appreciated.\n\nBest regards"
  },
  {
    "threadId": "1187026896572137502",
    "name": "Out of memory issue",
    "messages": "Yesterday, we encountered a significant issue with our system. We started receiving 503 service unavailable errors. Upon inspecting the logs, it was clear that we were running out of memory.\nThe metrics indicated a continuous increase in RAM usage without any subsequent decrease. After restarting the cluster, the issue persisted. It was only resolved when we deleted all the collections.\nIn our app, we utilize 'knowledge bases', which can consist of one or multiple embedded files. Our approach has been to create a new collection for each knowledge base.\nAccording to our research and documentation review, it's recommended to store all knowledge bases in a single collection, using fields like knowledgebase_id in the payload. This approach should include filtering that field for context retrieval of the wanted knowledge base. Also, we found that using In Disk storage and indexing on the knowledgebase_id field will help us mitigate this issue.\nWe're considering implementing these changes and possibly exploring quantization if necessary.\nWe would greatly appreciate your insights and suggestions on this situation. Any additional best practices or improvements are welcome."
  },
  {
    "threadId": "1192079882449076254",
    "name": "short document",
    "messages": "I have stored a record as \"booking form\" but when I perform a similarity search like \"ghjghjghj\", it always returns this booking form even though this one is irrelevant to the original document."
  },
  {
    "threadId": "1202747809212796928",
    "name": "Connection reset by peer -- Error",
    "messages": "Hi, I've been playing around with a cluster on the qdrant cloud and I'm unable to connect to the service. On the qdrant webpage, I'm unable to enter the dashboard of the cluster and in python I'm getting the following error. Has anyone encountered this before?"
  },
  {
    "threadId": "1202652524054716476",
    "name": "Sub-groups oriented configuration",
    "messages": "Where are the docs or how do I implement a solution like the one advised here? https://qdrant.tech/documentation/cloud/capacity-sizing/"
  },
  {
    "threadId": "1202756696393654322",
    "name": "Clarification on vectors_count",
    "messages": "When I delete points from qdrant, I notice that `points_count` goes down, but `vectors_count` doesn't. Does this mean some data persists in qdrant, or is `vectors_count` not accurate?"
  },
  {
    "threadId": "1201327444754444478",
    "name": "Can I set memory over 64GB In Qdrant Cloud?",
    "messages": "I see memory limit is 64GB. When nodes increasing, does cluster automatically scale horizontally? If set 4 nodes of 64GB memory, does cluster have 256GB memory capcacity?"
  },
  {
    "threadId": "1191740219469336576",
    "name": "Scale Up takes long time",
    "messages": "Hi.\n\nWe are trying to Scale Up our instance and it's taking quite some time (about 30 min so far) and our instance is pretty small 2 GB RAM, 1.37 GB Data so far.\n\nAny advice or idea why it is taking this long? In the meantime we have created new instance now so we can keep our production up but still would be interesting to understand why scale up on existing instance takes such a long time?"
  },
  {
    "threadId": "1191724110410166362",
    "name": "Docker build Qdrant image issue",
    "messages": "use below command я╝Ъ docker buildx build .  --platform=linux/amd64 --platform=linux/arm64 --build-arg  FEATURES=\"multiling-korean\"\nError info belowя╝Ъb\n------\nDockerfile:94\n--------------------\n  93 |     # https://github.com/tonistiigi/xx/pull/108\n  94 | >>> RUN PKG_CONFIG=\"/usr/bin/$(xx-info)-pkg-config\" \\\n  95 | >>>     PATH=\"$PATH:/opt/mold/bin\" \\\n  96 | >>>     RUSTFLAGS=\"${LINKER:+-C link-arg=-fuse-ld=}$LINKER $RUSTFLAGS\" \\\n  97 | >>>     xx-cargo build --profile $PROFILE ${FEATURES:+--features} $FEATURES --features=stacktrace --bin qdrant \\\n  98 | >>>     && PROFILE_DIR=$(if [ \"$PROFILE\" = dev ]; then echo debug; else echo $PROFILE; fi) \\\n  99 | >>>     && mv target/$(xx-cargo --print-target-triple)/$PROFILE_DIR/qdrant /qdrant/qdrant\n 100 |     \n--------------------\nERROR: failed to solve: process \"/bin/sh -c PKG_CONFIG=\\\"/usr/bin/$(xx-info)-pkg-config\\\"     PATH=\\\"$PATH:/opt/mold/bin\\\"     RUSTFLAGS=\\\"${LINKER:+-C link-arg=-fuse-ld=}$LINKER $RUSTFLAGS\\\"     xx-cargo build --profile $PROFILE ${FEATURES:+--features} $FEATURES --features=stacktrace --bin qdrant     && PROFILE_DIR=$(if [ \\\"$PROFILE\\\" = dev ]; then echo debug; else echo $PROFILE; fi)     && mv target/$(xx-cargo --print-target-triple)/$PROFILE_DIR/qdrant /qdrant/qdrant\" did not complete successfully: exit code: 101"
  },
  {
    "threadId": "1202035257243881532",
    "name": "update a qdrant docker image while functioning on server",
    "messages": "hello ЁЯШД\nhow can i update a functioning qdrant docker container running on server with vector data in it. what are the steps?"
  },
  {
    "threadId": "1202034353991843890",
    "name": "binary quantization with text-embeddings-small",
    "messages": "OpenAi's new text-embeddings are more 'sharper' than ada-002. Do you think they would do well with binary quantization with qdrant?"
  },
  {
    "threadId": "1201889578303365152",
    "name": "Payload KEYWORD on list metadata",
    "messages": "Hi! I have a question related to the filtering feature.\n\nWhen creating an index with Qdrant's PayloadSchemaType.KEYWORD, can you still benefit from the speed improvements of indexing even when filtering with list-type metadata?"
  },
  {
    "threadId": "1201453631283658762",
    "name": "Batch upload when vector list has more elements than idlist and payload list",
    "messages": "Hi, was doing a batch upload and because of some bad coding on my part, I fed more vectors than there were ids or payloads into the batch upload process. Qdrant ingested it and of course the search results started to get weird. Realize this is a mistake on my part, but it might be a good idea to throw and error when the number of elements are not of the same length."
  },
  {
    "threadId": "1201790332006563850",
    "name": "Error creating collection",
    "messages": "Thank you - here is the code , I can query and retrieve perfectly but can no longer create collections:\n '''\nfrom qdrant_client.models import PointStruct\nimport numpy as np\nfrom qdrant_client.models import Distance, VectorParams\nfrom qdrant_client.http import models\nfrom langchain_community.vectorstores import Qdrant\nfrom langchain_community.embeddings import OpenAIEmbeddings\nfrom qdrant_client import QdrantClient\nfrom dotenv import load_dotenv\nimport os\n\ncreate your client\nload_dotenv()\n\nclient = QdrantClient(\n    url=os.getenv('QDRANT_HOST'),\n    api_key=os.getenv('QDRANT_API_KEY')\n)\n\ncreate collection\ncollection = os.getenv('QDRANT_COLLECTION_NAME')\n\nclient.recreate_collection(\n    collection_name=collection,\n    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n)\n\nError: qdrant_client.http.exceptions.ResponseHandlingException: The read operation timed out'''"
  },
  {
    "threadId": "1200320681066373181",
    "name": "Querying Trained Qdrant DB Instance from AWS Lightsail container",
    "messages": "Hello,\nAs the title mentions, I have a \"trained\" qdrant database (currently trained on a chapter of a textbook only) that I want to send queries to from my Flask server, but I'm not sure how to actually send these requests over to my qdrant instance. \nDoes anyone know where I can find information on this/have any experience with this?\nIs there some kind of way to get a qdrant endpoint api key to send payloads to?\n\nThank you in advance."
  },
  {
    "threadId": "1189889380932919327",
    "name": "Help with Retrieving User Messages from Qdrant in a Chatbot Application",
    "messages": "Hi, I'm currently working on a chatbot and integrating Qdrant to save each message in the conversation in the vector database. I'm facing challenges with retrieving messages based on user_id and user_text. I want to fetch all messages for a specific user, and also find messages similar to a given text.\n\nThe product is a chatbot that saves each message in the vector database and uses it as a long term memory. Currently this is my code. I'm not sure if I'm on the right track since I'm a beginner. \n\nI see that the vector_count is increasing with the add_message function but it seems that the retrieve_messages function not retrieving the messages.\n\nAppreciate any help\n\nCode:\n\nqdrant_host = os.getenv(\"QDRANT_HOST\")\nqdrant_api_key = os.getenv(\"QDRANT_API_KEY\")\nclient = qdrant_client.QdrantClient(qdrant_host, api_key=qdrant_api_key)\n\nopenai_api_key = os.getenv(\"OPENAI_API_KEY\")\nembeddings = OpenAIEmbeddings(api_key=openai_api_key)\n\nvectorstore = QdrantVectorStore(\n    client=client,\n    collection_name=\"COLLECTION_1\",\n    embeddings=embeddings\n)\n\ndef add_message(user_id, user_text):\n    doc = Document(\n        page_content=user_text,\n        metadata={\"user_id\": user_id}\n    )\n\n    vectorstore.add_documents([doc])\n    \n    \ndef retrieve_messages(user_id):\n    query_filter = models.Filter(\n        must=[\n            models.FieldCondition(\n                key=\"user_id\", \n                match=models.MatchValue(value=user_id),\n            )\n        ]\n    )\n\n    search_params = models.SearchParams(hnsw_ef=128, exact=False)\n\n    neutral_query_vector = [0.0] * 1536\n\n    search_results = client.search(\n        collection_name=\"COLLECTION_1\",\n        query_filter=query_filter,\n        search_params=search_params,\n        query_vector=neutral_query_vector, \n        limit=10  \n    )\n\n    return [result.payload.get('message') for result in search_results]"
  },
  {
    "threadId": "1189679585336512653",
    "name": "My embedding generation model changed its vector size, I need to re upload my points?",
    "messages": "Hello,\n\nI use Cohere's (https://docs.cohere.com/reference/embed)  embed-multilingual-v2.0 (768 vector size) model to generate embeddings but now I want to switch to embed-multilingual-v3.0 (1024 vector size).\n\nCurrently, my Qdrant db uses only 1 vector with 768 size (since I use embed-multilingual-v2.0). \n\nDoes this means that in order to use the new Cohere's model I have to delete all my points, then change the configuration of the vector size to 1024 and then re upload the points?\n\nThis just sounds like I'd be doing something wrong, since embeddings models will be changing frequently overtime and deleting the vectors or points every time this happens doesn't sound like a good practice.\n\nI'd appreciate any guidance on what are the good practices here. Hopefully there is a way to change the vector size without having to delete the points.\n\n\nThanks!"
  },
  {
    "threadId": "1189032709868224512",
    "name": "@kapa.ai#2237 error 400 while inserting with llama index \"expected value\"",
    "messages": "Hello ! \nWhile inserting data in a qdrant cluster using llama index, I have often this error displayed : \nqdrant_client.http.exceptions.UnexpectedResponse: Unexpected Response: 400 (Bad Request)\nRaw response content:\nb'{\"status\":{\"error\":\"Format error in JSON body: expected value at line 1 column\n\nHowever, I checked on the field of my pandas df used (field \"text\") and there is no empty string neither any NaN value. \n\nWhere could it come from and how to avoid that?"
  },
  {
    "threadId": "1189651195103555675",
    "name": "Increasing self-hosted qdrant server's replica count without removing old deployment",
    "messages": "We have a self-hosted Qdrant running on a Kubernetes cluster. Is there a way to manually or automatically increase or decrease the size of the cluster without impacting the service? What about increasing the persistence size? We deploy it using Helm."
  },
  {
    "threadId": "1189446645671268446",
    "name": "Loading of payload index at node startup time",
    "messages": "From your documentation it looked like payload indexes at runtime always resides in RAM. But do you also save the payload indexes at disk so that at startup it can be quickly loaded from disk? I am asking this question because for our use-case, where we have a lot of payload indexes the nodes are taking a lot of time to start up. Just wondering if qdrant re-create the payload indexes each time at startup?"
  },
  {
    "threadId": "1189599967296372847",
    "name": "Are the points in a shard divided into segments based on some distance based clustering",
    "messages": "For the understanding of our team wanted to know what is the logic which determines which point will go to which segment of a shard. Is it based on the ordering of the points in WAL or something else. Can you please explain in a concise way how segments are created and points allocated to them?"
  },
  {
    "threadId": "1200226154225864774",
    "name": "How does # of open files scale in terms of # of payloads, and # of collections?",
    "messages": "Hi all,  \n\n\nWe've encountered https://github.com/qdrant/qdrant/issues/1725 a couple times. With too many open files, and I was hoping someone could help me understand the root cause a bit more. \n\nThe first time we encountered it the suggested https://qdrant.tech/documentation/guides/common-errors/ to increase the ulimit in the docker container fixed it (huge ЁЯЩП  to whoever wrote it.) \n\n\nThe second time we encountered again this time with much more collections and much more data and even with the max ulimit `--ulimit nofile=65535:65535` it was too much.\n\nI saw in the notes each collection requires some open files -- which I suspect is the problem--  but I wanted to  better understand what else contiributes  to this e.g. number of payloads, indices, etc!\n\nThanks in advance"
  },
  {
    "threadId": "1189321342487638096",
    "name": "Filter results: return only 1 result with x characteristic.",
    "messages": "Hi,\n\nI am using the search endpoint on the REST API to search for some product variants saved in my Qdrant \n\"Variants\" collection. Each variant represents a unique variation of a product. Example: the black shirt has size S, M, and L (3 variants saved in the collection). \n\nVariants of the same product are identified because they have the same \"product_url\" value in their json object.\n\nMy problem is tha every time I use embeddings to search for a black shirt size L, I get all the other variants as results as well. My objective is to retrieve only the most accurate result per each product. \n\nBasically, I need a filter that allows me to \"rertieve only 1 variant per product_url\".\n\nIs there any way to achieve this? \n\nThanks!"
  },
  {
    "threadId": "1199715723518414899",
    "name": "Build errors in`collection_service.proto`",
    "messages": "When I try to build the code in `lib/api`, I get this error. Also, my entire `collection_service.proto` file has red squiggles all over it. Wondering if anyone can help me out in this one"
  },
  {
    "threadId": "1188619563248664637",
    "name": "Golang Client filter MatchAny example",
    "messages": "Hi , I need to search with a filter of a field matching keyword value that is equal to one of the values in [\"a\", \"b\", \"c\"] as described in the document : https://qdrant.tech/documentation/concepts/filtering/. But I cannot find a way to do this in the Golang client. Can anyone please show me an example how this can be done with Golang client?"
  },
  {
    "threadId": "1188776881311461426",
    "name": "Incorrect Search Results comparing with scroll",
    "messages": "When I use scroll api with filter conditions like below, it returns one point\n```\nPOST /collections/t_feed_text_detail_recently/points/scroll\n{\n  \"filter\": {\n    \"must\": [\n      {\n        \"key\": \"time_id\",\n        \"range\": {\n          \"gte\": 1703409019298,\n          \"lte\": 1703495419298\n        }\n      },\n      {\n        \"key\": \"user_id\",\n        \"match\": {\n          \"value\": \"12312312312\"\n        }\n      }\n    ]\n  },\n  \"params\": {\n    \"quantization\": {\n      \"ignore\": false,\n      \"rescore\": false\n    }\n  },\n  \"limit\": 10,\n  \"offset\": 1,\n  \"with_payload\": true,\n  \"with_vector\": false\n}\n```\nAnd I add `vector` and `score_threshold` params, set `score_threshold` as 0.01я╝Мthen use search API, it returns with no points. What's wrong with me. The actually similarity was more than 0.8"
  },
  {
    "threadId": "1187662460514205757",
    "name": "LoadBalancer & SSL",
    "messages": "I am converting the helm ClusterIP Service into a LoadBalancer to expose qdrant on K8 for testing the easist way possible. I am having issues using ingress as it is only HTTP(S) on port 433 and 80 and I can't get the routing to work (SSL seemed easier with ingress through GKE managed certs). \n\nThe LoadBalancer is working fine for HTTP but I want to secure it with SSL and ApiKeys.\n\nI have set the config to:\n\n```\n  cluster:\n    enabled: true\n  service:\n    enable_cors: true\n    enable_tls: true\n    verify_https_client_certificate: false\n```\n\nI need to mount my cert as a secret to the statefull set it seems. Like so:\n\n```\n      volumeMounts:\n        - name: ssl-cert-volume\n          mountPath: \"/etc/ssl/certs\"\n          readOnly: true\n      volumes:\n      - name: ssl-cert-volume\n        secret:\n          secretName: my-ssl-secret\n```\n\nThe current helm chart doesn't support this and I am wondering if this is something that you guys are open to a PR on?\n\nhttps://github.com/qdrant/qdrant-helm/blob/main/charts/qdrant/templates/statefulset.yaml"
  },
  {
    "threadId": "1187746477523603588",
    "name": "How to change embedding model",
    "messages": "Hi,\n\nI am using Qdrant with fastembed in the Python Client API, as described in this documentation: https://python-client.qdrant.tech. I can see that the default embedding model that it is using is 'fast-bge-small-en', but I would like to experiment with different models to see which has the best accuracy for my use case. \n\nHow can I change the embedding model when using the new Python Client API (with the client.add and client.query methods)? Also what are the different options for embedding models?\n\nWould much appreciate your help as I couldn't find what I was looking for in the documentation"
  },
  {
    "threadId": "1186907871452069898",
    "name": "GKE Autopilot",
    "messages": "Will the Qdrant Helm Chart work well with GKE Autopilot? \n\nNew to k8 and trying to learn. How can I control the size of each node in both GKE autopilot and if not appropriate GKE standard (e.g. size of RAM and disk space per node).\n\nAlso, do I need to deploy a loadbalaner on GKE to publicly access qdrant or is this handled by the helm chart? \n\nI can see where i can set an API key in the config file."
  },
  {
    "threadId": "1187420170977824851",
    "name": "how to automate the process of updating my database",
    "messages": "I haven build a semantic search engine with qdrant.\nThe code encode some columns from a dataframe and then do the searching .\nHow should i handle the part of the dataframe column has changed l, deleted or updated?\nThe data is stored in another sql database.\nAnd help?and thanks in advance"
  },
  {
    "threadId": "1187404803744202752",
    "name": "Cloud cluster dashboard issue",
    "messages": "I'm trying to get into the dashboard of multiple clusters but I keep on getting \"We're really sorry, but something went wrong. Our team has been notified and is working hard to fix this.\nPlease try again later. We appreciate your patience and understanding.\" for all of them. \nIs this a known issue or just on my instance?"
  },
  {
    "threadId": "1187384086654886008",
    "name": "Fuzzing meta data filter",
    "messages": "Hello guys , i hve a question , ve been using qdrant with haystack last week and i m wondering about a specific feature , i m using embedding retriever in haystack with a Qdrant documentstore \n\nretrieved_docs = embedding_retriever.retrieve(query=query_text, filters=filters, top_k=top_k)\npassing on a meta data filter that filter documents before the retreival , my question is can this filtering process have some sort of fuzziness in it ? \nmeans instead of excat filter match will be some sort of close to the filter match ? is this supported in qdrant ?"
  },
  {
    "threadId": "1187352621661556808",
    "name": "Manual download of fast-bge-small-en-v1.5.tar.gz",
    "messages": "How would I go about manually downloading the model so that its available before making any calls to QdrantClient. Im using the python client.\nI could get it from huggingface i guess and tar it myself but wondering if there is some alternative?\n\n```\ngit clone git@hf.co:BAAI/bge-small-en-v1.5\ntar -czvf local_cache/fast-bge-small-en.tar.gz bge-small-en-v1.5\n```"
  },
  {
    "threadId": "1186651440735858768",
    "name": "Search by Time Range",
    "messages": "Does Qdrant support searching via metafilters by time range because I couldn't find examples of it in the docs?\n\nIdeally I want to use LlamaIndex + Qdrant to search for vectors from a set date range."
  },
  {
    "threadId": "1186992040408514570",
    "name": "Llama Index Docs add to Qdrant vectors_count/points_count confusion",
    "messages": "Maybe this isn't the right place to post since I'm using Llama Index but when running the following code and assuming the documents=NOTES I pass in have a length of 2500, I would assume I get vectors_count / points_count of at least 2500. if something gets chunked I should get even more records in the vector db.\n\n\nBut I end up with 1400-ish. Do you have any idea why that might be the case?\n\n```\nclient = qdrant_client.QdrantClient(url=\"http://localhost:6333\")\nvector_store = QdrantVectorStore(client=client, collection_name=\"NOTES\")\n\npipeline = IngestionPipeline(\n    transformations=[\n        SentenceSplitter(chunk_size=512, chunk_overlap=0),\n        HuggingFaceEmbedding(model_name='XXXXX'),\n    ],\n    vector_store=vector_store,\n)\n\npipeline.run(documents=NOTES)\n```"
  },
  {
    "threadId": "1186961227902111816",
    "name": "Qdrant v1.6.1 Configuration Issue with Read-Only API Key",
    "messages": "I've configured Qdrant v1.6.1 with two sets of keys in the config.yaml file. One set is at the instance level for both read and write access, while the other is exclusively for read-only access. However, when attempting to access the dashboard using the read-only key, it fails to authorize.\n\n\nLog:\n```\n2023-12-20T09:08:45.833491Z  INFO actix_web::middleware::logger: 10.12.1.210 \"GET /collections HTTP/1.1\" 403 15 \"http://qdrant-test.dev.engati.local:6333/dashboard\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\" 0.000039\n```\n\nconfig.yaml \n```\napi_key: <key>\nread_only_api_key: <key> \n```\nNote: The issue persists despite following the setup outlined in this https://qdrant.tech/documentation/guides/security/#read-only-api-key"
  },
  {
    "threadId": "1186912227131928616",
    "name": "Is Ingestion order affecting the retrival result?",
    "messages": "Do anyone experience different ingestion order cause retrieval result? If yes, any reason for that?\n\nFor example, if I have 5 years of corpus, and I ingest them in the order of 2015, 2014, 2013, 2012, 2011. Then I search using vector similarity search, the result will be different from the case that I ingest them in the order of 2011, 2012, 2013, 2014, 2015.\n\nIs this somehow related to the way ANN algorithm works?"
  },
  {
    "threadId": "1197541210294861865",
    "name": "Query and answer process",
    "messages": "Hello! I'm new to Qdrant and trying to understand how it works in terms of querying and answering. Specifically, since Qdrant takes vectors but can provide string-like answers, could someone explain the communication process between search API and Qdrant in simple terms? \nDoes Qdrant do its own embeddings, and if so, what kind of embedding model does it use? Thanks!"
  },
  {
    "threadId": "1197580697389498519",
    "name": "Moving Payload to Disk",
    "messages": "If we update a collection to have payload on disk, will the index need to be rebuilt and cause a lock?"
  },
  {
    "threadId": "1197634493570363533",
    "name": "Question on search/recommend result score ranking",
    "messages": "Hi, I perform search and recommend APIs and the ranking of result points is  based on the 'score'. Are these score solely depends on cosine distance (the collection was created with cosine distance as the vector param)? I compute the cosine distance between the input vector and the result vectors separately. I found that the ranking based on the  score is not the same as ranking based on my computed cosine distance.  Thanks for any help"
  },
  {
    "threadId": "1185627514190713075",
    "name": "Canonical Distributed Qdrant Setup",
    "messages": "Hi all! We are at the point where we want to think about load balancing our qdrant for higher throughput. I think from my understanding of these threads the canonical way is:\n\n1. AWS\n2. EBS instead of EFS \n3. https://qdrant.tech/documentation/guides/distributed_deployment/ for multiple nodes working together\n\nDoes that feel about right? Additionally is there any concept of read only qdrant clients to get around locking ЁЯдФ"
  },
  {
    "threadId": "1186541401475915837",
    "name": "Replication across nodes in distributed setup?",
    "messages": "Are shard replicas automatically placed on different nodes in a cluster?\n\nFor example, in the example in https://qdrant.tech/documentation/guides/distributed_deployment/#replication-factor if the cluster has 6 nodes, and the create collection request specifies 6 shards and replication factor 2, is it guaranteed that none of the two replicated shards would be placed on the same node?\n\nThe intention is to ensure fault tolerance of any one node. If it was not guaranteed that replicas were placed on different nodes, then to achieve this behavior it seems one would have to do this manually by creating multiple shards and moving them to differing nodes.\n\nNot able to find docs explaining this clearly.\n\nThanks!"
  },
  {
    "threadId": "1186452320758808596",
    "name": "Best practice for zero downtime when upgrade or rollout statefulset",
    "messages": "My qdrant cluster deployed as statefulset. The collection has 2 replicas. When upgrade the version or other rollout, the search queries are down too which is unexpected to me because replicas of the same shard are distributed into different nodes. I wonder if people have any recommendation to minimize or have zero downtime for such cases."
  },
  {
    "threadId": "1197248699273854976",
    "name": "Removing dead note fails",
    "messages": "I'm trying to delete dead node from the cluster as stated in the docs:\nhttps://qdrant.tech/documentation/guides/distributed_deployment/#node-failure-recovery\n> To exclude failed nodes from the consensus, use remove peer API. Apply the `force` flag if necessary.\nBut it fails with this error:\n```\n$ curl -X DELETE '127.0.0.1:6333/cluster/peer/3097135619420737?force=true'\n{\"status\":{\"error\":\"Service internal error: Waiting for consensus operation commit failed. Timeout set at: 10 seconds\"},\"time\":10.007115092}\n```\nIs this a bug or I'm misunderstanding how it should work?"
  },
  {
    "threadId": "1186305559499124786",
    "name": "RetrievalQA exception: 403 Forbidden",
    "messages": "The following code works good in  local machine. \n\nWhen I implement the same in third part server (pythonanywhere) it is throwing the following error:\n2023-12-18 13:27:39 RetrievalQA exception: 403 Forbidden\n\nAppriciate help. Thanks in advance\n\nqdrant_api_key = get_qdrant_api_key()\n\ndef get_chat_context(refined_prompt)\n    chat_context = ''\n    qdrant_embeddings = OpenAIEmbeddings() #paid embedding model\n    #embeddings = HuggingFaceInstructEmbeddings(model_name='hkunlp/instructor-xl')  #alternate opensource embedding model\n\n    #create vectorstore by interacting through qdrant_client and embeddings using OpenAI model\n    #Knowldge source collction is already created and stored in qdrant vector db using another program\n    try :\n        vectorstore = qdrant.Qdrant(\n            client=qdrant_client,               \n            collection_name=qdrant_collection_name, \n            embeddings=qdrant_embeddings            # the choice of embeddings model to be used \n        )\n    except Exception as e:\n        print (f'Vector store creation Exception:{e}')\n\n    llm = ChatOpenAI()    #LLM is defined using ChatOpenAI\n    try :\n        #creates qa instance to use agumented transformers to answer questions (combined and organized before passign to transformer model)\n        qa = RetrievalQA.from_chain_type (\n            llm = ChatOpenAI(),\n            chain_type = 'stuff', #In Langchain, 'stuff' argument denotes a default approach to concatenate retrived documents in single chain\n            retriever = vectorstore.as_retriever()\n        )\n        #Generate response within a Q&A model.I/P: RefinedPrompt O/P: IntelligentResponse Proceeser: LLM like ChatOpenAI()\n        chat_context = qa.run(refined_prompt) \n    except Exception as e:\n        print(f'RetrievalQA exception: {e}') #Most of the time it throws error due to unavailability of llm engine or invalid subscription\n\n    return (chat_context)"
  },
  {
    "threadId": "1186172195379089498",
    "name": "How to migrate qdrant  to a new 3-node server",
    "messages": "Hi team:\nThere is already a 3-node Qdrant cluster, and all 3 nodes are running and started by Docker using docker-compose.\nNow we need to migrate this node to a new 3-node server. \nWhat is the best practice?"
  },
  {
    "threadId": "1186352450874134679",
    "name": "Count with limit and offset",
    "messages": "I'd love to be able to implement on my Ui something similar to a google search (see photo). I know that qdrant supports limit and offset, but is there a way to get a count of total results that fall within my specified `score` and `filter` requirements without fetching all. I'd like to specify a `limit` for performance so the client doesn't return everything\n\nThank you!"
  },
  {
    "threadId": "1151256055037317151",
    "name": "k-means clustering?",
    "messages": "New to Qdrant -- so far really liking everything!  I created a demo for work and was asked if there was a method to assign points to a nearest clusters like in k-means clustering.  Then on search it could return the cluster id the encoded search doc?"
  },
  {
    "threadId": "1186283422155546726",
    "name": "Creating a related word search",
    "messages": "Hi, I have have extracted embeddings from X amount of words and stored them in QDrant. Now I want to make a word recommendation API that takes a list of words as a request and returns a the highest match from QDrant. I am thinking of 2 ways of doing it. 1. Extract embeddings for each individual word, calculate average vector and use Search API for that vector. 2. Concatenate the words like \"WORDA WORDB\", extract embedding and use it with Search API. Which method is preferable in this case?"
  },
  {
    "threadId": "1182305966247657575",
    "name": "Failure while upserting points, it shows collection not found. Where as collection is present",
    "messages": "{subprocess.py:93} INFO - Arguments: (UnexpectedResponse(),)\n{subprocess.py:93} INFO - Traceback (most recent call last):\n {subprocess.py:93} INFO -   File \"/home/airflow/qdrant_ingestor/utils/qdrant_utils.py\", line 200, in upsert_to_qdrant\n[2023-12-07, 10:50:41 UTC] {subprocess.py:93} INFO -     qdrant_client.upsert(collection_name=f\"{collection_name}\", points=points, wait=True)\n {subprocess.py:93} INFO -   File \"/lib/python3.10/site-packages/qdrant_client/qdrant_client.py\", line 747, in upsert\n{subprocess.py:93} INFO -     return self._client.upsert(\n{subprocess.py:93} INFO -   File \"/lib/python3.10/site-packages/qdrant_client/qdrant_remote.py\", line 1088, in upsert\n{subprocess.py:93} INFO -     http_result = self.openapi_client.points_api.upsert_points(\n {subprocess.py:93} INFO -   File \"/home/airflow/.local/lib/python3.10/site-packages/qdrant_client/http/api/points_api.py\", line 1160, in upsert_points\n{subprocess.py:93} INFO -     return self._build_for_upsert_points(\n {subprocess.py:93} INFO -   File \"/home/airflow/.local/lib/python3.10/site-packages/qdrant_client/http/api/points_api.py\", line 586, in _build_for_upsert_points\n{subprocess.py:93} INFO -     return self.api_client.request(\n{subprocess.py:93} INFO -   File \"/home/airflow/.local/lib/python3.10/site-packages/qdrant_client/http/api_client.py\", line 68, in request\n{subprocess.py:93} INFO -     return self.send(request, type_)\n{subprocess.py:93} INFO -   File \"/home/airflow/.local/lib/python3.10/site-packages/qdrant_client/http/api_client.py\", line 91, in send\n{subprocess.py:93} INFO -     raise UnexpectedResponse.for_response(response)\n{subprocess.py:93} INFO - qdrant_client.http.exceptions.UnexpectedResponse: Unexpected Response: 404 (Not Found)\n {subprocess.py:93} INFO - Raw response content:\n{subprocess.py:93} INFO - b'{\"status\":{\"error\":\"Not found: Collection `embeddings_2023-12-07` doesn\\'t exist!\"},\"time\":0.000548641}'"
  },
  {
    "threadId": "1196739136409849876",
    "name": "Retrieve last ID",
    "messages": "Hi! \n\nCould anybody suggest how to retrieve the last ID from Qdrant DB?\nAssuming that IDs are numeric and start from 0 or 1 at the first record.\n\nMany thanks,\nJanis"
  },
  {
    "threadId": "1185209577680154694",
    "name": "Alternative to PYPDF/PYPDF2 for Extracting text from PDFs ?",
    "messages": "I have ariund 700+ PDFs. I am using Langchain with Python PYPDF2 to transfer the text into Qdrant. Now my issue is PYPDF2 (or even PYPDF) is not working properly, i mean from a PDF of 150 Pages, it is collecting a TEST of no more than 10 to 20 pages, and some random alphabats or \"\\n\".\n\nI want to know, that is there any other Python Library you know, which can help me extract text accurately from PDFs ?"
  },
  {
    "threadId": "1185249828779855984",
    "name": "quantization PUT vs PATCH",
    "messages": "Is it possible to enable quantization on an existing collection?\nIf so pl provide a sample PATCH curl"
  },
  {
    "threadId": "1184610130508120217",
    "name": "Can't get to work with Langchain",
    "messages": "Hey there! I am trying to store my vectors using the Unstructured Document Loader, using the TextSplitter, and loading them into Qdrant with OpenAI Embeddings. I am following this tutorial:\n\nhttps://python.langchain.com/docs/integrations/vectorstores/qdrant\n\nBut when I try and upload (this works locally but not on DigitalOcean cloud) I get the error:\n\n```py\nvector_size = len(partial_embeddings[0])\n\nIndexError: list index out of range\n```\n\nThis is what my code looks like:\n\n```py\nloader = DirectoryLoader(\n                path=temp_path,\n                show_progress=True,\n                silent_errors=False,\n                use_multithreading=True\n            )\n            documents = loader.load()\n            num_docs = len(documents)\n            text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n            items = text_splitter.split_documents(documents)\n            embeddings = OpenAIEmbeddings()      \n            Qdrant.from_documents(\n                items,\n                embeddings,\n                url=QDRANT_API_URL,\n                # prefer_grpc=False,\n                api_key=QDRANT_API_KEY,\n                https=False,\n                collection_name=f\"{user_id}-{model_data.id}\",\n                timeout=1000,\n                force_recreate=True,\n            )\n```\n\nAny help would be appreciated, thanks."
  },
  {
    "threadId": "1184282656817557544",
    "name": "Permissions for specific collections",
    "messages": "When connecting to Qdrant's server, is it possible to limit the collections that each user can edit?"
  },
  {
    "threadId": "1184525489818316861",
    "name": "Using sparse vectors in rust",
    "messages": "I want to use qdrant sparse vectors in rust to replace our current tantivy full text implementation and combine our embeddings w/ our full text, but was wondering how I would be able to set up splade in Rust without having to use python bindings. Any help would be appreciated. Thank you!"
  },
  {
    "threadId": "1184393152489082910",
    "name": "Different vectors under same embeddings config",
    "messages": "Suppose i configure my collection to have :\n```\n{'name': 'vectors', 'size': 768}\n```\nAnd i insert 2 set of different embeddings under the same embedding object, say :\ngte-base for 100 points (with payload field : 'version': 1) and bge-base for another 100 points (with payload field : 'version': 2) . \n\nWould this create a problem while creating the HNSW index? Or it should work fine as long as we filter based on payload before search?"
  },
  {
    "threadId": "1195089940632047707",
    "name": "How does Qdrant do exception handling in database and trigger?",
    "messages": "In college, our professor gave us a project that we need to use a vector database and we chose Qdrant. But he said that we have to do exception handling in the database(not doing try except in back-end) and triggers like in SQL. But I don't know if these things are possible in Qdrant."
  },
  {
    "threadId": "1194925384156848158",
    "name": "Json file and upsert error",
    "messages": "Hello everyone. I've a problem: I've a Json file, with nested elements. When I put it as parameter to upsert method, give me an error. \"error\":\"Format error in JSON body: data did not match any variant of untagged enum PointInsertOperations\".\nWhat is the problem?? \nI see that when I read the Json Object with Python, i find it with single quote."
  },
  {
    "threadId": "1194966786584617061",
    "name": "`retrieve()` vs `filter()` for getting an exact list of point ids",
    "messages": "Hey! \n\nI am curious if there is any performance difference in using these two methods, where `retrieve()` would grab the point ids, and `filter()` would filter on a metadata id list with `MatchAny`\n\nAny insights would be helpful!\n\nThanks,\n\nRob"
  },
  {
    "threadId": "1182867606751940640",
    "name": "Cluster",
    "messages": "How to configure the cluster in such a ways where i can get 25K write in a batch or parallel thread."
  },
  {
    "threadId": "1184094158219132968",
    "name": "recover_snapshot: Failed to download snapshot and returns 403 Forbidden",
    "messages": "I tried to recover a collection from a snapshot but got this response:\n```\nUnexpectedResponse: Unexpected Response: 400 (Bad Request)\nRaw response content:\nb'{\"status\":{\"error\":\"Wrong input: Failed to download snapshot from\nhttp://localhost:6333/collections/xxx/snapshots/xxx.snapshot: status - 403 Forbidden\"} ...'\n```\n\n```\nclient = qdrant_client.QdrantClient(\n    host=config.QDRANT_DB_URL,\n    port=config.QDRANT_DB_PORT,\n    grpc_port=config.QDRANT_DB_GRPC_PORT,\n    prefer_grpc=True,\n    https=True,\n    verify=True,\n    timeout=120.0,\n    api_key=os.getenv(\"QDRANT_API_KEY\"),\n)\n\nresult = client.recover_snapshot(\n        collection_name=collection_name,\n        location=f\"http://localhost:6333/collections/{collection_name}/snapshots/{snapshot_name}\",\n    )\n```\nMaybe the API key was not used when downloading the snapshot?"
  },
  {
    "threadId": "1194941674007298078",
    "name": "Recover snapshot from local file in docker",
    "messages": "Hi, I'm trying to play with the practice datasets shared by Qdrant recently.\nSince the file is very large, recovering the snapshot from the url was not possible.\nI've downloaded the snapshot file and placed it in the qdrant_storage folder that I initialized the qdrant container with.\nI'm not able to figure out which path to provide in the recover API call.\nThis is what I tried:\n```\nPUT /collections/wolt_food/snapshots/recover\n{\n  \"location\": \"file:///wolt-clip-ViT-B-32-2446808438011867-2023-12-14-15-55-26.snapshot\"\n}\n```\nCan someone help me out?"
  },
  {
    "threadId": "1181717323455938703",
    "name": "How can i calculate the cost of  deploying qdrant into my own cloud(AWS.etc)",
    "messages": "I have been asked from my manager to discuss the cost of building the following \n1. Semantic search engine for car images and car parts\n2. Semantic search but for words not images \n3. Recommendation system \nI love qdrant and want to use it in  the production,  for our business case i must declare some baseline for the projects before starting working on them \nI don't have enought experience with cloud computing, so can someone give any guidence to manage this discussion, i mainly want to use qdrant but their cloud price for my region and the manager i work is really high!!!"
  },
  {
    "threadId": "1183694181311447111",
    "name": "FastEmbed L6 comparison",
    "messages": "i did a comparison of fast-embed l6 and sbert ( generic ) , \n\nand this is what i concluded ,\nspeed wise it was working well , but when i checked the cosine_similarity of both and compared the result was not good . \n\nalso to understand better fast-embed , i have concluded in fast-embed we are using quantised model weights and ONNX runtime and some CPU parallization . from model side the only thing to speed up is that the use of quantised model ?"
  },
  {
    "threadId": "1183608298939879485",
    "name": "Shard transfer while inserting points",
    "messages": "I want to use new shard transfer api in 1.7.0, to tranfer all nodes of my cluster to another physical machines. So I will use this api in iteration of all shards on all old nodes, and transfer them to new peer.\n```\nPOST /collections/{collection_name}/cluster\n{\n    \"move_shard\": {\n        \"shard_id\": 0,\n        \"from_peer_id\": 381894127,\n        \"to_peer_id\": 467122995,\n        \"method\": \"snapshot\"\n    }\n}\n```\nThe question is, while transfering, the application is still inserting new points. During transfering, the new added points will add to shard in old peer, or shard in new peer (trasfer to), or lost? After the transfer completed, should I tranfer the points added during tranfer manually to new peerя╝Я"
  },
  {
    "threadId": "1181818895485644960",
    "name": "Timeout Error?",
    "messages": "I'm running into an issue when trying to upload a new set of embeddings where Qdrant appears to be timing out. When I look at my cluster, I can see that there is a log that reads:\n\n[2023-12-06T04:43:44.153Z WARN qdrant::actix::helpers] error processing request: Failed to propose operation: leader is not established within 10 secs\n\nAny clue what is causing this and how to fix it?"
  },
  {
    "threadId": "1183464297360281690",
    "name": "BAAI/bge-large-en-v1.5 Permission Denied",
    "messages": "Hi! I am trying to follow this notebook, but with the large model, https://qdrant.github.io/fastembed/examples/Retrieval_with_FastEmbed/#setup\nI get:\nPermissionError                           Traceback (most recent call last)\nFile c:\\Users\\doug\\miniconda3\\envs\\embeddings\\Lib\\site-packages\\fastembed\\embedding.py:339, in Embedding.retrieve_model(self, model_name, cache_dir)\n    338 try:\n--> 339     self.download_file_from_gcs(\n    340         f\"https://storage.googleapis.com/qdrant-fastembed/{fast_model_name}.tar.gz\",\n    341         output_path=str(model_tar_gz),\n    342     )\n    343 except PermissionError:\n\nFile c:\\Users\\doug\\miniconda3\\envs\\embeddings\\Lib\\site-packages\\fastembed\\embedding.py:245, in Embedding.download_file_from_gcs(cls, url, output_path, show_progress)\n    244 if response.status_code == 403:\n--> 245     raise PermissionError(\n    246         \"Authentication Error: You do not have permission to access this resource. Please check your credentials.\"\n    247     )\n    249 # Get the total size of the file\n\nPermissionError: Authentication Error: You do not have permission to access this resource. Please check your credentials.\n\nI have no issue downloading the model using 'from FlagEmbedding import FlagModel' instead of 'from fastembed.embedding import FlagEmbedding as Embedding'. Any help? Thanks!"
  },
  {
    "threadId": "1192557531415982100",
    "name": "points",
    "messages": "Hi!\n\nIs there any way to move specific points from one collection to another? I haven't found anything related. Thanks!"
  },
  {
    "threadId": "1183155478709141565",
    "name": "Does memmap_threshold also affect payload indexes?",
    "messages": "Our points carry a fairly large payload to serve different querying scenarios. We are storing them on disk but it's impossible to query efficiently without an index on them. \n\nThe payload indexes are as large as the (binary quantized) vector index on RAM right now."
  },
  {
    "threadId": "1183045735206359120",
    "name": "metadata and document filtering",
    "messages": "hi there, here my requirement :\n1- perform filtering on two metadata fields:\n- ingredients: List[str]\n- tags : List[str]\ni want to perform filterings like :\n $not_contains: {\"ingredients\":[...]}\n $contains: {\"tags\":[\"...\"]}\ni checked out source code in both langchain and your official repo but it was so messy. I would really appreciate if you can help me to find out the documentation related to filtering."
  },
  {
    "threadId": "1183087243347374131",
    "name": "Spark connector",
    "messages": "Is this on the roadmap for qdrant?"
  },
  {
    "threadId": "1183071330367717570",
    "name": "startup with a large collection(200 million+ vectors)",
    "messages": "If \"hnsw_config\".\"on_disk\" is set as true vs false how does it impact qdrant startup timings ??\nBoth vectors and payload are on disk for us."
  },
  {
    "threadId": "1181919585612595270",
    "name": "How to provide unique name to unnamed vectors of a collection",
    "messages": "I have several collections in production with text embedding, which was not named in time of collection creation. Now, I need to add image vector to the records.\nFrom doc:\n```\nIt is possible to have multiple vectors per record. This feature allows for multiple vector storages per collection. To distinguish vectors in one record, they should have a unique name defined when creating the collection. \n```\nSo, how can I add name to my previously unnamed vector?\n\nAt another page I see unnamed vectors can be addressed as `\"\"` [link](https://qdrant.tech/documentation/concepts/collections/#update-vector-parameters) for vector field parameter update. Does this policy holds for other operations like `search` `scroll`"
  },
  {
    "threadId": "1182608509943152650",
    "name": "How to replicate full data to new node without changing replication_factor?",
    "messages": "I have a cluster with 2 peers:\n\n```json\n{\n  \"result\": {\n    \"status\": \"enabled\",\n    \"peer_id\": 7589863719433209,\n    \"peers\": {\n      \"7283158285224246\": {\n        \"uri\": \"http://emb-sharded-qdrant-1.emb-sharded-qdrant-headless:6335/\"\n      },\n      \"7589863719433209\": {\n        \"uri\": \"http://emb-sharded-qdrant-0.emb-sharded-qdrant-headless:6335/\"\n      }\n    },\n    \"raft_info\": {\n      \"term\": 7180,\n      \"commit\": 65,\n      \"pending_operations\": 0,\n      \"leader\": 7589863719433209,\n      \"role\": \"Leader\",\n      \"is_voter\": true\n    },\n    \"consensus_thread_status\": {\n      \"consensus_thread_status\": \"working\",\n      \"last_update\": \"2023-12-08T08:58:58.038092706Z\"\n    },\n    \"message_send_failures\": {}\n  },\n  \"status\": \"ok\",\n  \"time\": 7.265e-06\n}\n```\n\nMy collection:\n\n```\nnext message\n```\n\nand I want to replicate full data (with incremental updates) to 3 nodes (to another, to a third). Is it possible without recreating the collection? How many options do I have in this case?\n\nQdrant version: 1.4.0, kubernetes installation, self-hosted"
  },
  {
    "threadId": "1182305940624642078",
    "name": "There is a way for counting payload fields in collection ?",
    "messages": "My points in collection got an field wich is not unique and is used for grouping data according this field.\nI need to know how many different values existing for this field in the collection.\n\nThere is a way for doing this ?"
  },
  {
    "threadId": "1180476129669292103",
    "name": "qdrant_client Exception",
    "messages": "I am trying to insert multiple vectors into my collection asynchronusly using the below code\n\n```\nawait qdrant_client.upsert(collection_name=collection_name, points=points)\n```\nI am getting the follwing error:\n```\nraise ResponseHandlingException(e)\nqdrant_client.http.exceptions.ResponseHandlingException\n```\nThe exception does not include the reason so it's hard to debug. I tried to catch the exception and print as well using try catch but didn't work:\n```\ntry:\n        await qdrant_client.upsert(collection_name=collection_name, points=points)\n\n    except Exception as e:  # Catching a general exception\n        # Log the type and message of the exception\n        log.info(f\"Caught an exception of type: {type(e)}\")\n        log.info(f\"Exception message: {e}\")\n\n        # Log additional details if available\n        if hasattr(e, \"response\"):\n            log.info(f\"Response content: {e.response.content}\")\n        if hasattr(e, \"body\"):\n            log.info(f\"Response body: {e.body}\")\n        if hasattr(e, \"headers\"):\n            log.info(f\"Response headers: {e.headers}\")\n\n        # Re-raise the exception if you want it to propagate\n        raise e\n```"
  },
  {
    "threadId": "1182207675849777192",
    "name": "Payment Failed Error",
    "messages": "Hi Guys, I am not able to  send the data to the DB and I am getting that the Database failed because of the payment. But I see all the payment as paid. I need some help on that. The service is in production."
  },
  {
    "threadId": "1171388233599438878",
    "name": "How to use multilingual full text index search ?",
    "messages": "Hi teamя╝Мaccording to the document description, I need to build a new qdrant version, but I'm not quite sure how to build it. Could you please provide a standard build command?\nthanksЁЯШД"
  },
  {
    "threadId": "1182195697806737470",
    "name": "`using` for vector selection with search endpoint",
    "messages": "I want to make searches into a multi-vector collection. `recommendation` endpoint has support for a keyword `using` to select a vector. \nHow can I get the same functionality with `search` endpoint?"
  },
  {
    "threadId": "1182098304159469579",
    "name": "Answers with specific format",
    "messages": "Hi, I've built a chatbot integrating Qdrant, and it's the best-performing chatbot I've ever created! That's amazing, congratulations on your service!\n\nThe bot correctly answers almost 100% of the questions, but I would like the answers to be in a specific format.\n\nFor example, if a user asks, 'what are the suggested chinese restaurants?', the bot currently responds:\n\nThe suggested Chinese restaurants in Toronto are Mandarin Restaurant (ЁЯУЮ: +14164862222, ЁЯУН: https://maps.app.goo.gl/5w5n73xbAG9AasYW9), Hong Shing Restaurant (ЁЯУЮ: +14169773338, ЁЯУН: https://maps.app.goo.gl/6cUxj32T4j1n4ZkL7), and Yueh Tung Restaurant (ЁЯУЮ: +14169770933, ЁЯУН: https://maps.app.goo.gl/3y3dgFTtc2tyrHYo9).\n\nAs you can see, the answer format is a bit confusing. So, I built a function to format these answers in the following way:\n\nThe suggested Chinese restaurants in Toronto are:\n1) Mandarin Restaurant\nЁЯУЮ: +14164862222\n ЁЯУН: https://maps.app.goo.gl/5w5n73xbAG9AasYW9\n2) Hong Shing Restaurant \nЁЯУЮ: +14169773338\n ЁЯУН: https://maps.app.goo.gl/6cUxj32T4j1n4ZkL7\n3) Yueh Tung Restaurant\nЁЯУЮ: +14169770933\nЁЯУН: https://maps.app.goo.gl/3y3dgFTtc2tyrHYo9\n\nI built a functions that I will paste in the next message\n\nUnfortunately, the bot only formats the answer correctly in a few cases. In most instances, there are errors, or it provides a discursive answer like in the provided example.\n\nDoes anyone know how I could modify the function to work in every case?\n\nHere is the related part of the knowledge base, for reference:\n\nSuggested Chinese Restaurants in Toronto:\n    1) Mandarin Restaurant\n    Phone: +14164862222\n    Location: https://maps.app.goo.gl/5w5n73xbAG9AasYW9\n    2) Hong Shing Restaurant\n    Phone: +14169773338\n    Location: https://maps.app.goo.gl/6cUxj32T4j1n4ZkL7\n\n    3) Yueh Tung Restaurant\n    Phone: +14169770933\n    Location: https://maps.app.goo.gl/3y3dgFTtc2tyrHYo9\n\n\nIs there such a strategy to build the function and obtain the desired format?\n\nThank you in advance"
  },
  {
    "threadId": "1181752947298799679",
    "name": "search stabilization",
    "messages": "Let's say I throw 1000 points into a collection and then immediately search it. Is there a period of time where search quality is not 100%?\n\nThere was a report that came in from a user that the ranking was not what he expected then he waited awhile and the ranks went back to normal.\n\nIf that's normal and expected what collection or search parameters control the length of time that it takes it get to optimal search quality?"
  },
  {
    "threadId": "1181989614270038056",
    "name": "Effect of quantization on insert throughput",
    "messages": "What is the effect of quantization on insert throughput? In case of scalar quantization , are the stored vectors consisting of int8 only? That would mean a decrease in IO as compared to float32? Or are both int8 and float32 stored to enable rescore? If one doesnt want rescore, is it possible to save just int8 and possibly increase insert throughput??"
  },
  {
    "threadId": "1181866555072585870",
    "name": "How to recover the pod restarted when deploy in k8s?",
    "messages": "I want to deploy 3 nodes in cluster, as a scale out to expand the memory and disk resources. I don't mount any directories to my host machine, so the data in container will lose after it killed or restarted.\n\nAnd I start my first pod with `--bootstrap` param, and start other 2 pods with `--uri` param to join the cluster. And create a collection with 6 shards and 2 replicas.\n\nI think if I restart one of these 3 pods, the pod which restarted will lose all its data, but cause of my 2 replicas and other 2 nodes in cluster(it will store the whole collection), the pod restarted will recovery and retrieve the storage. But it eventually joins the cluster as a new pod. And the pod before restarted lost enternally. So the whole cluster becomes unstable.\n\nSo how can I deploy my cluster, so that it can be recovered even one pod in it restarts or crashed."
  },
  {
    "threadId": "1181519167573217290",
    "name": "Slow query on a small collection",
    "messages": "I create a collection for image search. It only has less than 10 points. It has a payload `enable` in type integer to mark its status.\nThe collection is store in a cluster with 3 nodes.\n\nThe Question is, my task will execute query (include a vector and enable) at a frequency of 10~qpm, most of them cost less than 5ms, but some of them cost several seconds. I don't know what happend in these long time cost querys. And during them occurred, there's no cpu usage suddenly increase, does it mean there's no background task in qdrant such as indexing or deleting.\n\nThat's the config of collection, the cluster config of collection and the config of cluster. All copied from webUI.\n\nThe image top is the time cost when query(p95, sampled every minute), down is query executed counts(in one minute).\n\nthe config of collection\n\n```\n{\n  \"result\": {\n    \"status\": \"green\",\n    \"optimizer_status\": \"ok\",\n    \"vectors_count\": 9,\n    \"indexed_vectors_count\": 0,\n    \"points_count\": 8,\n    \"segments_count\": 24,\n    \"config\": {\n      \"params\": {\n        \"vectors\": {\n          \"size\": 512,\n          \"distance\": \"Dot\",\n          \"on_disk\": true\n        },\n        \"shard_number\": 6,\n        \"replication_factor\": 2,\n        \"write_consistency_factor\": 1,\n        \"on_disk_payload\": true\n      },\n      \"hnsw_config\": {\n        \"m\": 16,\n        \"ef_construct\": 128,\n        \"full_scan_threshold\": 10,\n        \"max_indexing_threads\": 3,\n        \"on_disk\": false\n      },\n      \"optimizer_config\": {\n        \"deleted_threshold\": 0.2,\n        \"vacuum_min_vector_number\": 1000,\n        \"default_segment_number\": 0,\n        \"max_segment_size\": null,\n        \"memmap_threshold\": 20000,\n        \"indexing_threshold\": 10000,\n        \"flush_interval_sec\": 5,\n        \"max_optimization_threads\": 2\n      },\n      \"wal_config\": {\n        \"wal_capacity_mb\": 32,\n        \"wal_segments_ahead\": 0\n      },\n      \"quantization_config\": null\n    },\n    \"payload_schema\": {}\n  },\n  \"status\": \"ok\",\n  \"time\": 0.00054787\n}\n```"
  },
  {
    "threadId": "1169041863454490735",
    "name": "Checking memory usage of a collection",
    "messages": "Is there a way I can check how much memory a collection is using? I can easily check overall the amount of memory by measuring the container but I haven't found a way to check an individual collection .Thanks!"
  },
  {
    "threadId": "1181069204972118087",
    "name": "Failed to restart qdrant cloud",
    "messages": "Hi I'm getting this error on qdrant cloud\n\n```\n2023-12-04T03:03:04.990235Z ERROR qdrant::startup: Panic occurred in file /qdrant/lib/collection/src/shards/replica_set.rs at line 545: Failed to load local shard \"./storage/collections/github-autogen/0\": Service internal error: RocksDB open error: IO error: No space left on device: While appending to file: ./storage/collections/github-autogen/0/segments/6dd6fde0-20b6-43dd-b08e-22bedca8b610/MANIFEST-001038: No space left on device\n```"
  },
  {
    "threadId": "1181536733570027571",
    "name": "Perform vectorsearch on disk, and reduce RAM usage.",
    "messages": "Hello, Is it possible to perform vector search on disk without having to load the vectordb into RAM? I have tried using \"on_disk=True\" in the create_collection function \n\n    qdrant.create_collection(\n        collection_name=collection_name,\n        vectors_config=models.VectorParams(size=embedding_size,,\n                                           distance=models.Distance.COSINE),\n        optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000),\n        hnsw_config=models.HnswConfigDiff(on_disk=True),\n    )\n\n\n but I am unsure if this actually loads the entire vectordb into RAM during search. I have a few questions regarding this:\nCan we perform the search and retrieve results directly from disk?\nIf not, is there a way to limit the number of collections being loaded from disk?\nHow can we remove a collection from RAM after performing a search, to prevent it from taking up space continuously?"
  },
  {
    "threadId": "1181611522846367784",
    "name": "Cluster fails to establish a new leader",
    "messages": "Our adrant cluster version: 1.3.2 went to intresting state, where it is not able to establish a leader, all operations related to cluster are failing with message:\n```\n    \"status\": {\n        \"error\": \"Service internal error: Failed to propose operation: leader is not established within 10 secs\"\n    },\n```\n\nIn one of the node logs, we are seeing a lot of repeating errors like these:\n```\n[2023-12-05T14:51:14.554Z ERROR qdrant::consensus] Failed to forward message\"\" to message sender task 2348563927505599: message sender task queue is full. Message will be dropped.\n```\n\nand then in node `2348563927505599` we see different errors, example:\n\n```\n2023-12-05T14:56:23.574Z WARN  storage::content_manager::consensus_manager] Failed to send message to http://qdrant-prod-1.qdrant-prod-headless:6335/ with error: Error in closure supplied to transport channel pool: status: DeadlineExceeded, message: \"Timeout 1000ms reached for uri: http://qdrant-prod-1.qdrant-prod-headless:6335/\", details: [], metadata: MetadataMap { headers: {} }\n```"
  },
  {
    "threadId": "1181308913057021962",
    "name": "Quantization questions",
    "messages": "Hi, I have two questions regarding quantization:\n1. At what point do you determine the the quantization bounds (if, for example, you set quantile to 99%)? Say I ingest 1000 vectors and then later ingest 1000 - Are the bounds reassessed and the quantized embeddings regenerated when I add the second 1000?\n2. If we create our own quantized embeddings is it possible to ingest as ints? Or can quantization only happen on Qdrant side?"
  },
  {
    "threadId": "1190036051205554287",
    "name": "Error when upserting too many points at once with Python AsyncQdrantClient",
    "messages": "I am using the `AsyncQdrantClient` python client, and I am trying to upsert ~1000 points/vectors with some payload via `AsyncQdrantClient.upsert(...)` . After a short while, I am getting a `ResponseHandlingException`. I've attached a picture of the stack trace.\n\nIs there a limit on the client side on how many points can be upserted at once? I've decreased the number of points to ~300 and then it worked again, but anything much higher raises the error."
  },
  {
    "threadId": "1155671139545264189",
    "name": "Can I use qdrant with Azure AppService?",
    "messages": "I can only use AppService due to requirements issues. I am looking for a stable way to run qdrant on AppService on Azure.\n\nI used the following example first. I decided to mount AzureFiles in AppService.\nтА╗ ARM is Container Apps, but only there changed to AppService.\nhttps://github.com/Azure-Samples/qdrant-azure\n\nWhen I tried it, I could write and read without any problem. However, it took about five times longer to read/write than when the system was built on a virtual machine.\nтА╗ As a way to isolate the problem, I tried using AzureFiles without mounting it, but it did not cause any delays.\nтА╗ The mode of Azure Files was set to Transaction Optimized.\n\nUpon further research, it was stated that databases such as sqlite should not be used in this manner.\nтА╗ I don't know if the same problem occurs with RocksDB and sqlite used in qdrant...\nhttps://learn.microsoft.com/en-us/azure/app-service/configure-connect-to-azure-storage?tabs=basic%2Ccli&pivots=container-linux#best-practices\n\nIs it not recommended to use qdrant with Azure AppService? If anyone knows of any good way to build one, please help...\n\n-- Some assumptions ---\n* We are currently using qdrant by pulling it from docker hub.\n* The destination of the files created by qdrant is specified to the directory mounted by AzureFiles.\n* I contacted Microsoft, but they did not have qdrant-specific information."
  },
  {
    "threadId": "1181442342411059250",
    "name": "Source installation error",
    "messages": "I'm using QDrant for a college project, my professor asked me to use the source installation, I've already tried on windows and didn't accomplished it, I'm trying on Linux Ubuntu 22.04 and I'm having a problem with protoc dependency, I hope someone can help me fix it"
  },
  {
    "threadId": "1181315784396525640",
    "name": "Rust client TransportError",
    "messages": "I have a Tauri (Rust) desktop app that uses a local qdrant for vector storage. Even after waiting for the gRPC channel message and a health check from the server I still get the following error when later in the code I'm trying to use my generated rust client object:\n\n```Err(status: Internal, message: \"Failed to connect to http://localhost:6334/: transport error\", details: [], metadata: MetadataMap { headers: {} })\n```\nDoes anyone have any suggestions for what the issue might be? This only occurs on a subset of the machines I'm testing with."
  },
  {
    "threadId": "1182141299743268935",
    "name": "Help with slow query speed when first executed",
    "messages": "I have a collection with 8 millon points in product env. When I execute search by vector and other filter, the first query will be very slow, more than 10secs, and the following will be faster.\n\nThat's my search request body:\n```\nPOST http://10.19.76.14:6333/collections/t_feed_image_hot/points/search\n{\n   \"vector\" : [...a 512 dim vector],\n  \"filter\": {\n    \"must\": [\n      {\n        \"key\": \"time_id\",\n        \"range\": {\n          \"lt\": 1701770169349,\n          \"gt\": 1693907769000\n        }\n      }]\n  },\n  \"limit\": 100,\n  \"offset\": 0,\n  \"with_payload\": false,\n  \"score_threshold\": 0.8\n}\n\n```\n\n\nAnd I found my disk util reached 100% for over 10 seconds when I executed search query, that one result of iostat. So I guess qdrant read many tiny files when query? But repeat this query will NOT cause this, disk util rate remains 0%.\n```\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rMB/s    wMB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdb               0.00     0.00 1353.00    0.00     5.29     0.00     8.00     2.99    2.02    2.02    0.00   0.74 100.10\n```\n\nI try to replicate this question in dev environment, I create a collection with same structure and fill over 5 million points in it. But the time cost of search is below 200ms in average. And there's no disk busy like product env.\n\nI guess there are some reasons may cause this:\n- Some of points contains a large payload (a list property with more than 3000 elements) \n- Quering when adding new points to collection(usually 15 points per minute), maybe them are not indexed and must be readed from disk. (There's no adding new points in dev environment)\n\nSo what causes this? Thx for your answer or other possible reasons"
  },
  {
    "threadId": "1183930788941668382",
    "name": "Dead shards on cluster after rolling restart",
    "messages": "I'm running into a problem where a rolling restart of my qdrant cluster on kubernetes deployed with your helm chart is triggering shards to be resynced. For some reason its very reproducable on the production cluster (12M points, 6 nodes), but not on the dev cluster (2M points, 4 nodes). All shards have 2x replicas total\n\nWhat happens:\n- trigger a restart, qdrant-5 restarts\n- after fully being fully restarted, qdrant-4 restarts next. Some where in this transition, all of qdrant-5's shards are marked as Dead. They start syncing\n- restart continues, a similar thing happens to qdrant-4's shards\n- this continues, where it seems like a shard is not marked dead if its the lone replica left\n\nNote that qdrant-5 is `7728786804883842` and qdrant-4 is `7563199243562072`\n\nAny idea why this is happening, and ideas to prevent it from happening?"
  },
  {
    "threadId": "1184939325704646766",
    "name": "Pls check my rust code to generate splade embeddings",
    "messages": "I am trying to use the rust client to upsert sparse vectors and I am getting the error `Wrong input: Conversion between sparse and regular vectors failed` This is probably bc of the code I am using to generate the sparse vectors. It would be much appreciated if someone could verify my code. I am using huggingface candle to interface with the model and work with tensors."
  },
  {
    "threadId": "1184531852166451211",
    "name": "Qdrant Docker Container Dying When Upserting Sparse Vectors",
    "messages": "Hi I am trying to upsert sparse vectors through the python qdrant-client. Every time I initiate the upload the container dies with exit code 7.\n\nHere is a an example of what I am doing. sparse_embeddings is a tuple with id and a dict for the sparse embedding. e.g.\n```\n(\"123\", {\"indices\": [1, 2, 3, ... 100], \"values\": [0.32, 0.46, 0.64, ... 0.353]}\n```\n\n```\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import SparseVectorsParams, SparseVector, PointStruct\n\nsparse_embeddings = get_sparse_embeddings(content)\npoints = [PointStruct(id=sparse_embedding[0], payload={}, vector={\"sparse\": SparseVector(indices=sparse_embedding[1][\"indices\"], values=sparse_embedding[1][\"values])} for sparse_embedding in sparse_embeddings]\n\nqd_client = QdrantClient(\"localhost\", https=False)\nqd_client.create_collection(collection_name=\"test\",\n                            vectors_config={},\n                            sparse_vectors_config={\"sparse\": SparseVectorParams()})\n\nprint(qd_client.upsert(\"test\", points=points, wait=True))\n```\n\nThe docker contianer is dying with this line being printed out.\n\n```\n./entrypoint.sh: line 25:    7 Killed              ./qdrant $@\n```\n\nClient side I am getting\n \n```\nUNKNOWN:Error received from peer {grpc_message:\"Socket closed\", grpc_status:14, ...\n```\n\nIs there anything glaringly wrong with what I am doing? I am using qdrant-client 1.7.0 and qdrant version 1.7.2. Thank you for your time!"
  },
  {
    "threadId": "1187045674773135551",
    "name": "DiskANN",
    "messages": "Qdrant offers disk storage options . Recently research was conducted on some good nearest neighbour search algortihms for disks like DiskANN which can prove to give better results compared to hnsw  . Can we collaborate and implement this backed by proper testing  ?"
  },
  {
    "threadId": "1190011336650919977",
    "name": "Help with pull request",
    "messages": "I made a full request (https://github.com/qdrant/quaterion/pull/213). It shows that some checks were unsuccessful, can anyone help me know what I did wrong.\nI am new to open source, my apologies if I made some silly errors"
  },
  {
    "threadId": "1190026818955255828",
    "name": "handler is closed",
    "messages": "I've been trying to use the async python client with llama_index.  All is fine for the first query, but all subsequent queries fail with \"the handler is closed\" for http requests or \"event loop is not running\" when using GRPC.\n\nAn example partial trace is as follows:\n\n```\n  File \"/home/dustin/.cache/pypoetry/virtualenvs/llm-bot-RYr_jjlf-py3.11/lib/python3.11/site-packages/llama_index/vector_stores/qdrant.py\", line 564, in aquery\n    response = await self._aclient.search(\n--------\nINNER STACK TRACE REDACTED FOR BREVITY\n--------\n  File \"/home/dustin/.cache/pypoetry/virtualenvs/llm-bot-RYr_jjlf-py3.11/lib/python3.11/site-packages/qdrant_client/http/api_client.py\", line 195, in __call__\n    return await call_next(request)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dustin/.cache/pypoetry/virtualenvs/llm-bot-RYr_jjlf-py3.11/lib/python3.11/site-packages/qdrant_client/http/api_client.py\", line 175, in send_inner\n    raise ResponseHandlingException(e)\nqdrant_client.http.exceptions.ResponseHandlingException: unable to perform operation on <TCPTransport closed=True reading=False 0x7fdbd309d4b0>; the handler is closed\n```\n\nIt seems like the connection is getting cleaned up after the first query, then the client is trying to either close the already closed connection again or reuse the closed connection.  I haven't dug into the code deep enough to figure out the exact issue, but was hoping that this might be a known issue with a workaround.\n\nThanks!"
  },
  {
    "threadId": "1191303476643385374",
    "name": "[issue] Deploying Qdrant over azure Kubernetes Service",
    "messages": "I have deployed Qdrant over AKS and using  Azure managed disk to store data. And it works perfectly fine until I upscale the AKS server and it increased number of pods.\nWhen the number of Pods increases and I try to check the collection it saya \"Collection not found\". I hit refresh button 5,6 times and it shows the collection on Dashboard.\n\nI have checked that the managed disk path is properly mounted on all pods and from google i have found that I have to set ReadWriteMany option set for Managed Disk and need to set \"QDRANT__CLUSTER__ENABLED=true\" I did both but the issue is still there.\n\n**__Here is the config:__**\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: qdrant\n  namespace: qdrant\nspec:\n  replicas: 1 \n  selector:\n    matchLabels:\n      app: qdrant\n  template:\n    metadata:\n      labels:\n        app: qdrant\n    spec:\n      containers:\n        - name: qdrant\n          image: qdrant/qdrant:latest\n          ports:\n            - containerPort: 6333\n          volumeMounts:\n            - name: qdrant-storage\n              mountPath: /qdrant\n            - name: config-volume\n              mountPath: /qdrant/config\n      volumes:\n        - name: qdrant-storage\n          persistentVolumeClaim:\n            claimName: azure-file-pvc\n        - name: config-volume\n          configMap:\n            name: qdrant-config\n            items:\n            - key: \"config.yaml\"\n              path: \"config.yaml\""
  },
  {
    "threadId": "1190718511178322072",
    "name": "Langchain qdrant wrapper cant acess vectors uploaded by qdrant  api",
    "messages": "So i run a langchain wrapper for qdrant\n> from langchain.vectorstores import Qdrant\nand i use the standard openAI embeddings to both load and query vectors over it (size  = 1536, cosine distance)\n\nheres my langchain docstore:\n\ndoc_store = Qdrant(\n    client=client, \ncollection_name=os.getenv(\"QD_COLLECTION_NAME\"), \n    embeddings=embed,\n)\n\nheres how i  add text to it:\ndoc_store.add_texts(parsed_text))\n\nheres the query:  doc_store.similarity_search(query=user_input, k=1)\n\n\n\nbut in my other code i target the same collection but my uploads on the Qdrant API cant be seen by the langchain Wrapper \n\nheres my qdrant upsert func:\ndef upsert_func(docs:list[list], QDclient)->None:\n    print(\"rodou\")\n    QDclient.upsert(\n    collection_name= COLLECTION_NAME,\n    points= [\n        PointStruct(\n            id = idx,\n            vector = vector,\n            payload= {\"texto\":test_question}\n        )\n        for idx, vector in enumerate(docs,0)\n    ]\n    )\n\nheres my search func\n\ndef vector_search(vector:list[float], QDclient):\n   search = QDclient.search(\n    collection_name=COLLECTION_NAME,\n    query_vector = vector,\n    limit=1\n   )\n\n   return search\n\n\nwhy is this happening? i dont get any vector incopatibility warnings, just no results from the search"
  },
  {
    "threadId": "1190406399814025257",
    "name": "RE Multitenancy",
    "messages": "It seems like multitenancy is a necessity for my use-case. \n\nI'd like to go with the group_id solution, but it seems like there will be a bottleneck on searching. \n\nThe suggested way to alleviate this bottleneck pre-emptively is https://qdrant.tech/documentation/guides/multiple-partitions/#calibrate-performance, but I don't understand what it is doing. Specifically, what is `payload_m` in HSNW (there is no hyperlink reosurce), and `m` in HSNW?"
  },
  {
    "threadId": "1190711408753639464",
    "name": "Docker is started but not accessible is there anything i am missing",
    "messages": "i dont see any permission or other errors \nnot sure about permission if amy required\nOS: Mac\njust testing how it works no previous experience with qdrant or related services."
  },
  {
    "threadId": "1180739864543645706",
    "name": "upgrading qdrant from 0.11.4 to latest stable .",
    "messages": "Gonna upgrade the qdrant to latest stable version , \n1> so will i need to do re-index again the data ?\n2> what latest stable version should i go for , should be fine if it is not the latest one but should be stable and fine if it lags behind 1 or 2 to the latest one \n3> is fastemebd and PQ embedding help in memory requirement ? using l6 and l12 model for embeddings  \n4> also should i use EBS or EFS , and why ??\n\nfor stable version thinking this : https://github.com/qdrant/qdrant/releases/tag/v1.6.0\n\nalso if u can help highlight feature which can be helpful and things to remember while doing the update would be helpful . \n\ndeployment wise i will be deploying using single node , in 4 core 16gb arm box as a docker  , around 5 mill+ vector with 0.11.4 was ablt to handle so same or thinking enhanced performance i am expecting , but will do test for the same ."
  },
  {
    "threadId": "1180097326577102879",
    "name": "Safe to remove `closed-*` files in `wal`",
    "messages": "Hi, I have lots of files (435G size) in the `wal` directory of several collections. Can I remove them? Also, there are some `open-*` files, should I better keep them? All of those survive when I shutdown the docker container."
  },
  {
    "threadId": "1160071225847074897",
    "name": "Updating payload instead of overwriting",
    "messages": "How do I update the payload but I don't want to overwrite keys in the payload that are not in the update. Python/curl code please"
  },
  {
    "threadId": "1180589809912254596",
    "name": "suggestions on how to improve a chatbot",
    "messages": "I built a chatbot with qdrant RAG and langchain that itтАЩs not comparable to any other chatbot I already built regarding performances\n\nAnd I m a no coder, thatтАЩs incredible!\n\nSo first, thank you for this amazing service\n\nI would like to ask you few suggestions on how to improve the bot:\n\n1) is it possible link words that have the same meaning so the bot understand also the synonyms? Example: in the KB there is the address of the property, but if the user ask for the address of the condo, the bot doesnтАЩt recognise it\n\n2) is it possible give a context to the bot? For example instructions like how to show the data? Example: if the user asks for restaurants, the bot can respond with only the name, but in the KB there are also phone number and website associated to that name \n\n3) is it possible to give a memory of just 1 messages to the bot? Example: the bot respond with the name of something, and the user asks for more details without mentioning it. At the moment the bot responds with casual data because doesnтАЩt understand \n\n\nThank you again ЁЯЩП"
  },
  {
    "threadId": "1180098584310452245",
    "name": "I use Qdrant image with docker how to deactivate access to port 6333",
    "messages": "Or at least the dashboard?"
  },
  {
    "threadId": "1180486919931560008",
    "name": "Indexer/optimizer not using swap??",
    "messages": "I noticed that even on increasing swap from 256GB to 2TB my optimizer runs out of memory. Does it use RAM only and not virtual memory??\n{\"result\":{\"status\":\"red\",\"optimizer_status\":{\"error\":\"Out of memory, free: 94452981760, IO Error: Cannot allocate memory (os error 12)\"},\"vectors_count\":440170326,\"indexed_vectors_count\":8584195,\"points_count\":440113830,\"segments_count\":15,\"config\":{\"params\":{\"vectors\":{\"size\":1024,\"distance\":\"Cosine\",\"on_disk\":true},\"shard_number\":2,\"replication_factor\":1,\"write_consistency_factor\":1,\"on_disk_payload\":true},\"hnsw_config\":{\"m\":16,\"ef_construct\":100,\"full_scan_threshold\":10000,\"max_indexing_threads\":0,\"on_disk\":false},\"optimizer_config\":{\"deleted_threshold\":0.2,\"vacuum_min_vector_number\":1000,\"default_segment_number\":0,\"max_segment_size\":null,\"memmap_threshold\":0,\"indexing_threshold\":20000,\"flush_interval_sec\":5,\"max_optimization_threads\":1},\"wal_config\":{\"wal_capacity_mb\":32,\"wal_segments_ahead\":0},\"quantization_config\":null},\"payload_schema\":{}},\"status\":\"ok\",\"time\":0.00003819}"
  },
  {
    "threadId": "1180025517844463636",
    "name": "Qdrant Data on env got deleted somehow ( 80GB ) only process was running on the box was qdrant",
    "messages": "version deployed : 0.11.4 \ndata was mounted EFS to docker image .\n\nthere were no CPU or memory alert based on EFS activity was able to see the sudden decrease in the EFS storage , and it started around 10.35 UTC we had data and then it suddenly decreased and the whole EFS was deleted/removed . Also the when i went and checked docker ps , no process was running \n\nwe checked the access of EFS etc , just to check if someone has moved or deleted but that also does not look like . Basically we want to know if there is some vulnerability with the qdrant image or version or dependencies that can modify the files owned by qdrant."
  },
  {
    "threadId": "1179710638310825994",
    "name": "Search result is different a lot than my own dot product implementation",
    "messages": "Hey, I observe that when I perform search using same embedding model, the result is very different than my own implementation.\nMy dot product implementation:\n```\ntexts_embeddings = model.encode(texts,normalize_embeddings=False)\nquery_embeddings = model.encode(query,normalize_embeddings=False)\nsimilarity = query_embeddings @ texts_embeddings.T\n```\n\nThe result is different than the search result returned by Qdrant even though I am using the same embedding model. Is anyone have any idea?"
  },
  {
    "threadId": "1180029091320172616",
    "name": "does listener nodes in a cluster receive data of all shards?",
    "messages": "hi noob question, does listener nodes in a cluster receive data of all shards? (or can it be set up in this way?) I'm wondering if it can act as a pure data node -- so I can scale out a cluster using data from listener node(s), without affecting other nodes with query traffic. Thanks"
  },
  {
    "threadId": "1179435701633290363",
    "name": "Azure cloud deploy date ?",
    "messages": "Hello for professional use we'll be interested in the cloud Qdrant solution based on Azure. From what i can see it's not yet supported (Coming soon). Is there a way to know when it's going live ? \n\nTy, Adrien"
  },
  {
    "threadId": "1160250562474950696",
    "name": "indexed vectors count not increasing",
    "messages": "I enabled my index again as follows\nurl -X PATCH --location 'http://192.168.10.122:6333/collections/us_fto_collection_v2' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"optimizers_config\": {\n        \"indexing_threshold\": 20000\n    }\n}'\nHowever the indexed vectors count is not increasing\ncurl -X GET \"http://192.168.10.122:6333/collections/us_fto_collection_v2\" \n{\"result\":{\"status\":\"green\",\"optimizer_status\":\"ok\",\"vectors_count\":438994106,\"indexed_vectors_count\":226553203,\"points_count\":428332334"
  },
  {
    "threadId": "1179586649173278840",
    "name": "Filter with int64 value",
    "messages": "Hi team,\n\nI noticed the filter doesn't work with int64 as the docs has stated. Does anyone experience this? I have to change the field type to string in order to do exact match, but I lose the ability to do the range filter. Is it a known issue for Qdrant?"
  },
  {
    "threadId": "1179572182939078726",
    "name": "Help with troubleshooting Qdrant slow searches with Filters",
    "messages": "I'm looking for advice with troubleshooting slow searches with large filters. \n\nSetup: \n- Docker Qdrant \n- Deployed to Amazon EKS\n- 4GB RAM per pod\n- CPU 600m per pod\n- 12 replicas\n- 24 shards\n- Index on a field called \"locationId\"\n- Connecting to qdrant using the python client\n- 12,624,912 vector count\n- Querying with a filter with 8,000 items.\n\nWhat I'm experiencing is very slow (greater than 10 second) searches when adding large filters. I define a large filter as a filter with a must clause on that indexed \"locationId\" field where the array has more than 1,000 items. Most of the time, results just time out. Remove the filter and results are speedy.\n\nWhat's strange is that the dev environment is basically provisioned in the exact same way. The differences that I can tell are that dev receives less writes and has a smaller vector count: 1,887,821. Queries on my dev system don't usually time out. \n\nI'm looking for advice on diagnosing what the issue is. Why is it that it works on dev and not live? What can I do to pinpoint what the problem is?"
  },
  {
    "threadId": "1179394496937599006",
    "name": "Help with very slow query speeds",
    "messages": "Hi. I have millions of vectors with sizes in the 1000's. For problem-specific reasons, it's not ideal to downscale these further. I have been trying out qdrant to do vector based search given these vectors, but I'm running into prohibitively slow query times which I really hope someone could help me solve. \n\n**My usecase:**\nI need to query a vector database containing ~10 million vectors of dimension ~2000. These queries need to return the most similar vector for each query vector. Returning the top 5 or top 10 would be nice, if possible. I need to be able to process at least 100.000 query vectors pr. 24 hours, which is just around 1 query pr. second.  It is OK if parallelism is required to achieve these speeds. \n\n** What I have tried:**\nI have so far worked with a local database containing a single vector collection. I have not used the docker image because I do not have rights to run docker images locally, and I have assumed that converting it to a singularity image wouldn't work (correct me if I'm wrong!).  I have attached a MWE of my code in this message.\n\nHere's what I find using the attached MWE:\n\nFor a database containing 10.000 vectors of length 1024, I am able to execute  `QdrantClient.search(.. limit = 1)` about 11 times a second. \n\nFor a database containing 50.000 vectors of length 1024, I am able to execute `QdrantClient.search(.. limit = 1)` about 2 times a second. \n\nAs soon as the databases begin to be semi-realistic in size (~1 million vectors), the query speeds are prohibitively slow, essentially making qdrant unusable for me. \n\nThese tests were carried out on a beefy workstation with 1TB of RAM,  2X AMD EPYC CPUs and the database was stored on a fast NVME drive.\n\n\n**Could you please take a quick look and let me know  if I am missing something?**"
  },
  {
    "threadId": "1179109785560424468",
    "name": "Logging request body sent from Rust Client",
    "messages": "Hey folks, is there a way to log the request body that is being sent to the rust_client. The SearchPoints doesn't have serde Serialize added to it AFAIK. Any help is appreciated! Thanks!"
  },
  {
    "threadId": "1178688155067633845",
    "name": "Sparse vector in storage",
    "messages": "Hi,\nI'm just wondering how do you store sparse matrices, do you use an optimisation during storage or it will take the same space as a normal vector?\nThanks."
  },
  {
    "threadId": "1178682099566395483",
    "name": "Qdrant website",
    "messages": "Hi.\nThe qdrant website seems a bit off. There is text over text and it doesn't seem like it should behave this way.\nTested on both chrome & firefox."
  },
  {
    "threadId": "1178499944261357638",
    "name": "Index stuck issue",
    "messages": "Im indexing vector field and its stuck, total points are 40M and vector index count is 8M, its not changing from last 2 days and collection is in yellow state"
  },
  {
    "threadId": "1175998006802268271",
    "name": "the size of a tensor a 541 must match the size of tensor b at non-singleton dimension 1",
    "messages": "I am following the qdrant blog about building a semantic search \nI have a dataframe contains a string columwn which i am trying to embed with the following code \n```python \nencoder = SentenceTransformer(\"medmediani/Arabic-KW-Mdel\")\n\nfreelancers['tags'] = freelancers['tags'].astype(str)  # Convert lists to strings\nfreelancers['full_info'] = freelancers['tags'] + \" \" + freelancers['about'].fillna('') + \" \" + freelancers['job_title'].fillna('')\n\nqdrant = QdrantClient(\":memory:\")\n\nqdrant.recreate_collection(\n    collection_name=\"kafeel\",\n    vectors_config=models.VectorParams(\n        size=encoder.get_sentence_embedding_dimension(),\n        distance=models.Distance.COSINE,\n    ),\n)\n\nqdrant.upload_records(\n    collection_name=\"kafeel\",\n    records=[\n    models.Record(\n        id=idx, \n        vector=encoder.encode(doc[\"full_info\"]).tolist(), \n        payload=doc.to_dict()  # Assuming 'to_dict()' method returns a dictionary\n    )\n    for idx, doc in freelancers.iterrows()\n    ],\n)\n```\nHow to solve this error! the strange thing for me is that the encoder_dimension size is 768"
  },
  {
    "threadId": "1177923849086574673",
    "name": "issue with qdrant upload records",
    "messages": "```python\nimport pandas as pd\nfrom qdrant_client import QdrantClient, models\nimport qdrant_client\nfrom qdrant_client.models import FieldCondition, Filter, Range\nfrom sentence_transformers import SentenceTransformer\n# sentence_transformer_model = \"medmediani/Arabic-KW-Mdel\"\n# sentence_transformer_model = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\nsentence_transformer_model = \"sentence-transformers/multi-qa-mpnet-base-cos-v1\"\n\nfreelancers_data = \"cleaned_freelancers.csv\"\njobs_data = \"cleaned_ai_jobs.csv\"\ncollection_name = \"kafeel\"\nfreelancers = pd.read_csv(freelancers_data)\nai_jobs = pd.read_csv(jobs_data)\nencoder = SentenceTransformer(sentence_transformer_model)\nqdrant = QdrantClient(\n    host=\"localhost\",\n    port=6333,\n)\n\ncollection_info = qdrant.get_collection(collection_name=collection_name)\nprint(collection_info)\nassert collection_info.status == models.CollectionStatus.GREEN\nassert collection_info.vectors_count == 0\n\nqdrant.recreate_collection(\n    collection_name=collection_name,\n    vectors_config=models.VectorParams(\n        size=encoder.get_sentence_embedding_dimension(),  # type: ignore  # noqa: PGH003\n        distance=models.Distance.COSINE,\n    ),\n)\nrecords = [\n    models.Record(\n        id=idx,\n        vector=encoder.encode(doc[\"short_combined\"]).tolist(),\n        payload=doc.to_dict(),\n    )\n    for idx, doc in freelancers.iterrows()\n]\nqdrant.upload_records(\n    collection_name=collection_name,\n    records=records,\n)\n```\nand i get the following error  \nqdrant_client.http.exceptions.UnexpectedResponse: Unexpected Response: 400 (Bad Request)\nRaw response content:\nb'{\"status\":{\"error\":\"Format error in JSON body: expected value at line 1 column 18751\"},\"time\":0.0}'\nhow to solve it !"
  },
  {
    "threadId": "1175053498887454741",
    "name": "Issue with qadrant-java-client",
    "messages": "I'm using v0.13.0 of https://github.com/metaloom/qdrant-java-client and trying to do a vector search with this (simplified) Kotlin code:\n\n```\nval vectors = listOf(0.1f, 0.2f, 0.3f)\nval searchRequest = PointsSearchRequest().setVector(NamedVector().setVector(vectors)\nqdrantHttpClient.searchPoints(\"my_collection\", searchRequest)\n```\n\nIt sends this request, which responds with a 422 Unprocessable Entity:\n`POST /collections/my_collection/points/search -d '{\"vector\":{\"vector\":[0.1,0.2,0.3]}}'`\n\nAccording to Qdrant's documentation, the request should be\n`POST /collections/my_collection/points/search -d '{\"vector\":[0.1,0.2,0.3]}'`\nso, is the Java SDK implemented wrongfully, or what am I doing wrong?"
  },
  {
    "threadId": "1188549923457732668",
    "name": "JSON Filtering",
    "messages": "I can't seem to get JSON filtering to work properly, and I was looking for advice. I've consulted the docs, and I don't see what I could be doing wrong. (edit: typing out fully...)"
  },
  {
    "threadId": "1176863747508273182",
    "name": "What header could be invalid during collection creation",
    "messages": "What header could be invalid during collection creation, we see this in the logs. The logs doesn't indicate which header caused this error\n\n> ERROR actix_http::h1::dispatcher: stream error: request parse error: invalid Header provided"
  },
  {
    "threadId": "1177290246761742436",
    "name": "Understanding cluster and replication",
    "messages": "Hello, I currently have a qdrant cluster with 3 nodes, I have setup collections to have a replication factor of 2.\n\nI see how two nodes are working, but the third one is idle and is not getting any shard. Will it get data at some point or do I need to manually move shards to the third node?\n\nHow does this work, I thought it would balance between the nodes."
  },
  {
    "threadId": "1177504947588321330",
    "name": "Double Chain with context",
    "messages": "How do I make my chatbot work correctly with double chain, on the one hand I want it to use a vector database to give me answers that are consistent from the book, but when I want to add that VDB to the history of the prompt template It doesn't work (or I don't know how to do it), is there a way to make it work like that?\nI'm a trainee, excuse my ignorance ЁЯШж"
  },
  {
    "threadId": "1177280983402303509",
    "name": "Does this seem like a sane approach for a recommendation system?",
    "messages": "I'm looking into building a recommendation system for books. Each book will be chunked into multiple passages, which are then embedded and stored in Qdrant individually with a `document_id` key in the payload (unique ID for each book).\n\nGiven a list of document IDs (`liked_books`), the goal is to find other similar books. I am considering the following approach:\n\n1. Use the \"Search points\" endpoint with `filter: document_id must be in <liked_books>` to get the point IDs of all passages appearing in one of `liked_books`\n2. Use the \"Recommend point groups\" endpoint with `positive: <result from previous request>`, `group_by: document_id`, `filter: document_id is not in <liked_books>` and optionally another filter like `filter: author must be XX YY`\n\nMy question is whether this seems like a sane approach? How will Qdrant perform with potentially thousands of `positive` points in the recommendation endpoint query? Will it even give useful results when `positive` points might point in many different directions?"
  },
  {
    "threadId": "1176866974899392645",
    "name": "qdrant-client throws SSL: WRONG_VERSION_NUMBER during collection creation",
    "messages": "We are using the latest `qdrant-client 1.6.9 `against `qdrant server v1.6.1` on `python 3.9 `runtime.\n\nWe see an error during collection creation on startup, refer attachment"
  },
  {
    "threadId": "1177085961847439440",
    "name": "Read Only API Key",
    "messages": "Is there any further auth, such as read only API key. I'm worried if there are any of.my team member who use Qdrant that has no responsibility in inserting data would delete entire collection. Or even basic auth by level of authority such super admin, admin, or user. Thanks"
  },
  {
    "threadId": "1170022164930506822",
    "name": "When does create_payload_index take effect?",
    "messages": "I've created a payload index on a collection using the API, when I check the info on the collection, I see:\n\n```json\n    \"payload_schema\": {\n      \"timestamp\": {\n        \"data_type\": \"integer\",\n        \"points\": 0\n      }\n    }\n```\n\nWill it only index subsequent payloads? I see `\"points\": 0` which leads me to believe it did not index existing points.\n\nI was unable to find any documentation beyond https://qdrant.tech/documentation/concepts/indexing/#payload-index that discusses which points are affected by calling `create_payload_index`."
  },
  {
    "threadId": "1164242595250192424",
    "name": "Taking snapshots during production?",
    "messages": "Is it possible to take snapshots of a collection while it is being queried/upserted into? If not, what is the best approach for backing up a production deployment that cannot be paused?"
  },
  {
    "threadId": "1176822025495519263",
    "name": "Is api_key optional while running Qdrant as a local server",
    "messages": "Is api_key optional while running Qdrant as a local server or is it possible to define and pass a key, so that the code is consistent immaterial of whether your client is connecting to a local or a cloud instance?\n\n`client = QdrantClient(\n    host=QDRANT_HOST,\n    port=QDRANT_PORT,\n    # api_key=QDRANT_API_KEY,\n)`"
  },
  {
    "threadId": "1176821096717238364",
    "name": "How to auto create a collection on docker startup",
    "messages": "How to auto create a collection on docker startup. We see the following files under the volume folder, is there a way to auto create or skip if the collection is existing\n\n`drwxr-xr-x 2 root root 4096 Nov 22 09:44 aliases\ndrwxr-xr-x 2 root root 4096 Nov 22 09:44 collections\n-rw-r--r-- 1 root root  299 Nov 22 09:44 raft_state.json`"
  },
  {
    "threadId": "1176512591787462706",
    "name": "query design for a batch scroll approach",
    "messages": "Hi all. \n\nI have a collection that has an indexed field тАЬobject_nameтАЭ. Now given a list of object_names, I want to return the top n similar points for each of them. \n\nMy current approach is as follows: \nFor each object_name do a scroll filtering to give me the point ids that match that object_name. Then use each of these point ids in a batch recommendation request to get the most similar points for each object_name. \n\nMy main issue in this approach is that I am doing many scroll requests if my list of object_names is large. There doesnтАЩt seem to be a batch scroll that I can use in this case. \n\nThere is the match any condition for which I could give a list of all the object_names, but then IтАЩm not sure how to determine which returned point corresponds to which object_name unless I return the whole payload for each point and post process that. \n\nWhat would be my best approach here? \n\nThanks!"
  },
  {
    "threadId": "1175945733787107489",
    "name": "Turn off colour in qdrant logs",
    "messages": "Hi amazing qdrant community, is there a easy way to turn off colour in qdrant logs? \nIdeally an environment variable"
  },
  {
    "threadId": "1176585639379095562",
    "name": "Meta question on PRs",
    "messages": "If I've submitted a PR for a minor change -- should I ask for a review (and how?)\n\nMy latest PR: https://github.com/qdrant/landing_page/pull/417"
  },
  {
    "threadId": "1176520506158096464",
    "name": "Qdrant db size",
    "messages": "Hi,\n\nOur current payload on an avg is about 10KB(which I retrieve from qdrant) and vector(1024) should be 4KB giving us a total of about 14KB per point. However the db size is currently at 2.3 TB for 450 million points which is an avg of about 5KB total for both vector and payload. \nIs there some compression going on behind the scenes??"
  },
  {
    "threadId": "1176472272463220867",
    "name": "Speeding up group search",
    "messages": "The query looks something like this:\n```typescript\n    const filter = {\n      must: [\n        {\n          key: 'pool_id',\n          match: {\n            value: item.pool_id,\n          },\n        },\n        {\n          key: 'customer_id',\n          match: {\n            value: item.customer_id,\n          },\n        },\n      ],\n      must_not: [\n        {\n          key: 'removed',\n          match: {\n            value: true,\n          },\n        },\n      ],\n      should: [{key: 'sizes', match: {value: 'S'}}],\n    }\n    const query = {\n      filter,\n      positive: [item.id],\n      group_by: `groups.dc5687c9-6130-4754-bd19-6c23e3e37785`,\n      using: 'vector',\n      group_size: 30,\n      limit: 15, // the number of groups to return, so max total items returned is\n    }\n```\n\nWhat can I do to speed up group search, other than:\n* reducing the number of groups returned\n* reducing the number of hits returned per group\n* reducing the total number of groups in the field we group on\n* reducing the dimensions of the vectors\n\nShould I create an index on the field I am grouping on in the query?\nShould I create an index on the nested object field that contains the field I am grouping on?\nShould I move the grouping field out of the nested object and into the payload?\n\nAny advice is greatly appreciated, and thank you for your wonderful product & support!"
  },
  {
    "threadId": "1176308018242125875",
    "name": "Search with \"priority\" parameters",
    "messages": "Is there any way to sort (not to filter, but sort) results in a search BASED on a payload metadata ?\nI have all my embeddings with a payload called \"priority\", and I would like the search method to prioritize first the ones with higher \"priority\" ЁЯШЙ"
  },
  {
    "threadId": "1176180846177288263",
    "name": "Transfer ownership of the cluster",
    "messages": "Good afternoon, \nWe have a colleague who is leaving the company. He has the ownership of all the clusters, is there a way to transfer the ownership to someone else under the company name?\nThank you in advance for your help."
  },
  {
    "threadId": "1176086384281395263",
    "name": "Low performance of Python qdrant-client compared to raw RestAPI calls",
    "messages": "**Problem Summary**\n\nPython `qdrant-client`'s `upload_collection` method works *way* slower than raw RestAPI calls. Is this the expected behavior?\nDetails are provided below.\n\n**Details**\n\n**Installation:** We are using helf-hosted Qdrant installation in Docker mode.\n**Qdrant version:** 1.4.0.\n**Connection methods:** both `qdrant-client` and custom wrapper around RestAPI calls.\n**Current collection settings:**\n```json\n{\n    \"status\": \"yellow\",\n    \"optimizer_status\": \"ok\",\n    \"vectors_count\": 654783,\n    \"indexed_vectors_count\": 654783,\n    \"points_count\": 654783,\n    \"segments_count\": 5,\n    \"config\": {\n        \"params\": {\n            \"vectors\": {\n                \"size\": 128,\n                \"distance\": \"Dot\",\n                \"hnsw_config\": null,\n                \"quantization_config\": null,\n                \"on_disk\": null\n            },\n            \"shard_number\": 1,\n            \"replication_factor\": 1,\n            \"write_consistency_factor\": 1,\n            \"on_disk_payload\": true\n        },\n        \"hnsw_config\": {\n            \"m\": 16,\n            \"ef_construct\": 100,\n            \"full_scan_threshold\": 10000,\n            \"max_indexing_threads\": 0,\n            \"on_disk\": false,\n            \"payload_m\": null\n        },\n        \"optimizer_config\": {\n            \"deleted_threshold\": 0.2,\n            \"vacuum_min_vector_number\": 1000,\n            \"default_segment_number\": 5,\n            \"max_segment_size\": 200000,\n            \"memmap_threshold\": 50000,\n            \"indexing_threshold\": 20000,\n            \"flush_interval_sec\": 1,\n            \"max_optimization_threads\": 1\n        },\n        \"wal_config\": {\n            \"wal_capacity_mb\": 32,\n            \"wal_segments_ahead\": 0\n        },\n        \"quantization_config\": null\n    },\n    \"payload_schema\": {\n        \"public_flg\": {\n            \"data_type\": \"integer\",\n            \"params\": null,\n            \"points\": 654783\n        }\n    }\n}\n```"
  },
  {
    "threadId": "1175948667711131759",
    "name": "Multiple types of Dashboards in the docs",
    "messages": "As I go through documentation examples, I see references to two different dashboard locations:\n\n- localhost:6333/dashboard\n- cloud.qdrant.io\n\n(Did I miss any?)\n\nI'm thinking that we could \"just\" link the cloud dashboard examples in https://qdrant.tech/documentation/cloud/ to https://cloud.qdrant.io. For that purpose, I've set up https://github.com/qdrant/landing_page/pull/414 ."
  },
  {
    "threadId": "1175124369333305364",
    "name": "How to call Recommend with only negative points",
    "messages": "Reading the docs it would seem that as long as you use the strategy \"BEST_SCORE\" you can use recommend with only negative points.\n\nhttps://qdrant.tech/documentation/concepts/search/\nUnder the heading \"Using only negative examples\"\n\nHowever, I consistently get the error \"Bad Request: At least one positive vector ID required\".\n\nI'm using Python qdrant-client==1.6.9\n\nShould this be supported? Or am I mistaken?"
  },
  {
    "threadId": "1174426610338889820",
    "name": "Avoiding failed writes while updating cluster",
    "messages": "I have a Qdrant cloud cluster running an older version of Qdrant. I want to update this cluster. The update process involves one or more restarts and thus I am worried about failed writes causing issues with my prod system during this period. Do you have any advice as to how I can avoid failed writes/lost data while my cluster restarts?\n\nI have explored creating a new cluster with the latest software version and migrating to it but my collection is large enough that using the Python library's migrate() function fails after several hours due to connectivity issues (timeouts or 502 errors) with no option to resume.\n\nAnother option I have explored is temporarily diverting my prod system to a fresh cluster cluster to capture new writes then changing back to the original cluster and backfilling the vectors from the new cluster after the update finishes. This would require less data to be copied and thus less susceptible to connectivity issues but I don't see any built-in way to backfill data into an already populated collection (i.e. there is no native method to do this nor do I see any way to return all vectors in a collection so that I can upload them to another collection)\n\nPlease advise on how to go about updating a cluster version while minimizing lost data."
  },
  {
    "threadId": "1174193086700736572",
    "name": "Request keyin API key before access http://localhost:6333/collections",
    "messages": "I installed local server, now i would like to make it more security.\nI created API key and put it in config.yaml. Now I would like to request people to give API key before access  http://localhost:6333/collections\n\nThank you for your help"
  },
  {
    "threadId": "1173271576205082654",
    "name": "I've got a reindexing question",
    "messages": "I'm trying to understand how best to set up qdrant for production reindexing in real-time as new data is added to the collection.  I understand that vector reindexing runs after new data is added, and that can be suppressed for a period and rebuilding the index later.\n\nI have a use case where regularly I have background tasks adding in new payloads and vectors into a production database.  This production database will also be used for vector similarity queries the whole time on a read-only basis.  I want to make sure that we don't disrupt or slow the read processes by writing new data.   When a new vector and payload is added  the index will be rebuilt.  By default will this cause any latency or outage issues on the production reads?  Is there a way to stop that happening?  I guess what I would like is that the old index is used up until the moment that the new index has already been rebuilt and then is instantaneously is switched in.  In my use case it doesn't matter if there is a delay for the new data becoming available on search.  \n\nI've had a look at the docs and I can't quite see how it works.  Thanks"
  },
  {
    "threadId": "1174729140432277605",
    "name": "Increased post-snapshot memory consumption after v1.6.1 upgrade",
    "messages": "We recently did an upgrade from v1.4.0 to v1.6.1 on our self-hosted 3 node cluster which we manage via the qdrant helm chart and the upgrade went very smoothly.\nHowever, when we ran our first cluster backup job after the upgrade I noticed some strange memory usage during the first snapshot compared to v1.4.0's behavior and afterwards the cluster remains at a higher memory usage than before (unlike v1.4.0).\n\nA brief description of our backup process:\n- For each peer and each collection, we call the snapshot API and then stream the snapshot body from Qdrant API directly to S3 through the job runner\n- We then delete any snapshots on the peer older than the latest via the Qdrant API\n- We have 2 collection, largest collection snapshot size is 4GB. The other is 5 MB's.\n\nWe haven't kicked off any major indexing processes during this time, so apart from intermittent point additions our only workload should be searches, recommendations, and daily snapshots.\n\nI've attached a screenshot of our P99 memory working set amount over the last 7 days for each node, with a marker showing when we did the upgrade to v1.6.1.\n\nPlease let me know if I can provide any additional information.\n\nThanks,\nMichael"
  },
  {
    "threadId": "1174697342411231232",
    "name": "set_indexing_threshold() only triggers indexing with value `1` (not `2` or `20_000`)",
    "messages": "When creating collections (including the upsert of points) we want to pause the vector indexing. After the upsert of points is done, we directly set the indexing threshold to `1` and the indexing of vectors begins. But it does not work for any other number. Do you know about this? Are we assuming something wrong?"
  },
  {
    "threadId": "1185329130766614568",
    "name": "Text match either one or the other",
    "messages": "Hello there,\n\n```json\n\n{\n  \"key\": \"description\",\n  \"match\": {\n    \"text\": \"good cheap\"\n  }\n}\n\n```\n\nIs it possible to return all elements that have either \"good\" OR \"cheap\" (not necessarily both) ?"
  },
  {
    "threadId": "1172189864456888452",
    "name": "Server getting crashed, while trying to upload the created embeddings",
    "messages": "Dataset size 2M rows"
  },
  {
    "threadId": "1174345996793303191",
    "name": "Panic: attempt to divide by 0 on numeric index",
    "messages": "I set a payload index on an existing collection, which worked just fine locally ~ pushing the change to production results in the below panic when trying to filter on the payload index.\n\n\n```\n2023-11-15T13:47:14.224164Z ERROR qdrant::startup: Panic occurred in file lib/segment/src/index/field_index/numeric_index/mod.rs at line 197: attempt to divide by zero \n2023-11-15T13:47:14.224613Z  WARN qdrant::actix::helpers: error processing request: 1 of 1 read operations failed:\n  Service internal error: task 511 panicked\n2023-11-15T13:47:14.224697Z  INFO actix_web::middleware::logger: 127.0.0.1 \"POST /collections/article_text_embedding_ada_002/points/search HTTP/1.1\" 500 126 \"http://localhost:6333/dashboard\" \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/119.0\" 0.001006\n2023-11-15T13:47:14.224736Z ERROR qdrant::startup: Panic backtrace:\n   0: qdrant::startup::setup_panic_hook::{{closure}}\n   1: <alloc::boxed::Box<F,A> as core::ops::function::Fn<Args>>::call\n             at /rustc/d5c2e9c342b358556da91d61ed4133f6f50fc0c3/library/alloc/src/boxed.rs:2007:9\n   2: std::panicking::rust_panic_with_hook\n             at /rustc/d5c2e9c342b358556da91d61ed4133f6f50fc0c3/library/std/src/panicking.rs:709:13\n   3: std::panicking::begin_panic_handler::{{closure}}\n             at /rustc/d5c2e9c342b358556da91d61ed4133f6f50fc0c3/library/std/src/panicking.rs:595:13\n   4: std::sys_common::backtrace::__rust_end_short_backtrace\n             at /rustc/d5c2e9c342b358556da91d61ed4133f6f50fc0c3/library/std/src/sys_common/backtrace.rs:151:18\n   5: rust_begin_unwind\n             at /rustc/d5c2e9c342b358556da91d61ed4133f6f50fc0c3/library/std/src/panicking.rs:593:5\n   6: core::panicking::panic_fmt\n             at /rustc/d5c2e9c342b358556da91d61ed4133f6f50fc0c3/library/core/src/panicking.rs:67:14\n   7: core::panicking::panic\n             at /rustc/d5c2e9c342b358556da91d61ed4133f6f50fc0c3/library/core/src/panicking.rs:117:5\n   8: segment::index::field_index::numeric_index::NumericIndex<T>::range_cardinality\n   9: <segment::index::field_index::numeric_index::NumericIndex<T> as segment::index::field_index::field_index_base::PayloadFieldIndex>::estimate_cardinality\n  10: segment::index::struct_payload_index::StructPayloadIndex::condition_cardinality\n  11: segment::index::query_estimator::estimate_filter\n  12: <segment::index::struct_payload_index::StructPayloadIndex as segment::index::payload_index_base::PayloadIndex>::estimate_cardinality\n  13: <segment::index::hnsw_index::hnsw::HNSWIndex<TGraphLinks> as segment::index::vector_index_base::VectorIndex>::search\n  14: <segment::segment::Segment as segment::entry::entry_point::SegmentEntry>::search_batch\n  15: collection::collection_manager::segments_searcher::execute_batch_search\n  16: collection::collection_manager::segments_searcher::search_in_segment\n  17: tokio::runtime::task::raw::poll\n  18: std::sys_common::backtrace::__rust_begin_short_backtrace\n  19: core::ops::function::FnOnce::call_once{{vtable.shim}}\n  20: <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once\n             at /rustc/d5c2e9c342b358556da91d61ed4133f6f50fc0c3/library/alloc/src/boxed.rs:1993:9\n  21: <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once\n             at /rustc/d5c2e9c342b358556da91d61ed4133f6f50fc0c3/library/alloc/src/boxed.rs:1993:9\n  22: std::sys::unix::thread::Thread::new::thread_start\n             at /rustc/d5c2e9c342b358556da91d61ed4133f6f50fc0c3/library/std/src/sys/unix/thread.rs:108:17\n  23: <unknown>\n  24: <unknown>\n\n```"
  },
  {
    "threadId": "1174243640470687774",
    "name": "How to check if payload index has been created.",
    "messages": "Trying to create a full-text index for a payload field. I can see the payload schema has been updated. I want to be sure if the index has been created for the intended field. \n\n**Background**\nI want to use full text match/search along with vector search. after I have updated the payload schema using `/index` endpoint  I'm getting same search results as I get with only vector search. what could be reasons?\n1. Maybe the index has not been created yet, maybe there is an index optimizer threshold by default. How can I check if an index has been created.\n2. Filtering doesn't impact vector search score at all. \n\nAlso, what will be your suggestions to implement a hybrid search with qdrant?"
  },
  {
    "threadId": "1173630313432367124",
    "name": "scroll ordering/sorting",
    "messages": "I have points being stored with snowflake ids, and i have a use case where I would essentially like to get something along the lines of \"given a point, fetch the next 10 youngest points\", the snowflake ids enable this since you can get a timestamp from them, but from my understanding, snowflake ids will be larger the younger they are, which means the scroll api will return the oldest points first, and I want the opposite ordering. Does anyone have any thoughts on how I could get this ordering to be reversed?"
  },
  {
    "threadId": "1171071709571457145",
    "name": "Are their any benchmarking pitfalls?",
    "messages": "From various reliable sources we have heard that qdrant performs quite well.\n\nBut e.g. looking at https://ann-benchmarks.com/ the results seem rather underwhelming. Now ann-benchmarks doesn't track with how much less resources qdrant could be run, but when running some custom benchmarks  things did not look  grate for qdrant either  (e.g. more CPU usage and ~half the throughput then elasticsearch). Through it's [qdrant] RAM usage was always much smaller.\n\nNow I am wondering if there are some benchmark pitfalls when benchmarking with filters and  multiple nodes (we used 3 nodes, 3 shards, replication of 1) or if qdrant only starts outperforming elasticsearch if the RAM usage of elasticsearch becomes unsustainable high?\n\n(I know I didn't provide much details, for now I just want to know if there are some general know pitfalls, and if not I can provide more context later on.)"
  },
  {
    "threadId": "1169467781410996245",
    "name": "Keyword Payload: Single vs. Array",
    "messages": "Hi, I'm very much a Qdrant (and Python) novice. So forgive me if this is a very dumb question.\n\nI am creating a product description vector DB in Qdrant and I want to have a \"sports\" payload that denotes whether or not a product is associated with one or multiple sports. I understand that for a single keyword payload the formatting would look like \"basketball\". But if I'm adding a payload for a product that corresponds to multiple sports, I believe I would need an array like such, [\"basketball\", \"baseball\", \"football\"]. \n\nIs it okay if some values for some products are single keywords and some are arrays? Or should I make sure they're all arrays, even if it's just a single value in the array тАФ i.e. [\"basketball\"]ЁЯЩП"
  },
  {
    "threadId": "1163305971783843995",
    "name": "Java sdk",
    "messages": "Is there a plan to public an official Java SDK?"
  },
  {
    "threadId": "1150257212871151717",
    "name": "Secure Qdrant Instance in Railway - Log in Credentials",
    "messages": "Hello Guys!\n\nWhen we create a Qdrant instance in Railway it is built successfully, however, when you go to the actual URL you just get instant access without any form of credential/login info so anyone can find the URL and  access the instance and modify our data. \n\nSo I am basically looking for the kind of validation/authorization something like the cloud-hosted version of Qdrant that at least asks for an api key to access.\n\n\nQdrant docs make reference to an api_key implementation here: https://qdrant.tech/documentation/guides/security/, however, since I am not a developer,  I am not sure if this should go in a docker yml or if it is making reference to something else. Or if I could just load an api_key env variable in Railway and set up my key like API-KEY=12345 to secure it.\n\n\nWe are using  Qdrant v1.5.0  through a Docker instance using Railway. We just forked the repo and added it to Railway and it just deployed it there. \n\n\nThank you for your time!"
  },
  {
    "threadId": "1173705946548150332",
    "name": "Qdrant config within distinct EC2 instances",
    "messages": "**Context**: I have the below config for qdrant, but requests last too much in distinct instance types.\n\nBelow are two scenarios, both with the following collection and its config:\n\n```\n{\"status\":\"green\",\"optimizer_status\":\"ok\",\"vectors_count\":8112979,\"indexed_vectors_count\":8111922,\"points_count\":7978822,\"segments_count\":13,\"config\":{\"params\":{\"vectors\":{\"image\":{\"size\":2048,\"distance\":\"Cosine\"}},\"shard_number\":1,\"replication_factor\":1,\"write_consistency_factor\":1,\"on_disk_payload\":true},\"hnsw_config\":{\"m\":16,\"ef_construct\":100,\"full_scan_threshold\":10000,\"max_indexing_threads\":0,\"on_disk\":true},\"optimizer_config\":{\"deleted_threshold\":0.2,\"vacuum_min_vector_number\":1000,\"default_segment_number\":0,\"max_segment_size\":null,\"memmap_threshold\":10000,\"indexing_threshold\":20000,\"flush_interval_sec\":5,\"max_optimization_threads\":1},\"wal_config\":{\"wal_capacity_mb\":32,\"wal_segments_ahead\":0},\"quantization_config\":null},\"payload_schema\":{\"property_type\":{\"data_type\":\"keyword\",\"points\":7978822},\"sepomex_id\":{\"data_type\":\"keyword\",\"points\":7978822},\"ad_id\":{\"data_type\":\"keyword\",\"points\":7978822}}}\n```\n\nIn summary, 8.1M vectors of dim 2_048 on disk.\n\n**Scenarios**\n\n*aws ec2 c6a.8xlarge*: 7 aws batch jobs where send simultaneously with average response times for search + scroll of X embeddings with an average response of 0.14 seconds. In this case, our resources where boost up to 100% vCPUs usage and memory stayed below 7%. 32 vCPUs and 64GB of RAM.\n\n*aws ec2 c6a.2xlarge*: 2 aws batch jobs are being sent simultaneously, however, the CPU usage does not goes upper than 2% in average and memory usage stays equivallently in 28%. This instance has 8 vCPUs and 16GB in memory. Sadly, the average response goes up to 20 seconds.\n\nThe question is: why is not qdrant using the 100% of CPU resources?\n\nAlready reviewed the ec2 resource assignation to docker and we guess it is a qdrant config problem.\n\nAny idea how to improve this?"
  },
  {
    "threadId": "1173555690346913822",
    "name": "Bootstrap arg for the seed node.",
    "messages": "For the first seed node of a cluster, the bootstrap arg can be omitted whilst cluster bootup. But, what should be the bootstrap arg set to post the cluster is up. Supposing the qdrant process is being managed by systemctl, should the bootstrap arg of the seed node be kept empty as is. Will that ensure that the node can rejoin the cluster properly if the process somehow restarts?"
  },
  {
    "threadId": "1182777389684629514",
    "name": "How to correctly use GRPC",
    "messages": "I'm using python `qdrant-client` version 1.6.2 with `AsyncQdrantClient`. I thought I was using gRPC since I was creating the client with \n\n```\nclient = AsyncQdrantClient(\n                    'localhost',\n                    port=6333,\n                    grpc_port=6334,\n                    prefer_grpc=True,\n                    )\n```\n\nI started running into issues with the client throwing a `ResponseHandlingException` when loading snapshots (ie `await client.recover_snapshot(collection_name, snapshot_filename)`. I traced the error back to a timeout error, and fixed it by passing a `timeout` value to `AsyncQdrantClient`. This confused me because the docs said the default timeout for gRPC was unlimited.\n\nDo I need to do something additional for the client to use gRPC? Or is the gRPC method not implemented for recovering snapshots?"
  },
  {
    "threadId": "1182951186614194266",
    "name": "Infinite Build Time when Building for Local Development",
    "messages": "On following the steps as mentioned in https://github.com/qdrant/qdrant/blob/master/docs/DEVELOPMENT.md#local-development I reach to the last step of the building process with cargo build but it is stuck at the absolute last build for qdrant(bin). I have attached the screenshot. This has been stuck for a couple of hours now and I am unable to understand the reason behind this. There is 30G available on the disk, so storage should not be an issue."
  },
  {
    "threadId": "1173383945283436555",
    "name": "Finding Dense Groupings or \"Hot Spots\"",
    "messages": "I have several use cases in which it would be very valuable to find points (in respect to a given filter) which have an exceptionally large number of neighbors within a certain distance. Basically finding hot spots where points are densely grouped, indicating \"there are lots of similar items here\". \n\nI can do this by simply using the scroll endpoint with a filter, extracting all points, and using other packages for the analysis...\n\nBut am I missing any feature of the QDrant API that lets you do this more efficiently?"
  },
  {
    "threadId": "1183667450588438558",
    "name": "Collection breaks on payload indices deletion",
    "messages": "Qdrant version: 1.6.1.\n\nWe have a collection with the following configuration:\n```json\n{\n    \"status\": \"green\",\n    \"optimizer_status\": \"ok\",\n    \"vectors_count\": 7769360,\n    \"indexed_vectors_count\": 7769360,\n    \"points_count\": 7769360,\n    \"segments_count\": 8,\n    \"config\": {\n        \"params\": {\n            \"vectors\": {\n                \"size\": 128,\n                \"distance\": \"Cosine\",\n                \"hnsw_config\": null,\n                \"quantization_config\": null,\n                \"on_disk\": true\n            },\n            \"shard_number\": 1,\n            \"replication_factor\": 1,\n            \"write_consistency_factor\": 1,\n            \"read_fan_out_factor\": null,\n            \"on_disk_payload\": true\n        },\n        \"hnsw_config\": {\n            \"m\": 16,\n            \"ef_construct\": 100,\n            \"full_scan_threshold\": 10000,\n            \"max_indexing_threads\": 0,\n            \"on_disk\": true,\n            \"payload_m\": null\n        },\n        \"optimizer_config\": {\n            \"deleted_threshold\": 0.2,\n            \"vacuum_min_vector_number\": 1000,\n            \"default_segment_number\": 0,\n            \"max_segment_size\": 600000,\n            \"memmap_threshold\": 200000,\n            \"indexing_threshold\": 5000,\n            \"flush_interval_sec\": 1,\n            \"max_optimization_threads\": 0\n        },\n        \"wal_config\": {\n            \"wal_capacity_mb\": 32,\n            \"wal_segments_ahead\": 0\n        },\n        \"quantization_config\": null\n    }\n}\n```"
  },
  {
    "threadId": "1183695876925964298",
    "name": "Can we run Qdrant on our local-machine Offline ?",
    "messages": "I am new to programing, especially LLMs and VectorDBs. I want to know that, Is it possible to use QDRANT offline on our machine, because i have read this on QDRANT website, but can't see any dedicated video or blog on it. There is no Youtube video on, How can we use QDRANT on our own Local Machine Offline."
  },
  {
    "threadId": "1183746156182917162",
    "name": "Are there any other charges for deploying your Vector for commercial purpose other than $25?",
    "messages": "I have created a CHATBOT Application. For My Application, One Node will be more than enough, and In next month, i am planning to commerlize my application into production. \n\nSo my Question is : Will the price will be 25$ or there are any other charges included, eg: Per Request Cost or Any other hidden cost?\n\nThanks"
  },
  {
    "threadId": "1183661899972354099",
    "name": "Docker version, persistant volume",
    "messages": "Hi is there any way to use a persistent storage if using the docker version?\n\nThx"
  },
  {
    "threadId": "1184107198670975059",
    "name": "variable payloads in points of collection",
    "messages": "Hi Team, \n\nI currently have a vectorDB with around 70M points, which constantly keeps increasing. Each point has a vector and payload associated. \nThe payload has fixed set of 4 keys - `key_1`, `key_2`, `key_3`, `key_4` of which all are indexed in RAM. \n\nNow i want to add a field in my payload  `key_5` for the next point in inserted. \n\nSo at 80M the situation would be:\nfirst 70M have - 4 keys - 4 indexed\n\nbut the next 10M\nI want the 5th key to be indexed as well.\n\nSo the indexation count should ideally look as:\n `key_1` - 80M\n`key_2` - 80M\n`key_3` - 80M\n`key_4` - 80M\n`key_5` - 10M\n\nIs it possible to do this without the need to recreate the DB again? Do we support a point to be indexed if it misses certain payload fields?"
  },
  {
    "threadId": "1183708581049159680",
    "name": "Limit results",
    "messages": "Hello guys is there a way where instead of  limiting result per number, i want to limit the number of results based on proximity score. \n\nExample : I want all the results where score proximity > 80% ?"
  },
  {
    "threadId": "1172199757943414846",
    "name": "Service fails after a certain amount of uploads",
    "messages": "I am trying to test qdrants performance on a database of up to 200 million points. However after around 40M points the server is killed with the following message: \n```\n./entrypoint.sh: line 25:     7 Killed                  ./qdrant $@\n```\nI run the service via docker run. Before I encountered the same problem but earlier. I searcht the discord and found a page that referred to the tutorial about bulk upload: https://qdrant.tech/documentation/tutorials/bulk-upload/. I followed the instructions in  the tutorial. I have now created the collection like this:\n```client.create_collection(\n    collection_name=\"test_collection\",\n    vectors_config=VectorParams(size=20, \n                                distance=Distance.DOT, \n                                on_disk=True))\n\nclient.create_payload_index(collection_name='test_collection', \n                            field_name='doc_id', \n                            field_schema='integer')\nclient.update_collection(\n    collection_name=\"test_collection\",\n    optimizer_config=models.OptimizersConfigDiff(indexing_threshold=0,\n                                                  memmap_threshold=None),\n    \n)````\nIf I understand this correctly all the points should be placed on disk, no indexing should take place untill i up the threshold again, and also no power is lost on the memmap. There is plenty of storage left on the device. Therefore i dont understand why the service is having problems.\n\nAlso, once the service is killed during upload, I cannot restart the service anymore. It gives a similar error while loading the collection\n```INFO storage::content_manager::toc: Loading collection: test_collection\n./entrypoint.sh: line 25:     7 Killed                  ./qdrant $@ ```\n\nDoes anybody know what I am doing wrong?"
  },
  {
    "threadId": "1172466019944628254",
    "name": "Collections not shown",
    "messages": "Using latest version qdrant 1.6.1\n\nWhen trying to access http://localhost:6333/collections I get  `Error: Failed to fetch`\n\nUsing the console, I get the following stacktrace:\n```  \n\"stack\": \"AxiosError: Network Error\\n    at c.onerror (http://localhost:6333/dashboard/assets/index-22a6d29c.js:295:4409)\",\n```\n\nApart from that, qdrant works normally. But I am a bit worried that this could stop anytime.\n\nDo you have any clue what's going on and how I could fix that? Increasing the number of file descriptors did not help."
  },
  {
    "threadId": "1182469046353010758",
    "name": "i have deleted complete points from qdrant clount but vector count showing same",
    "messages": "hey i have deleted points using this code\n```typescript\nawait client.delete('Embeddings', {\n              filter: {\n                must: [\n                  {\n                    key: 'page_id',\n                    match: {\n                      value: pageId,\n                    },\n                  },\n                ],\n              },\n            })\n ```\nЁЯСЖthis  deleted 2 points from qdrant\nbut in the clound UI here what im seeing is points_count reduced by 2 but vectors_count is still same\n(those deleted 2 points doesnt have indexes so indexes stays same)"
  },
  {
    "threadId": "1182318983563726968",
    "name": "HOW GET MAX ID OF COLLECTION USING QDRANT CLOUD CLIENT",
    "messages": "hey im trying find js code which returns the max id of a collection. I'm using qdrant client"
  },
  {
    "threadId": "1161722111782572082",
    "name": "Influence of Number of Shards on CPU Utilization",
    "messages": "I am currently creating an Index of about 40 mio vectors and when I set the sharding to 1 I have an extremly low CPU  load. This increases when I increase the number of shards.\nCan anyone confirm or deny this or even give an explanation?\n\nAs far as I understand the documentation the number of shards should only be increased when using a distributed deployment.\nThanks in advance"
  },
  {
    "threadId": "1170079151936516126",
    "name": "How to implement \"Load More\" button for search?",
    "messages": "Is this possible, instead of being limited to a fixed number, to allow user to keep loading more and more. How could one implement it?"
  },
  {
    "threadId": "1181605973098500096",
    "name": "Handle missing point error for recommend_batch",
    "messages": "Hi,\n\nMy problem is a bit similar than this one : https://discord.com/channels/907569970500743200/1158272095516643338\n\nI want to use the recommend_batch method with the Python Client but I don't know how to handle the case of missing points in the batch. (https://qdrant.tech/documentation/concepts/search/#batch-recommendation-api)\n\nFor instance, let's say that I want to search for the following list of ids : [1,2,3].\n\nI will send each vector in a RecommendRequest like this : \n\n`RecommendRequest(positive=[****], negative=[], filter=None, params=None, limit=30, offset=0, with_payload=True, with_vector=None, score_threshold=None, using=None, lookup_from=None)`\n\nNow let's imagine that one of these points is missing : id = 2 is not in the Qdrant database for a random reason. When one of the points of the batch is missing in my Qdrant database, I get an error for the entire batch instead of just ignoring this particular id.\n\nI would like to be able to retrieve the recommendation for the existing points and ignore the errors when a point is missing. In my case, the expected result would have been a response for id=1 and id=3, while ignoring the error for id=2.\n\nIs it possible using the current recommend_batch() method in python or do I have to use the simple recommend() method for each point ? \n\nThank you very much and sorry if my question is not clear enough !"
  },
  {
    "threadId": "1181380187972190298",
    "name": "Updating indexing threshold",
    "messages": "I have a collection with more than 28 million vectors. If I update or change the indexing threshold, will it recreate the all the indexes? This would take so long."
  },
  {
    "threadId": "1169285310023204865",
    "name": "Get the size param from FastEmbed",
    "messages": "2023-11-01 16:41:30   File \"/app/./app.py\", line 18, in params\n2023-11-01 16:41:30     'size': embedding_model.get_sentence_embedding_dimension(),\n2023-11-01 16:41:30             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2023-11-01 16:41:30 AttributeError: 'FlagEmbedding' object has no attribute 'get_sentence_embedding_dimension'"
  },
  {
    "threadId": "1168564249795239957",
    "name": "How to start a cluster on a single machine",
    "messages": "Hi, Im trying to start a cluster on a single (64 VCPU) machine to take advantage of multiple NVME mounted disks I have on this machine.\n\n1.I start the leader on 1 NVME disk\n./qdrant --uri 'http://qdrant_node_1:6335' &\n\n2.Then I try to start the another peer after cd to another NVME disk\n./qdrant --bootstrap 'http://qdrant_node_1:6335' &\n\nHowever I get an error\n2023-10-30T14:44:52.340025Z ERROR qdrant::startup: Panic backtrace: \n   0: <unknown>\n   1: <unknown>\n   2: <unknown>\n   3: <unknown>\n   4: <unknown>\n   5: <unknown>\n   6: <unknown>\n   7: <unknown>\n   8: <unknown>\n   9: <unknown>\n  10: <unknown>\n  11: __libc_start_call_main\n             at ./csu/../sysdeps/nptl/libc_start_call_main.h:58:16\n  12: __libc_start_main_impl\n             at ./csu/../csu/libc-start.c:392:3\n  13: <unknown>\n    \n2023-10-30T14:44:52.340041Z ERROR qdrant::startup: Panic occurred in file src/main.rs at line 296: Can't initialize consensus: Failed to initialize Consensus for new Raft state: Failed to create timeout channel: transport error    \n\nWhat do I need to do? I am using the qdrant linux binary v 1.6.1 from https://github.com/qdrant/qdrant/releases/tag/v1.6.1"
  },
  {
    "threadId": "1169038846709747824",
    "name": "understanding performance degradation with upgrade from 1.4.1 -> 1.6.1",
    "messages": "Hello! First of all, thanks for building a great product! I've been very impressed with performance and ease of use.\n\nI'm playing around with self-hosted Qdrant in a k8s cluster (16 replicas, handling ~250 reads & writes per second), and just went ahead with an upgrade from 1.4.1 -> 1.6.1, but noticed pretty a considerable increase in search latency (round trip), P50 latency from ~6ms -> 27ms.\n\nI also noticed the memory utilization dropped quite a bit while CPU utilization increased considerably.\n\nI've attached some metrics we're sending to datadog so you can see how the upgrade affected our instance.\n\nWe also upgraded the helm chart version from 0.2.8 -> 0.6.1, but are still using Python client version 1.1.7 for requests.\n\nI'd love to understand:\n1. Why memory utilization dropped & CPU increased because of the upgrade (was payload pushed entirely to PVs?)\n2. What might've changed in this upgrade that could cause this latency increase - do we need to upgrade our Python client version or change how we're querying the database?\n\nOne other potential relevant piece of information is that we're running several collections that use payload indexing on the same cluster (for isolation and simplicity). I know this isn't recommended, but it seemed to work fine with version 1.4.1 - did something change about how things works?"
  },
  {
    "threadId": "1178968282422849596",
    "name": "Response error in create_full_snapshot()",
    "messages": "Hi! I'm trying to create a full snapshot of my data, but I am getting an error.\nCode: \n```\nqdrant = QdrantClient(\"localhost:6333\")\nqdrant.create_full_snapshot()\n```\n\nResult:\n```\n    217 with map_httpcore_exceptions():\n--> 218     resp = self._pool.handle_request(req)\n    220 assert isinstance(resp.stream, typing.Iterable)\n...\n    102 except Exception as e:\n--> 103     raise ResponseHandlingException(e)\n    104 return response\n\nResponseHandlingException: timed out\n```\n\nHowever, if I list the full snapshots it appears as created: `SnapshotDescription(name='full-snapshot-2023-11-28-07-55-00.snapshot', creation_time='2023-11-28T07:55:00', size=609943040)`\n\nI believe last week I could run the same code without any error, I don't know if it's related.\nThanks in advance"
  },
  {
    "threadId": "1178908938578698270",
    "name": "Store vectors on disk and in memory",
    "messages": "Hi guys! Is there a way to store a subset of vectors in a collection in memory, with the rest in persistent storage? More specifically, we're running out of memory in our cluster, but we know that not all of our vectors are accessed with the same frequency. Some are hardly accessed at all, and some are accessed only a moderate amount. Is there some way of configuring qdrant such that only frequently (or recently) used vectors are stored in memory?"
  },
  {
    "threadId": "1178954660917284875",
    "name": "Dot distance metric return confusing scoring value",
    "messages": "Hi all.\nIm using qdrant 1.6.1\n\nI have a collection of blog posts which will encode post title into vectors and store in qdrant.\nQuatization: scalar\nBut when i do recommend search it gives me top 10 result where\nthe first point is largest points (> 1) and desc to the last point \n\nBut i understand that:\n+ Euclid will return the score is the distance between vectors, the lower the score is, the nearer the vectors.\n+ Cosine: will return the score is the similarity score between vectors: 0 -> 1. with 1 is the most  similar.\n+ Dot:????\n\nThanks in advance"
  },
  {
    "threadId": "1179753333339209778",
    "name": "Create Collection is Not Working",
    "messages": "curl --location --request PUT 'host/collections/collection_name' \\\n--header 'Content-Type: application/json' \\\n--header 'Accept: application/json' \\\n--data '{\n    \"vectors\": {\n        \"size\": 1536,\n        \"distance\": \"Cosine\"\n    }\n}'\n\nThis gives us a 200 response after 10s. \n\nHowever, when I am trying to get the collection, I get the following error : {\"status\":{\"error\":\"Service internal error: The replica set for shard 2 on peer 686755113886562 has no active replica\"},\"time\":0.00052994}\n\nWe do these intermittent warning logs in the server : \n2023-11-30T11:52:53.245103Z  WARN storage::content_manager::consensus_manager: Failed to apply collection meta operation entry with user error: Wrong input: Replica 686755113886562 of shard 0 has state Some(Active), but expected Some(Initializing)\n\nAbout our setup : 5 node cluster, installed manually. On each of the nodes we run docker processes."
  },
  {
    "threadId": "1179786679012315226",
    "name": "Is there any vector count limit in batch upserting?",
    "messages": "Hello, I'm struggling with Bad Request (400) when I try to upsert 1140 vector at once, while it works with 570 vectors.\n\nThere seems not to be documented about this problem (https://qdrant.tech/documentation/concepts/points/#upload-points)..."
  },
  {
    "threadId": "1179832552144506901",
    "name": "Customizing Score Computation in Qdrant",
    "messages": "Is it possible to customize the score computation in Qdrant by incorporating both distance and weight considerations between the query title and query description?\n<#1149327864936808529>"
  },
  {
    "threadId": "1179867468328611852",
    "name": "SELECT all for metadata field.",
    "messages": "I want to get the metadata \"id\" from all my vectors in a collection so I can check I am not adding duplicates in an upload script. What is the best way to do this?"
  },
  {
    "threadId": "1179908438143291482",
    "name": "Does Qdrant force unqiueness on Point.id?",
    "messages": "Does qdrant force Point.id's to be unique? My Point.id is set manually using a UUID generated in python."
  },
  {
    "threadId": "1165911811808301166",
    "name": "Visualizing Loss and Accuracy during training",
    "messages": "Hey, is there any built in way to visualize loss and accuracy during training with quaterion? If not is there anything you recommend to use instead?"
  },
  {
    "threadId": "1177991512911515688",
    "name": "missing snapshot",
    "messages": "I tried to create a full storage snapshot using POST /snapshots...I noticed the machine was doing some writing onto qdrant_storage/tmp/ folder. Now I find the tmp folder is blank and I dont know where I need to look for the snapshot file. I cant see any snapshot folder in the qdrant_storage folder. I am using the default docker 1.6.1 qdrant with no customization of any sort and only defaults as available. Where should I be looking??"
  },
  {
    "threadId": "1166498140409569351",
    "name": "aws",
    "messages": "I have a docker compose with 2 images, it is posible upload it to ECR aws?"
  },
  {
    "threadId": "1175019665697800193",
    "name": "Upgrading persistence size with Qdrant helm chart.",
    "messages": "My Qdrant cluster ran out of space, so I'm trying to increase the storage size, which is set here: https://github.com/qdrant/qdrant-helm/blob/main/charts/qdrant/values.yaml#L119. Running a helm upgrade fails with: \"Forbidden: updates to statefulset spec for fields other than 'replicas', 'template', 'updateStrategy', 'persistentVolumeClaimRetentionPolicy' and 'minReadySeconds' are forbidden.\n\nI've stumbled accross this post: https://serverfault.com/questions/955293/how-to-increase-disk-size-in-a-stateful-set, but I'm wondering what a best practice would be for doing this with Qdrant."
  },
  {
    "threadId": "1161228945094606948",
    "name": "Timeout error while writing to DB",
    "messages": "We had started a cloud instance via AWS with an 8GB DISK, 2GB RAM and 0.5 vCPU. \nWhile writing data to the db from a local Python client a timeout error is thrown.  The RAM and CPU usage is below the maximum capacity. Also the vector count in the database is only 15,444.\n\nEven after scaling up the instance to 2CPU and 8GB RAM the timeout error is thrown. Please find attached additional information to help with the issue."
  },
  {
    "threadId": "1177564621494812692",
    "name": "Backup in local mode",
    "messages": "Hi! I want to use Qdrant in porduction inside a VM. I was wonder if I can keep track of backups in some way. I have tried to use snapshots, but I get this error:\n`NotImplementedError: Snapshots are not supported in the local Qdrant. Please use server Qdrant if you need full snapshots.`\nSince I have allocated the full VM for the qdrant service, I want to avoid using qdrant in a docker container, but I don't know if there is a solution for having backups in local mode."
  },
  {
    "threadId": "1162006203765899294",
    "name": "Using Qdrant and Langchain RetrievalQA",
    "messages": "Hey guys,\nthis question is not especially about Qdrant but in combination of Qdrant and Langchain.\nI am building an app based on  Qdrant that can store multiple documents and I want to ask the LLM about a specific document by telling the the name of the document or furthermore I want to compare multiple documents.\n\nSo far, I created the collection and the vectors are on the cloud and having a payload that looks like this\n{\n  \"name\": \"name of the document\",\n  \"page_content\": \"content of the document\"\n}\n\n\nNow I am using RetrievalQA to ask questions, but the LLM does not know what I am talking about when I mention a specific name of a document/vector that is in the collection!?\n\n\nThis is my code of the RetrievalQA:\n\nllm = ChatOpenAI(temperature=0.0, model_name=\"gpt-3.5-turbo\")\n\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/msmarco-MiniLM-L-6-v3\")\n\nclient = qdrant_client.QdrantClient(\n    host,api_key=qdrant_api,\n    prefer_grpc=False,\n)\n\n\nqdrant = Qdrant(\n    client=client,\n    collection_name=\"Hydrant\",\n    embeddings=embeddings,\n    metadata_payload_key=\"payload\",\n)\n\n\nretriever = qdrant.as_retriever()\nqa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\nquestion = input(\"Gebe eine Frage ein:\")\nanswer = qa.run(question)\nprint(answer)\n\n\n\nAny idea how to fix this? I appreciate any help!ЁЯШК"
  },
  {
    "threadId": "1166121980978790440",
    "name": "Search fails to find vector similar to the exact one retrieved from store",
    "messages": "In the Dashboard I run a\n`GET collections/[collection_name]/points/[point_id]`\nwhich gives me that point's vector data.\n\nAnd then I run a\n```POST collections/[collection_name]/points/search\n{\n  \"vector\": [...]\n...\n```\nwith that very same vector (copy/paste from result) тАУ┬аbut the result does not contain the original point. The highest score is around 0.7 and the original point is nowhere to be found.\n\nShould that search not have the original point as a top result with score 1.0 ??"
  },
  {
    "threadId": "1176538324165476463",
    "name": "returning only vector of record",
    "messages": "Hi i am using Qdrant together with the python client.\n\nI wanted to use the .scroll method to return only the vector of returned points. Is this possible? Or does the client always send back a tuple with a list of qdrant records? Is there a way to optimize obtaining just the matrix of returned vectors?"
  },
  {
    "threadId": "1175670975082278983",
    "name": "Guide to build recommendation system with the power of Qdrant",
    "messages": "I want to build a recommendation system with Qdrant but i really didn't have good experience with recommendation system field. \n\nI want to build a recommendation system to recommend freelancers to posted jobs \nI have two csv files one contain the jobs description, title and some tags \nthe other csv file contain the freelancers data like:\n['name',\n 'username',\n 'about',\n 'rating_average',\n 'rating',\n 'buyer_level',\n 'job_title',\n 'registered_at',\n 'last_seen',\n 'last_seen_human',\n 'country',\n 'state',\n 'tags',\n 'sold_services_count',\n 'completed_projects',\n 'winning_contests',\n 'sales_in_dollar',\n 'customers_count']\nI want to build a recommendation system for this task and  i am wondering is therey any usecase or some documenation, guide, help or anything to let me solve this problem efficiently and quickly!"
  },
  {
    "threadId": "1175568421258264616",
    "name": "Filter based on Langchain metadata",
    "messages": "Asking for a lifeline. \nHow would I use QdrantClient to identify all points that contain the value \"MSM\" in the metadata.source field.  My end goal is to then delete them, so any advice on whether to delete the point or the vector would be appreciated. BTW: if anyone is interested in helping me on this volunteer project (RAG) for the US Coast Guard Auxiliary, please DM me.\n\n\nHere's an example payload: \n{\n  \"metadata\": {\n    \"page\": 162,\n    \"source\": \"References/Not catalogued/MSM Vol III Personnel CMINST 16000.8B Change 2.pdf\"\n  },\n  \"page_content\": \"COMDTINST M16000.8B  \\nUSCG Marine Safety Manual, Vol. III:  Marine Industry Personnel  \\nPART A: MARINER CREDENTIALING  \\nCHAPTER 12:  LICENSING FOR ENGINEERING OFFICERS  \\n \\nA12-2 \\n B. MMD Endorsements Accompanying Licenses.    \\nEngineers holding licenses that authorize service on inspected vessels of more than 2000 \\nhorsepower are entitled by 46 CFR 12.02- 11(d)(2) to an MMD endorsed for any unlicensed \\nrating in t he engine department. Such license holders should be encouraged to obtain an MMD \\n when the license is issued.  In many cases an MMD is required to legally serve aboard a vessel.  (See 46 CFR 12.02- 7). \\n \\nC. Creditable Service.\"\n}\n\nThe code I used always returns [] regardless of the keys and values I use: \n\nfrom qdrant_client.http import models\nfrom qdrant_client import QdrantClient\n\nclient = QdrantClient(path=qdrant_path)\n\nclient.scroll(\n    collection_name=qdrant_collection_name,\n    scroll_filter=models.Filter(\n        must=[\n            models.FieldCondition(\n                key=\"payload.metadata.source\",\n                match=models.MatchValue(\n                    value=\"MSM\"),\n            ),\n        ]\n    ),\n)"
  },
  {
    "threadId": "1168848343766683770",
    "name": "Qdrant create  indexing make collection un responsive, is there any way to use while indexing also.",
    "messages": "Qdrant create  indexing make collection un responsive, is there any way to use while indexing also."
  },
  {
    "threadId": "1168473719262412861",
    "name": "Should i clean the data for resume data before calculating the embedding or not!",
    "messages": "I have a dataset of around 2500 pdf files that i extracted a csv file with the following data:\n1. ID \n2. Resume Str \n3. Resume HTML \n4. Category \nand i want  to build a semantic search for this data. \nmy question is should i clean the data before calculating the embedding or just keep it ? \nshould i clean it with tokenization,lemmtization or just clean the white space? \nhere is a sample"
  },
  {
    "threadId": "1168224876100866119",
    "name": "Qdrant Payload or NoSQL, Which one?",
    "messages": "Hello,\n\nWe are excited about migrationing our entire Tech Stack to the qdrant platform ЁЯЪА. Currently, we are conducting our tests on local machines.\n\nWe would like to discuss a matter with you;\n\nIn your terminology, we equate the concept of \"payload\" to the data held by collections in NoSQL. In our previous projects involving vector databases, we were storing Payload data in a NoSQL database, enabling simultaneous operation with the vector database. Despite qdrant having a vector database feature, it allows filtering on payload data at an entry level. In this context, we are curious if it's truly necessary to use NoSQL for data matching purposes since qdrant already provides payload functionality.\n\nAdditionally, we like to learn about the depth limit in nested filtering operations?\n\nWarm regards."
  },
  {
    "threadId": "1165449210074767440",
    "name": "client.retrieve does not return points in  the order of the ids?",
    "messages": "```\n  records = client.retrieve(\n            collection_name=\"test\",\n            ids=[\"a\", \"b\", \"c\"],\n            with_vectors=False,\n            with_payload=True,\n        )\n```\nI need the returned results to be exactly in the order corresponding to the ids \"a\", \"b\", \"c\".\nBut I notice the results are not in that order.\nIs that expected? And if so, how do I achieve what I want?"
  },
  {
    "threadId": "1149792856333103154",
    "name": "Qdrant Cluster Not Serving Requests",
    "messages": "Hi team,\nWe have deployed Qdrant in Production on k8s and have 5 replicas and are using version 1.3.0.\n\nThe pods have all started up, and all our collections are loaded.\n\nHowever, when I try to search on the collection \n\n{\"result\":{\"status\":\"green\",\"optimizer_status\":\"ok\",\"vectors_count\":2760344,\"indexed_vectors_count\":2729565,\"points_count\":2760344,\"segments_count\":39,\"config\":{\"params\":{\"vectors\":{\"size\":1536,\"distance\":\"Cosine\"},\"shard_number\":5,\"replication_factor\":1,\"write_consistency_factor\":1,\"on_disk_payload\":true}\n\n\n it is just timing out. Each of our pod has 15GB of memory, and 8 cores assigned. On checking the logs we get an error in logs: \n\n[2023-09-08T19:41:52.643Z WARN  qdrant::actix::helpers] error processing request: 1 of 1 shards failed with: Service internal error: Tonic status error: The operation was cancelled[2023-09-08T19:41:52.644Z INFO  actix_web::middleware::logger]  \"POST /collections/collection_name/points/search HTTP/1.1\" 500 139 \"-\" \"PostmanRuntime/7.32.2\" 180.332577\n\nThis keeps on happenign very frequently, and after a point it gets frustrating as all our clients are impacted in the production setup. We don't even know what to do in such cases. \n\nWould request someone to kindly help us out here urgently."
  },
  {
    "threadId": "1174754995514642504",
    "name": "Cannot upload file>2MB to Qdrant",
    "messages": "Hello,\nI have self hosted qdrant on my own VPS server. But I get error on uploading files greater than 2mb. What could be the issue?\n\nThanks for your help"
  },
  {
    "threadId": "1173944223746248745",
    "name": "Qdrant Cloud  replication factor",
    "messages": "Hello team, I have a question. If I use a cloud service and initially replication factor with 1, but later want to scale up to multiple instances, do I need to stop the service to upgrade the number of instances?"
  },
  {
    "threadId": "1174035158463172712",
    "name": "What ingestion tools are available for processing audio, video files from cloud storage",
    "messages": "We have an application which stores audio, video files in AWS or Google storage. We would like to use an ingestion tool which creates vector embeddings on the fly and ingest the vector embeddings into qdrant on a real time basis as an when the files are uploaded into the cloud storage. What ingestion options are currently available for use with qdrant"
  },
  {
    "threadId": "1173984089934676079",
    "name": "Distributed Deployment on EC2",
    "messages": "Hi,\n\nI am trying to set up a distributed deployment on EC2 in order to be able to evaluate the database as a fit for being used in the company and I am having some issues. \n\nI have been able to run the first node by running `docker run -p 6333:6333 -v $(pwd)/custom-config.yaml:/qdrant/config/config.yaml qdrant/qdrant ./qdrant --uri 'http://10.44.42.218:6335'` and it starts correclty and can interact with it just fine. However, when I try to start a second node on another EC2 instance with the command `docker run -p 6333:6333 -v $(pwd)/custom-config.yaml:/qdrant/config/config.yaml qdrant/qdrant ./qdrant --bootstrap 'http://10.44.42.218:6335/'` it does not start and throws the error I am sending in the first reply.\n\nI checked and the connectivity between instances works (I can do a curl to the first node's endpoints) and I verified that the security groups allow port 6335 too. Any idea on what is happening?\n\nMany thanks in advance"
  },
  {
    "threadId": "1173977414473355357",
    "name": "Native binary vector storage on cloud platform",
    "messages": "Hi, I'd like to store binary vectors that are 2500 dimensions in size.  I was just wondering if there are any special settings that I'd need to apply to leverage storing and retrieving these efficiently. For example I know that you support binary quantisation, but that's not needed here, unless enabling it switches on some kind of under the hood binary vector optimisation?"
  },
  {
    "threadId": "1173870845869969508",
    "name": "How does Qdrant store metadata?",
    "messages": "Does qdrant use disk or RAM for metadata? \n\nFor performance, RAM has to scales with index / vector size (ideally we have enough ram for the HSNW index). However, will metadata scale with disk space or do we need some of it in RAM?"
  },
  {
    "threadId": "1171348908249714728",
    "name": "dashboard",
    "messages": "Hi Guys! I neede to figure out dashboard, couldn't go further could you give some advices or  any tutorial on that, i was using user manual on documentation but couldn't understand ЁЯШж\nThank you in advance!"
  },
  {
    "threadId": "1171758112429068388",
    "name": "Get Error when embedding the pdf file",
    "messages": "I'm new to Qdrant server on Mac Studio.\nI was trying to create a *.py file to embedding the pdf file. But it cannot be done as expected. I hope you can help me see why it can't be done. Thank you so much.\nThe error as below\n\npython '/Users/nad/Desktop/embeding_Qdrant.py'\nTraceback (most recent call last):\n  File \"/Users/nad/Desktop/embeding_Qdrant.py\", line 49, in <module>\n    embed_and_save_single_pdf(pdf_to_embed, collection_name)\n  File \"/Users/nad/Desktop/embeding_Qdrant.py\", line 42, in embed_and_save_single_pdf\n    qdrant_client.upsert(collection_name=collection_name, points=vectors)\n  File \"/Users/anaconda3/lib/python3.11/site-packages/qdrant_client/qdrant_client.py\", line 796, in upsert\n    return self._client.upsert(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/anaconda3/lib/python3.11/site-packages/qdrant_client/qdrant_remote.py\", line 1050, in upsert\n    points = models.PointsList(points=points)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"pydantic/main.py\", line 341, in pydantic.main.BaseModel.__init__\npydantic.error_wrappers.ValidationError: 384 validation errors for PointsList\npoints -> 0\n  value is not a valid dict (type=type_error.dict)\npoints -> 1\n  value is not a valid dict (type=type_error.dict)\npoints -> 2\n  value is not a valid dict (type=type_error.dict)\npoints -> 3"
  },
  {
    "threadId": "1172193578345300059",
    "name": "Recover from snapshot error",
    "messages": "I asked a question in the thread, but I didn't get an answer, so I'm trying again here.\nI need to copy a collection from one server in cluster mode to another server in single mode.\nI tried to do it through the snapshot POST api (collection and shard), but when I used PUT api to restore from snapshot on the another server, I got an error `Can't restore snapshot is local mode with missing data at shard`.\nWhat did I do wrong? Please tell me how to transfer a collection from one server to another."
  },
  {
    "threadId": "1171747321462128701",
    "name": "Issue with creating snapshots using Qdrant UI",
    "messages": "Hi guys, are there any issues with creating snapshots using Qdrant UI (qdrant version 1.6.0), I created snapshot and uploaded it to new qdrant instance and it migrated only half of the collection. Any workarounds? \n\nbonus case -\nsometimes it migrates empty collection (0 points) or\nyou it downloads empty snapshot file."
  },
  {
    "threadId": "1171544046473330719",
    "name": "qdrant_client.http.exceptions.ResponseHandlingException: [Errno 99] Cannot assign requested address",
    "messages": "Hello I am trying to connect to a qdrant docker image and and creata a collection, But this error keeps coming up. Please let me know how do I fix this."
  },
  {
    "threadId": "1171482519380693013",
    "name": "How can it be that scroll is slower than exact search?",
    "messages": "I have just started with Qdrant. In my usecase I need exact scores for a set of vectors compared to a filtered subset of my database of vectors. Therefore I was interested in the scroll method to return all vectors that pass a certain condition, and then compare them outside qdrant. However, then I found that using the same filter, and with exact set to True in the SearchParams, the operation time decreases. See my test results when comparing 1 vector to query_size in the picture. \n\nhow can this happen? In both operations, qdrant would need to go through the collection and filter the payload and retrieve those embeddings, but in the search version, it would stil need to do a dot product (i believe even without GPU)\n\nSome info about setup:\nQdrant via docker daemon\npython client\nApple macbook air M2\n\nCollection info:\n6.5 million vectors of size 20\nAll points have a payload of doc_id: int\nAbout 1.3 million unique doc_ids, each doc has 1-10 (avg 5) points associated with it\nSimilarity measure: dot_product\n\nFilter\n```models.Filter(\n        must=[\n            models.FieldCondition(\n                key = 'doc_id', match = models.MatchAny(any=query)\n            )\n        ]\n    )\n```\nwhere query is a list of document_ids of length query_size, randomly generated\n\nScroll\n```     output = client.scroll(\n        collection_name='test_collection_3',\n        scroll_filter=f,\n        with_vectors=True,\n        limit = 100000)\n```\nSearch\n```    output = client.search(\n        collection_name='test_collection_3',\n        query_filter=f,\n        query_vector=[0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6],\n        with_vectors=True,\n        search_params=models.SearchParams(exact=True),\n        score_threshold= 7,\n        limit = 100000)\n```\nPlease let me know if you see why this is possible? Am i not performing the scroll in an optimised manner? Or do i fundamentally not understand how qdrant works? Would love to hear from you"
  },
  {
    "threadId": "1171395952490315796",
    "name": "Fetch latest inserted id",
    "messages": "Hi Team, is there a way we could retreive the latest inserted point id to the vector DB?"
  },
  {
    "threadId": "1167835699169472522",
    "name": "Getting Started README",
    "messages": "Hi, I was just trying the Getting Started notebook (getting_started.ipynb), and noticed that the screenshot for `docker run` output differed from my own. (I'm running Ubuntu 22.04.) I had some success with the guide, as it successfully ingested the files I selected.\n\nBut the screenshot made me wonder if I was doing something wrong.\n\nI note the following differences (between the screenshot in https://github.com/qdrant/examples/tree/master/qdrant_101_getting_started and what I see on my system):\n\n- I start from my home directory\n   - The first line (`qdrant_chl git:(master)`) suggests that we should start from a Git repo. From my home tests, I get the same result whether I start in my home directory or the qdrant repo\n- When I run the command (`docker run -p 6333:6333 -v $(pwd)/qdrant_storage:/qdrant/storage qdrant/qdrant`) my output points me to a different URL, `http://localhost:6333/dashboard` \n   - The current screenshot points to https://ui.qdrant.tech/?v=v1.1.1 as the \"Web UI\". It's a Qdrant OpenAPI (\"Swagger\") page, \n\nSo I see two possibilities:\n\n1. The page needs updated instructions that help the user match what's shown in the current screenshot (and explain the link to the OpenAPI page)\n2. The screenshot should be revised\n    a. In both cases, maybe the screenshot should be revised, as I don't see a `qdrant_chl` repository.\n\nLink to current screenshot: https://github.com/qdrant/examples/blob/master/images/docker_qdrant.png"
  },
  {
    "threadId": "1170617842786386002",
    "name": "Configure no of search threads in recreate_collection call",
    "messages": "I see it is possible to specify this in the config.yaml but can it be done via the recreate_collection call? This is my code right now: \nresp =  client.recreate_collection(\n            collection_name=f\"{collection_name}\",\n            vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),\n            hnsw_config=models.HnswConfigDiff(on_disk=False, m=48, ef_construct=1000),\n            optimizers_config=models.OptimizersConfigDiff(max_optimization_threads=12, indexing_threshold=indexing_threshold),\n            on_disk_payload=False,\n            )\n\nIs there a way to modify this to include no. of search threads? \nPS: I already have a collection created with points indexed, so I dont want to reset the whole collection. I just want to change the threads param? Was also wondering what the default no. of search threads is? I'm running qdrant via docker."
  },
  {
    "threadId": "1168838564608479232",
    "name": "Hello. Is there any file size limit that i can upload on qdrant?",
    "messages": "Hello. Is there any file size limit that i can upload on qdrant? I've self-hosted qdrant and few pdf files (700+ pages) are not getting upserted. I get error 500 (bad request)"
  },
  {
    "threadId": "1171489834108456961",
    "name": "Why my vector is None ?",
    "messages": "Hello, I'm new to qdrant and I don't understand why when I upsert a point with a vector, the vector is None when I scroll the collection. For example, this code :\n```\nimport uuid\nfrom qdrant_client import models, QdrantClient\nfrom sentence_transformers import SentenceTransformer\n\nMY_COL = \"my_col\"\n\nencoder = SentenceTransformer(\"all-MiniLM-L6-v2\", device='cpu')\nclient = QdrantClient(host=\"vecdb\", grpc_port=6334, prefer_grpc=True)\nclient.recreate_collection(MY_COL, vectors_config=models.VectorParams(\n        size=encoder.get_sentence_embedding_dimension(),\n        distance=models.Distance.COSINE,\n    ),\n)\n\nmy_dict = {\n    \"name\": \"test\",\n}\n\nid = uuid.uuid4().hex\nvector = encoder.encode(my_dict[\"name\"]).tolist()\n\nfor i in range(3):\n    print(vector[i], end= \" \")\nprint(\"...\")\n\nclient.upsert(MY_COL, points=[models.PointStruct(id=id,payload=my_dict,vector=vector)])\nhits = client.scroll(MY_COL)\n\nprint(hits)\n```\ndisplay :\n```\n0.011573401279747486 0.025136196985840797 -0.03670182824134827 ...\n([Record(id='caa07c47-07de-414f-8e3a-c3b17b6595f0', payload={'name': 'test'}, vector=None)], None)\n```"
  },
  {
    "threadId": "1157341219068850186",
    "name": "Grouping news articles by topics",
    "messages": "We are trying to do a proof of concept, wherein we would like to cluster a bunch of news articles based on a topic of discussion (say title and body). What call can we use to fetch the list of articles grouped by topic automatically across a certain language and eventually across similar news in other international languages apart from english (say french, spanish, german ..). Appreciate any advice on this"
  },
  {
    "threadId": "1170256056560124004",
    "name": "Search API",
    "messages": "Anyone have any idea why my basic search request is failing for '[Errno 111] Connection refused'? I'm able to connect and upsert vectors but can't search them. \n\nqdrant_api_key = os.environ.get(\"QDRANT_API_KEY\")\nqdrant_url = os.environ.get(\"QDRANT_URL\")\n\n client = qdrant_client.QdrantClient(\n            url=qdrant_url, api_key=qdrant_api_key)\n\n        res = client.search(\n        collection_name=\"main-collection\",\n        query_vector=queryVector,\n        with_vectors=True,\n        with_payload=True,\n        limit=1,\n        )"
  },
  {
    "threadId": "1161976258238886020",
    "name": "Default cleaning?",
    "messages": "I have yet to explicitly make any calls to delete points, but when I check point count with /collections/collection_name, it is clear that it is not accumulating all points.\n\nIs Qdrant doing some cleanup/delete by default?"
  },
  {
    "threadId": "1153398394195288136",
    "name": "StatusCode.DEADLINE_EXCEEDED",
    "messages": "Hi, we're using qdrant cloud, and we're starting to consistently see a `StatusCode.DEADLINE_EXCEEDED` error in production. We're using the Python SDK. We instantiate the client using the `QdrantClient` constructor and call its methods pretty much immediately after instantiation, so I'm not too sure why we're seeing this error so often."
  },
  {
    "threadId": "1158699661650034748",
    "name": "Optimization task panicked",
    "messages": "Hello everyone!\n\nI'm importing half a million records to Qdrant, but I'm getting a strange error:\n\n```\n\"Service internal error: Optimization task panicked: range end index 8334336 out of range for slice of length 8332800\"\n```\n\nAny idea what might cause this?\n\nQuick note: With version 1.4.1 I had no problems when importing that much records."
  },
  {
    "threadId": "1168580655962132620",
    "name": "Returning duplicates with batch searching",
    "messages": "I like using the [batch search](https://qdrant.tech/documentation/concepts/search/#batch-search-api)  API in qdrant because it really speeds things up. It *seems like* it doesn't return duplicates, however. I also don't see anything about that in the [API spec](https://qdrant.github.io/qdrant/redoc/index.html#tag/points/operation/search_batch_points).\n\nThe team is interested in running a search **for each vector** and then returning the hits that appear the most. For example, suppose I have 50 text/natural language queries and want to run a search against each 50 of those. Assume each query returns the same item ID for all 50 searches. In that instance I'd want to bubble that result up to the top. If the API supported returning duplicates, then we could count occurence of each id and then bubble those up. However, the result of `search_batch` doesn't seem to return duplicates, It seems to just return `50*k` unique hits.\n\nI can achieve the above by making 50 unique calls, but thats horribly inefficient. Is there a way to return duplicates with batch searching? Or is the return of batch search already doing this for me, bubbling up results that occur many times inside the HNSW store?"
  },
  {
    "threadId": "1171114377055842434",
    "name": "Difference between Neural Search Service and Semantic Search",
    "messages": "I am reading your documentation on Neural Search Service and Semantic Search. And I canтАЩt understand what the difference is? \n\nThe difference is what's in neural search deploy the search with FastAPI?\nBut does the search itself occur in the same way as in a regular semantic search?\n\nAm i right?"
  },
  {
    "threadId": "1170142305303396386",
    "name": "Is there a black and white qdrant logo available?",
    "messages": "I'd like to include a logo in our app's search panel \"powered by Qdrant\" but our color scheme is minimalist black and white with some cyan undertones. The red doesn't really work.\n\nPerhaps there is a black and white version available? If not would it be ok to change the colors to someting more laid back to make it blend better with our app?"
  },
  {
    "threadId": "1170670043210719303",
    "name": "Range filter on unix timestamps",
    "messages": "Hello, \n\nWe tried to filter in vector search using a unix timestamp with gte and lte. \n\nHowever it is too slow since it has to read the payload on disk when searching. \n\nI figure it would be better to take the iso-date and do a payload index on that instead.  But I do not see any documentation if range filter would work for anything that isn't an integer / float."
  },
  {
    "threadId": "1159859669741879306",
    "name": "Disparity in collections count using init_from",
    "messages": "I noticed a slight reduction in count on creating a collection from another collection using init_from.\nThis is how I created\ncurl -X PUT --location 'http://192.168.10.122:6333/collections/us_fto_collection_denorm' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"vectors\": {\n      \"size\": 1024,\n      \"distance\": \"Cosine\"\n    },\n    \"init_from\": {\n       \"collection\": \"us_fto_collection_tmp\"\n    }\n}'"
  },
  {
    "threadId": "1159172043774894200",
    "name": "Snapshot behaviour",
    "messages": "Hello,\n\nI have a small question about snapshots. We have an helm cluster deployment of Qdrant and we want to setup snapshots. When running the `POST /collections/{collection_id}/snapshots`, with the snapshot path configured to the PVC of Qdrant nodes, I see a folder is created for my collection on each node's PVC, but only one receives the actual file.\nI was wondering if I can mount a single PVC to all my nodes just for the snapshot ? Therefore I can avoid looping through each of my PVC before finding the file.\n\nThank you by advance,\nLouis"
  },
  {
    "threadId": "1158321006272847892",
    "name": "Suggestion words to increase the similiarty score",
    "messages": "I have two document file one called my_resume.txt and the other called my_job_desc.txt I have calculate the embedding with sentence_transformer and then calculated the embedding and added them to qdrant.\nthe score is for example a 60% matching \nI also processed the txt into a json file which contains the  extracted keywords(\"Job\",\"Front\",..etc) and keyterms (\"modern javascript framework\"),0.261310 ...etc)  \nhow to can i do  the suggestion of the keywords that i can get from the job_description into the resume to increase the similiarity score !? is therey any methods or ways that is related the qdrant or it's related to other nlp preprocessing task!?any recommendation or help in this will be appreciated \nThe way i  extracted the keyterms is with textacy and sgrank"
  },
  {
    "threadId": "1170440445239758921",
    "name": "Vector indexing stopped working suddenly",
    "messages": "Hello, our collection on kubernetes of 400k points suddenly stopped working on the /search/ endpoint with an internal timeout. \n\nI noticed that the indexed_vector count was equal to 0 and if I search with indexed_only set to true, it will return an empty list. The memory usage in the pod also seems very low at 200MB, where it uses 2 gigabyte when I run it locally with the same collection. The limit on kubernetes is set to 20GB memory, so it seems very odd..."
  },
  {
    "threadId": "1158498211711950858",
    "name": "What is the equivalent async API to get all the collections from qdrant.",
    "messages": "I am using the qdrant_client.get_collections() to get the list of collections. I need an equivalent async method for this. \nCan anyone help with this."
  },
  {
    "threadId": "1151395842444513280",
    "name": "Binary Quantization / RAM Usage",
    "messages": "Hello team,\n\nI have ~190k vectors saved and was almost limited by RAM.\nI was using scalar quantization so I decided to use binary quantization to compress the space my vectors take up in memory (by a 32x factor, right?). \n\nIn practice, this seems to have worked well, but I have the feeling that on monitoring, I've completely smashed the RAM limit. What's wrong?"
  },
  {
    "threadId": "1168571201828946022",
    "name": "Only negative examples in Recommend API",
    "messages": "We have a database of 432K images that are vectorized by Unicom ViT. We are trying out the recommend API and the positive examples work very well and return results as expected. \n\nWe are also interested in results given only negative examples to find vectors that are furthest away from the input, and although there are in fact many other vectors, it's giving us the exact same vectors as the nearest ones. We are using the *Best Score* strategy. We tried both point ID and its vector values, but it did not change the result.\n\nWe are using Qdrant v1.6.1 and the Go client v1.6.0. \n\nIs there a known issue about this, or are we doing something wrong?"
  },
  {
    "threadId": "1169270818967130205",
    "name": "Is it ok to use ULID as identifier for points?",
    "messages": "I am very new and would like to ask this simple question. Is this advised to scalability vs integer ids or doesn't matter with qdrant?"
  },
  {
    "threadId": "1158272095516643338",
    "name": "How to ignore missing points or check for a list of points before calling recommendation api?",
    "messages": "I'm making a new version of the recommendation system based on Qdrant and ran into a problem:\nsome data may not be synchronized with Qdrant which leads to errors.\nIs there any way to check points existense before sending request to recommendation api?\n\nPlease look at this https://github.com/qdrant/qdrant/issues/2313, it looks like someone has already encountered a similar situation.\n It would be nice to be able to get a list of the data that is already in Qdrant in order to modify the request or make sure that Qdrant does not crash with an error (I think this would be better in my situation)."
  },
  {
    "threadId": "1169444636708843581",
    "name": "Filtering by relationship, how to store?",
    "messages": "I have a situation where I store ~8M images.\n\nI am adding a feature where others can like images, and view what they liked. In Postgres user_id/image_id is stored in another table to represent what they have liked and they can be retrieved that way.\n\nIтАЩd like users to be able to search what they liked, semantically with Qdrant. \n\nI had thoughts about storing the user IDs as payload on each vector to represent who liked something, but I would upsert frequently and it seems very inefficient to do it that way.\n\nIs there a recommended way to achieve this scenario in Qdrant?"
  },
  {
    "threadId": "1169371533806948452",
    "name": "Qdrant with Docker Segmentation error",
    "messages": "My simple app works from my desktop. That is with python service_v2.py. (code given below)\n\nSomething simple is casuignt his problem. Please help \n\nBut with Docker container, I get a segmentation error with curl .\n\n curl -X POST http://0.0.0.0:8000/api/search -H \"Content-Type: application/json\" -d \"{\\\"query\\\":\\\"innovation\\\"}\"\n\n\n\nMy Service_v2.py \n\n```python\n\n#%%\nfrom qdrant_client import QdrantClient\nfrom sentence_transformers import SentenceTransformer\n\nclass NeuralSearcher:\n\n    def __init__(self, collection_name):\n        self.collection_name = collection_name\n        # Initialize encoder model\n        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n        # Initialize Qdrant client\n        self.qdrant_client = QdrantClient(\n            url=\"https://a280s-east4-0.gcp.cloud.qdrant.io:6333\", \n            api_key=\"d_77oAA2iw\")\n\n    def search(self, text: str):\n        # Convert text query into vector\n        print(\"Converting text to vector\")\n        vector = self.model.encode(text).tolist()\n        print(\"Vector created\")\n\n        # Use `vector` for search for closest vectors in the collection\n        print(\"Initiating vector search\")\n        try:\n            search_result = self.qdrant_client.search(\n                collection_name=self.collection_name,\n                query_vector=vector,\n                query_filter=None,  # No filters for now\n                limit=5  # Limiting to 5 closest results\n            )\n            print(\"Vector search completed successfully\")\n\n            # Process search results\n            payloads = [hit.payload for hit in search_result]\n            if payloads:\n                print(f\"Found {len(payloads)} results\")\n            else:\n                print(\"No results found\")\n\n            return payloads\n\n        except Exception as e:\n            print(f\"Error during vector search: {str(e)}\")\n            raise e\n\n#%%\n\n\n```"
  },
  {
    "threadId": "1162449981173670041",
    "name": "Trouble replicating shards but only for one node",
    "messages": "Hi all, I'm self-hosting a Qdrant cluster of 4 nodes and trying to replicate shards.\n\nWas able to replicate to/from nodes A, B, and C, but on node D, replication to and from either fails (results in a dead shard) or takes forever (much longer than others).\n\nCurrent shards on Node D are \"active\", and I don't see anything in that node's logs or metrics to indicate a problem.\n\nHow can I diagnose and solve?"
  },
  {
    "threadId": "1167709242090868807",
    "name": "filter",
    "messages": "Partial text match filtering - is it possible?\n\nMay I build filter with partial text match in one of the keys?\n\nFor example the filter below works, find all entities that include the text \"yoga\", \"fitness\" or \"hiking\" rather than have full match with the values?\n\n\nPOST collections/products-1-1536/points/scroll\n{\n  \"limit\": 10,\n  \"filter\": {\n    \"should\": [\n      {\n        \"key\": \"description\",\n        \"match\": {\n          \"any\": [\n            \"yoga\",\n            \"fitness\",\n            \"hiking\"\n          ]\n        }\n      }\n    ]\n  }\n}"
  },
  {
    "threadId": "1169159052564107304",
    "name": "Deploying a qdrant cluster with Kubernetes, and Storage Related questions",
    "messages": "Hello experts and folks from Qdrant. Is there a way to deploy a Qdrant Cluster using Kuberntes with 3 or 5 replicas? or is the only way to create a distributed deployment using https://qdrant.tech/documentation/guides/distributed_deployment/ which i assume will run as VMs..\n\nAlso on the storage portion, lets say i assign 32GB of RAM per Pod (if k8s) or VM and i have 1TB of Storage mounted to the Pod or VM, how will storage of the vectors work? will the vectors sit in memory, while a copy of the vectors stay safe on persistent disk? or a portion of the vectors will be in Memory, and the rest on disk? i.e if the Pod/VM fails, whatever is in memory will be lost?"
  },
  {
    "threadId": "1166215614554198046",
    "name": "Crashing Shortly AFter Start",
    "messages": "Hi all, Just started getting this issue where qdrant crashes shortly after starting. It crashes after it's been posted to a couple times. No error message just completely stops"
  },
  {
    "threadId": "1168869240040861706",
    "name": "Question about the recommend_groups API and limitations of the food discovery app.",
    "messages": "Hello Andre and Qdrant team,\nI recently setup the newer food discovery demo so I could understand the recommend groups api a bit better. You guys wrote a bunch of great code around the \"handle\" function to handle cold start and all the different scenarios, kudos!\n\nOne thing I noticed is that some searches return the same images multiple times, this is because there are multiple of the same images in the dataset.\n\nIf I wanted to extend the qdrant demo application to do an additional grouping by images, what would the ideal approach be?\n\nWould it involve:\n1. Returning the vectors from the recommend_groups API.\n2. Preforming a HNSW on the vector that are returned in the results.\n3. Grouping and returning in some custom json payload\n\nI asked a similar question a couple weeks back linked here but that was a bit more customized. https://discord.com/channels/907569970500743200/1163335538271596654/1163850403843166259 I'm not sure if the same score threshold or MRR would apply\n\nIn this case i'm specifically wanting to extend the qdrant food discovery demo and hopefully contribute some code."
  },
  {
    "threadId": "1167577307255414895",
    "name": "Load already normalized vectors to use it with cosine similarity",
    "messages": "I've been using cosine similarity in python and I already have normalized embeddings. I see that qdrant automatically normalize the vectors when  cosine similarity is used and I don't see any option to disable this behavior. Is it possible to use cosine similarity avoiding this automatic normalization? I process the embeddings offline to do relevant stuff (i have to compute cosine similarities and at this stage don't need any kind of database to do it) and the original embeddings are lost, so i don't have them at the time of loading them into the database."
  },
  {
    "threadId": "1166740430084386847",
    "name": "Help Reducing Disk Writes",
    "messages": "So we are still having to host Qdrant on ECS with EFS for persistence.\n\nOur indexing of new documents is kicked off by a number of event publishers that are all at a 1 minute cadence. As we have been growing we have seen a large increase in EFS writes as Qdrant persist and optimizes the index.\n\nWe looked at the configuration option and noticed we could increase the interval for optimization. Would this reduce writes significantly in anyway?\n\nhttps://github.com/qdrant/qdrant/blob/master/config/config.yaml#L90\n\nAre there any other settings for Qdrant generally, or on a collection level we could set to reduce writes?"
  },
  {
    "threadId": "1166745915411927040",
    "name": "Getting UserWarning: \"Api key is used with unsecure connection\" When Using QdrantClient",
    "messages": "Hello everyone,\n\nI'm currently working on a Python project that utilizes Qdrant for managing vector storage. I've deployed Qdrant using its official Docker image in production mode.\n\nHere is the relevant part of my Python code: ```from qdrant_client import QdrantClient\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores.qdrant import Qdrant\n\nclass QdrantManager:\n    def __init__(self, api_key, api_url, embedding_model_name, user_id):\n        self.QDRANT_API_KEY = api_key\n        self.QDRANT_API_URL = api_url\n        self.EMBEDDING_MODEL_NAME = embedding_model_name\n        self.USER_ID = user_id\n        \n        self.qdrant_client = QdrantClient(\n            url=self.QDRANT_API_URL,\n            api_key=self.QDRANT_API_KEY,\n            prefer_grpc=True,\n        )\n        # ... rest of the code``` When I run the code, I get the following warning:  ```UserWarning: Api key is used with an unsecure connection.\nwarnings.warn(\"Api key is used with unsecure connection.\")``` I understand that this warning likely refers to some security concerns. Could anyone shed some light on the issue and suggest a way to resolve or suppress it?\n\nThank you in advance for your help!"
  },
  {
    "threadId": "1166721173715951657",
    "name": "Fulltext bug??",
    "messages": "I noticed something a bit different after querying a fulltext index field. In my query I was passing \"tokenA tokenB\" but the top result was one with only \"tokenA\". Interestingly a result with \"tokenA tokenB and a few other tokens\" was returned AFTER this result. Is that the expected behavior. I was using \"must\" and not \"should\" in my filter. From my experience with other  full text engines , its not the required or expected behaviour. Please guide on how to make this work as expected."
  },
  {
    "threadId": "1161252210441715793",
    "name": "Vector Search refinements",
    "messages": "As a preface:\n--------------\n- I run Qdrant as a docker image client on a VM\n- I use the langchain integration of qdrant to upsert points (using add_texts)\n- I use a single collection to host all my vector points, where each point has a unique id entry containing: payload information (page metadata, page text content), and vectors of d=1536 dimensions (OpenAIEmbeddings). each point represents a single page of document information, the collection represents all documents in my database.\n- I implement quantization and optimisation configs in my collection params and in my search params (but havent implemented hnsw configs)\n\nWhat I'd like help with:\n-----------------------\n<1> My search results are monotonous. Two different pages from two completely different documents that are entirely unrelated will produce scores that have a difference of  < 0.002, this in turn produces poor and contextually inaccurate search results. What could be responsible for this?\n\n<2> In what ways can I realistically adjust my quantization, optimisation, and hnsw configs? The guides available in the documentation don't go very deep into the relationships between parameters and their effects on the algorithm and its search/store capabilities (aside from disk space and speed, but even then at a shallow level). Any documentation or resource more thorough that I'd be able to refer to?\n\n<3> Does Qdrant have Re-Ranking functionality for search result refinement?\n\nMy goal:\n---------\n- I'd like to be able to run a search request on a vectorstore instance for a collection of document pages and see accurate search results. as it stand, my queries will return inaccurate and irrelevant results very often.\n\nReference:\n-----------\nI've attached a python file containing relevant snippets from my code that: sets up configs, creates the collection if it doesnt exist, creates a vectorStore instance, performs vector search"
  },
  {
    "threadId": "1166361919154827345",
    "name": "add new named vector",
    "messages": "I made a collection with named vector and put the vectors. After some time, I wanted to add a new named vector to the collection. But when I did this, the data of the previous named vector was deleted. I did this with recreate. When I did this with the update collection, I encountered an error that I could not find anything about and did not understand. I need your help on this. Thank"
  },
  {
    "threadId": "1163050734737162350",
    "name": "correct config setting/model calls",
    "messages": "this is trivial, but to be sure, ive add the code i use to define the configs for my created collections below. is the way im defining and calling them appropriate? i ask cause there are a number of variations in the parent callers within the client library and within the creation function (e.g. vector config can contain other config)\n\ndef setUp(self):\n  self.qd = qdrant_client.QdrantClient(url=self.qdrant_endpoint)\n  \n  self.vectors_config = qdrant_client.http.models.VectorParams(\n      size=1536,\n      distance=qdrant_client.http.models.Distance.COSINE\n  )\n  \n  self.quantization_config=qdrant_client.http.models.ScalarQuantization(\n      scalar=qdrant_client.http.models.ScalarQuantizationConfig(\n          type = qdrant_client.http.models.ScalarType.INT8,\n          quantile = 0.99,\n          always_ram = True,\n      ),\n  )\n  \n  self.optimizers_config=qdrant_client.http.models.OptimizersConfigDiff(\n      memmap_threshold = 20000,\n      \n      deleted_threshold = None,\n      vacuum_min_vector_number = None,\n      default_segment_number = None,\n      max_segment_size = None,\n      indexing_threshold = None,\n      flush_interval_sec = None,\n      max_optimization_threads = None,\n  )\n  \n  self.hnsw_config=qdrant_client.http.models.HnswConfigDiff(\n      m = 64,\n      ef_construct = 512,\n      on_disk = True,\n      \n      full_scan_threshold = None,\n      max_indexing_threads = None,\n      payload_m = None,\n  )\n  \n  self.wal_config=qdrant_client.http.models.WalConfigDiff(\n      wal_capacity_mb = None,\n      wal_segments_ahead = None,\n  )\n  \n  collection_names = [collection.name for collection in self.qd.get_collections().collections]\n  if \"TEST\" not in collection_names:\n      self.qd.create_collection(\n          collection_name=\"TEST\",\n          vectors_config=self.vectors_config,\n          quantization_config=self.quantization_config,\n          optimizers_config=self.optimizers_config,\n          hnsw_config=self.hnsw_config,\n          wal_config=self.wal_config,\n      )"
  },
  {
    "threadId": "1166263225541730334",
    "name": "Bug in ids of documents on dashboard",
    "messages": "Hi - I have created the id of the document using following hash function.\n``` record_id, _ = mmh3.hash64(file_data, signed=False)\n```\nIf I check the ID using python client, I get the actual id. On Qdrant dashboard, it seems that we approximate the value to nearest thousand\nSuppose the actual id is `298631332630714982` . On UI, we get id `298631332630715000` \nIt makes it slightly difficult to use the qdrant dashboard using record_id."
  },
  {
    "threadId": "1165987075334606900",
    "name": "Is there any way to avoid limit in scroll request?",
    "messages": "I am using the scroll method to get some specific items in my database (or to get all points in my collection is another use case). I want to get all the items that has a specific metadata value, but I don't want to limit the number of results. I think setting the lmit value to a high number is not the solution as I don't know how the number of points will grow. Is there any way to override the limit value and return all items?"
  },
  {
    "threadId": "1165235517986701383",
    "name": "How to  change  distance  dot to  update cosine distance ?",
    "messages": "I want to be change distance in existing collections \n<#1149327864936808529>"
  },
  {
    "threadId": "1162775132482314322",
    "name": "Pros and cons of increasing `max_request_size_mb`",
    "messages": "Hey everyone,\n\nI noticed some of my payloads are starting to exceed the `max_request_size_mb`. Naively, I am tempted to simply double it to 64 mb. But can you provide me some deeper insights into why it may be a bad idea to increase this value? And if so, should I be looking to gain performance by breaking up my payloads into 32 mb chunks?"
  },
  {
    "threadId": "1162355485282934915",
    "name": "Around Recovering Snapshots",
    "messages": "If I have to move a collection ( 5 shards 1 replica) from one cluster (5 nodes)  to another cluster (5 nodes), do I have to go on every node, take a snapshot, download the file, and upload it on a node, and then restore that particular snapshot on that node?"
  },
  {
    "threadId": "1163959259285950474",
    "name": "qdrant_client.update_vectors() not working",
    "messages": "```python\nresult = client.update_vectors(\n    collection_name=\"test123\",\n    points=[\n        models.PointStruct(\n            id=0,\n            vector={\n                \"image\": vector\n            }\n        )\n    ]\n)\n```\nI got the error:\n```bash\n{'detail': '1 validation error for UpdateVectors\\npoints -> 0 -> payload\\n  extra fields not permitted (type=value_error.extra)'}\n```"
  },
  {
    "threadId": "1163809411249225879",
    "name": "Unable to download snapshot from dashboard",
    "messages": "I am trying to download a snapshot from the Qdrant dashboard. I navigate to the collection snapshots, the snapshot I need is listed as 4.5 gb. I need to download that, so I click \"download.\" \n\nQdrant downloads a `.snapshot` file that is 0 bytes in size. I also tried clickng on the name to download it, same thing.\n\nAny ideas why I cnanot download the snapshot using the dashboard?"
  },
  {
    "threadId": "1163335538271596654",
    "name": "Not sure if the group API can handle my scenario. Thoughts?",
    "messages": "I have documents in qdrant and each one has a text embedding, an image embedding and a payload. I am currently doing searches on the text embedding and it returns results. All of these results have the words I searched for, the words might be in a different order but they all match. The images that are returned are duplicates or they are very very similar.\n\nI'm trying to figure out if the grouping API can de-duplicate or merge the results based on the image vectors.\n\nFor example- searching for a *Harry Potter* book returns 70 results from 10 different places. There are only 7 different harry potter books so there are 7 unique images with 10 instances each. Can these be grouped together?"
  },
  {
    "threadId": "1162275389641592852",
    "name": "qdrant fastembedding on serverless lambda function",
    "messages": "Hello everyone o/\n\nWe were doing same tests on QDRANT and hit a blocker and wanted to know any fix/suggestions.\n\nWe have some serverless lambda functions on AWS. One of our endpoints is basic add operation to QDRANT DB. \n\nWhile using the QDRANT[fastembed], add operation uses fastembed in the background. \n\nfastembed creates a cache file in the working directory. This file has a parameter to set its path but it is not possible to set it through QDRANT_CLIENT. This results to failing add operation because serverless lambda in AWS does not allow to create any files on the disk except **\"/tmp\" **folder. We would like to know if anyone used QDRANT[fastembed] with serverless and was able to implement it.\n\nBest Regards,\n\nAlim"
  },
  {
    "threadId": "1162147580768161822",
    "name": "Does Qdrant vectorize out of the box?",
    "messages": "I would like to store a lot of descriptions in a db and then find similar ones. Does Qdrant vectorize out of the box or do I have to do it manually?"
  },
  {
    "threadId": "1161812088633700372",
    "name": "Different Filter behaviour Rest-Api/Python-Client",
    "messages": "I am currently working on a filtered search. My goal is to get all the neirest neighbours but I want to ignore them if the field \"product_hashes\" matches a given array.\n\nWhen using the python client everything works as expected. I can filter by using:\nfilter_keys = [5713290336289334020,\n                   4593730084807288382\n                   ]\n    match_except = models.FieldCondition(key=\"product_hashes\", match=models.MatchExcept(**{\"except\": filter_keys}))\n    scroll_filer = models.Filter(\n        must=[match_except],\n    )\n\n    scroll = client.search(\n        collection_name=collection_name,\n        query_vector=some_vector\n        query_filter=scroll_filer,\n        limit=10,\n    )\n\nWhen sending a request vie the Rest-Api the filter simply does not work (see the attached image)\n\nIs this a known Bug? The Specification of the Rest-Api specifically allows int Filter (https://qdrant.tech/documentation/concepts/filtering/#match-except)\n\nThank you in advance"
  },
  {
    "threadId": "1161684365722521680",
    "name": "Display images in dashboard",
    "messages": "I want to use qdrant for image embeddings. I followed the https://qdrant.tech/documentation/tutorials/create-snapshot/ Tutorial. I see that some images get displayed and other are not. I want to display images in my collection but I don't understand when images are displayed and when they aren't. Some invalid urls even get displayed\nI would really appreciate any help"
  },
  {
    "threadId": "1161674386022076536",
    "name": "Scroll functions",
    "messages": "I am using:\n\npoint_contents = qd_client.scroll(\n        collection_name=\"TEST\",\n        limit=2, #qd_client.get_collection(collection_name=\"TEST\").points_count,\n        with_payload=['page_content'],\n        with_vectors=False,\n    )\n\nthis returns a format of:\n\n([Record(id='point 1 id', payload={'page_content': ' text '}, vector=None), Record(id='point 2 id', payload={'page_content': ' text '}, vector=None)], 'point 3 id')\n\nim confused why its returning the id of point 3. did i make an error in my scroll call somewhere? and why is it returning this id as a second element where the point_contents object is now a list, id couple\n\nyes, all 3 ids exist in the collection, the matched id to payload is correct. im just confused why that third id is in the output"
  },
  {
    "threadId": "1161615466821845052",
    "name": "Max payload size for Batch Insertion.",
    "messages": "Hi Team,\nI currently am using a cluster of 9 machines.\n1. Wanted to know what is the maximum supported size (in MiBs/KiBs) for a batch insertion request\n2. Does it depend on the number of nodes present in the cluster?\nAssuming we are running Qdrant on docker and making batch insertion request using the python requests module directly to port 6333. \nCould not find a number in documentations."
  },
  {
    "threadId": "1155840791105785966",
    "name": "[Errno 61] Connection refused",
    "messages": "Hi, Im trying to access to Qdrant cloud cluster in the free versi├│n and when instanciating the Qdrant client it returns me a socket connection error. Other API's an d services are working properly in the same PC, and I dont have any firewalls. Ive deleted the cluster and created again but it keeps appearing:\n\nqdrant_client = QdrantClient(\n    url=\"...\",\n    api_key=\"...\",\n)\n\n___________________\n\nFile /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:836, in create_connection(address, timeout, source_address, all_errors)\n    835     sock.bind(source_address)\n--> 836 sock.connect(sa)\n    837 # Break explicitly a reference cycle\n...\n    102 except Exception as e:\n--> 103     raise ResponseHandlingException(e)\n    104 return response\n\nResponseHandlingException: [Errno 61] Connection refused"
  },
  {
    "threadId": "1154464764618096720",
    "name": "Missing results when searching?",
    "messages": "Hey, I recently transfered our company vector database to GCP from AWS, and after updating our data today, and adding a payload to every vector, some oddness seem to occur. I have several data points which have the same vector values, but when I query for the vector the result is missing.\n\nImage 1: Query where filter includes the two ids who have similar vector\nImage 2: Query without filter\n\nWhen i run it on my personal free AWS database, which was the trial database for this project, I get the expected result\n\nImage 3 (white): AWS fetching correct results\n\nThe only difference between the GCP and AWS, other than vector counts, is that I added payloads to the GCP results today, and `segment_counts` when I run `GET /collections/twindex` is 18 on AWS and 8 on GCP"
  },
  {
    "threadId": "1158342071086764062",
    "name": "Updating payloads",
    "messages": "Hello,\n\nNeed some clarifications on how set_payload works. \n\nFrom the documentation of the python function it's written :\nIf payload value with specified key already exists - it will be overwritten\n\nDoes this means than If one shared key is found the whole payload will be overwritten or does this works for each key/value of the payload ? \n\nIf I have this payload in my collection for a point \n{'a': 1, 'b':2}\n\nand I use the set_payload with \n{'b': 3}\n\nWill I end up with {'a': 1, 'b': 3} or just {'b': 3} ?\n\nThanks !"
  },
  {
    "threadId": "1156521596157173820",
    "name": "Weird behaivour on search",
    "messages": "the We using qdrant/qdrant:v1.2.2 in production, and have collection with this params:\n{\n    \"result\": {\n        \"status\": \"green\",\n        \"optimizer_status\": \"ok\",\n        \"vectors_count\": 178249,\n        \"indexed_vectors_count\": 170238,\n        \"points_count\": 178249,\n        \"segments_count\": 15,\n        \"config\": {\n            \"params\": {\n                \"vectors\": {\n                    \"size\": 512,\n                    \"distance\": \"Cosine\",\n                    \"on_disk\": false\n                },\n                \"shard_number\": 3,\n                \"replication_factor\": 1,\n                \"write_consistency_factor\": 1,\n                \"on_disk_payload\": false\n            },\n            \"hnsw_config\": {\n                \"m\": 64,\n                \"ef_construct\": 512,\n                \"full_scan_threshold\": 10000,\n                \"max_indexing_threads\": 0,\n                \"on_disk\": false\n            },\n            \"optimizer_config\": {\n                \"deleted_threshold\": 0.2,\n                \"vacuum_min_vector_number\": 1000,\n                \"default_segment_number\": 0,\n                \"max_segment_size\": null,\n                \"memmap_threshold\": null,\n                \"indexing_threshold\": 5000,\n                \"flush_interval_sec\": 1,\n                \"max_optimization_threads\": 1\n            },\n            \"wal_config\": {\n                \"wal_capacity_mb\": 32,\n                \"wal_segments_ahead\": 0\n            },\n            \"quantization_config\": null\n        },\n        \"payload_schema\": {}\n    },\n    \"status\": \"ok\",\n    \"time\": 0.309021774\n}\n\n\nThe problem is when we try to search, firs ~30 results have score above 2, and only if i scroll 2-3 pages, then it start to be good (screnshots attached). But first 30-40 vectors has nothing to do with search vector.\nI want to try to reindex it, how can i trigger it on 1.2.2 version?"
  },
  {
    "threadId": "1161246698090279014",
    "name": "Clear the collection",
    "messages": "Is there a way to clear the collection, or in other words, is there a way to empty the collection?"
  },
  {
    "threadId": "1159828670207238255",
    "name": "How to group similar items using qdrant_client API",
    "messages": "We have a bunch of items (say articles) which are being vectorized and stored in qdrant. web-ui initiative groups the items closer if they look similar, is there a way to fetch items which are in a group via the python API as well? We couldn't find anything in the API which would help fetch these groups and items within it"
  },
  {
    "threadId": "1159957642987450388",
    "name": "How to upload embeddings txt file onto Qdrant.",
    "messages": "Hey guys! I am a brand new Qdrant user and I want to be able to upload a txt file full of embeddings . These embeddings contain animal pictures but I donтАЩt know how to upload them to the cloud. IтАЩm not too familiar with Qdrant terminology but I know the basics. Could someone help me out and explain the process? (Python)"
  },
  {
    "threadId": "1159436903842582568",
    "name": "Best way to leverage 2tb of memory for a 8tb dataset?",
    "messages": "I am using qdrant to store the embeddings of  several hundred million dowsnized images as well as a paragraph of text for each which has a TextIndex setup on that field. This is running on a larger server with 2tb of memory and a couple of NVME drives attached.\n\nIn this scenario is it better to setup the paragraph in one collection that is in memory and the images in another collection which is memmapped; using an id to link between records? Or is there a better way to handle data when it goes over the memory available on the machine? Something like a hot/cold cache or..."
  },
  {
    "threadId": "1157362734237753404",
    "name": "Proper Langchain retriever way to use compound Metadata filters in Qdrant?",
    "messages": "We are currently still in proof-of-concept mode on our document chat project, and have had the most success (thus far) using Qdrant as our vector store.  We are currently using a topography of one Qdrant collection per high-level grouping, and then using metadata tags (probably 2 or 3 max) to be able to further filter our searches.  If we had a metadata tag with a key name of \"tag2\", let us say that some items might have that populated with a specific value (\"123\", \"456\", etc.), but others might have a value of \"$$$\" to indicate a global item (which should be searched/included for all \"tag2\" values).  Then we need to also include another metadata tag/filter (i.e. \"tag1\") on top of that.\n\nMy trouble/issue seems to be around how to properly construct my Langchain retriever to do such a search -- basically I want to set my retriever filter to be something like:\n\n`where \"tag1\" = \"{some-value}\" AND where (\"tag2\" = \"{some-tag2-value}\" OR \"tag2\" = \"$$$\")`\n\nI found one example on the internet that showed the following approach, but it definitely is not working as expected:\n\n`retriever.search_kwargs = dict(\n    filter=qdrant_models.Filter(\n        must=[\n            qdrant_models.FieldCondition(\n                key=\"tag1\",\n                match=qdrant_models.MatchValue(value=\"abc\")\n            ),\n            qdrant_models.FieldCondition(\n                key=\"tag2\",\n                match=qdrant_models.MatchAny(any=[\"$$$\", \"123\"])\n            )\n        ]\n    )\n)`\n\nHopefully that makes sense -- I am basically needing compound metadata filtering (using 2 or 3 tags) where one filter might need to have an \"or\" value.\n\nThanks!"
  },
  {
    "threadId": "1157442824250347552",
    "name": "Read only mode?",
    "messages": "I'm interested in using segments as an API in a read only context. Today that's difficult because RocksDB's read only mode is not used, even for immutable segments. Enough lower level APIs are exposed that I can open a Segment's database in read-only mode and construct the Segment myself outside of SegmentBuilder. However the APIs required to do the same for payload indexes are not available. I've forked qdrant and made the smallest change I'd need, which is just adding `StructPayloadIndex::open_read_only`. Is this something you're interested in seeing a PR for? If you'd rather I might be able to spend some time working on adding a higher level read-only open mode to Segment?"
  },
  {
    "threadId": "1168618473073680496",
    "name": "When i doing a search request using python lib. The server timeouts in 60s no matter what",
    "messages": "I changed the defalut timeout to 100s from 5 sec but the server timeouts in 60s. is there something i am missing. Is it a bug or limitation you guys have put up?"
  },
  {
    "threadId": "1168700696774836224",
    "name": "Nearest neighbor search doesn't return the optimal result",
    "messages": "Hi,\nI have initialized a qdrant DB with ~27'000 vectors (w/ payload) and every vector has a dim of 128.\n\nHere's how I create/initialize the DB:\n```py\ndef _recreate_collection(self):\n    self.db_client.recreate_collection(\n        collection_name=FSC_DB_COLLECTION_NAME,\n        vectors_config=VectorParams(size=128, distance=Distance.EUCLID),\n    )\n\ndef _upsert_embeddings(self):\n    points = []\n    for i, ((_, payload), embedding) in enumerate(zip(self.cur_batch, self.embeddings)):\n        points.append(PointStruct(\n            id=random.getrandbits(64),\n            vector=embedding.tolist(),\n            payload=payload,\n        ))\n\n    op_result = self.db_client.upsert(\n        collection_name=FSC_DB_COLLECTION_NAME,\n        points=points,\n        wait=True\n        )\n    assert op_result.status == UpdateStatus.COMPLETED\n```\n\nI'm performing an exact NN search:\n```py\ndef _retrieve_with_embedding(embedding, count: int = 50):\n    query_results = db_client.search(\n        collection_name=FSC_DB_COLLECTION_NAME,\n        query_vector=embedding.tolist(),\n        with_vectors=True,\n        limit=count,\n        search_params=models.SearchParams(exact=True)\n    )\n    return query_results\n```\n\nThe optimal result isn't returned! How do I know it's not the optimal --> I've tried taking all of my embeddings vs the `test_embedding` and print out the distance (using `torch.norm(test_embedding - embedding)`). Can anyone help me dig out why?"
  },
  {
    "threadId": "1168512066332672062",
    "name": "Disable logs",
    "messages": "Hi! How can I disable logs?\n\nI have tried to change the `log_level` param in the `config.yaml` file, but it keeps printing them in the CLI"
  },
  {
    "threadId": "1156621261590499339",
    "name": "Poor ingestion performance",
    "messages": "Hey all, new to Qdrant (and vector dbs in general). I'm test driving the Python client on an ingestion task very similar to the [Fast Embeddings + Simpler API](https://github.com/qdrant/qdrant-client#fast-embeddings--simpler-api) docs using a local disk to persist changes on a EC2 instance on AWS. The performance I'm getting is far below my expectations, about ~5 documents/second according to tqdm:\n```49%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦО                                                            | 4900/10000 [15:28<15:13,  5.58it/s]\n```\nAm I missing something?"
  },
  {
    "threadId": "1167384418831319071",
    "name": "Can u help with logs?",
    "messages": "Hello again. \nUnfortinatly i have to ask you give logs from 26.10 time about 11:55. In this time DB deleted again. Can u show from command DELETE was? And check was command CREATE  next?"
  },
  {
    "threadId": "1167372007948697650",
    "name": "How to bring randomness into vector search?",
    "messages": "Hi guys,\n\nCan you help me? I wanna add some randomness into my search, I would like to add some type of noise into my search. Is there any good way to do this? Vectors are embeddings so I cant add random number from 0-1. thanks!"
  },
  {
    "threadId": "1161957213963833395",
    "name": "Default config values?",
    "messages": "I have a sense from the documentation and the source code that the default config values are these:\n\nhttps://github.com/qdrant/qdrant/blob/master/config/config.yaml\n\nBut I did not see any explicit mention of it. Is it true?"
  },
  {
    "threadId": "1165570996309340211",
    "name": "scroll all indexed keywords",
    "messages": "Is there efficient way to scroll all indexed keywords in payloads?\nUsing points/search/groups with group_size 1 and very large limit can list up unique keywords but I'm wondering if there is better way."
  },
  {
    "threadId": "1166880284931784704",
    "name": "vector-db-benchmark graph code availability",
    "messages": "Hey all, I'm reproducing benchmarks through the qdrant vector-db-benchmark repository and I'm wondering if the code for the interactive graph that's up on the qdrant benchmarking webpage is available/public."
  },
  {
    "threadId": "1166728935447482418",
    "name": "Vectorization of book paragraphs for searching in Qdrant",
    "messages": "I'm doing a small project on parsing and vectorizing book text.\nGoal: Collection, vectorization and indexing of book text for subsequent quick search and analysis.\n\nBut there's something I can't understand.\nI'm going to vectorize the text paragraph by paragraph and send it to Qdrant. Next, based on a specific request, I want to pull out the required paragraph. Did I understand correctly that the paragraph itself needs to be added to the vectorized paragraph data in the payload?\n\nThere are a lot of books, and accordingly there are a lot of paragraphs. Isn't it too resource-intensive for Qdranta to store both the vectorized text and the text itself in the payload?"
  },
  {
    "threadId": "1167028589166874664",
    "name": "scroll all, limited payload",
    "messages": "Is there a way to limit the payload I get when I scroll. \n\nIf I want to recieve all items, but not all the content.\n\nExample: I only want \"metadata.id\" and not all the other info"
  },
  {
    "threadId": "1156199574306820206",
    "name": "How can I benchmark a model using dbpedia-entities-openai-1M dataset ?",
    "messages": "I would like to know how you benchmarked binary quantization with this dataset,  and if it can be reproduced using your benchmark repository."
  },
  {
    "threadId": "1161489379034943488",
    "name": "Recommend takes too long to return the 1st request. But it is fast in the later request",
    "messages": "the hi all, Im having a problem that :   Im doing recommend search. But it takes so long to get the response (up to 20-30s). But if i send the same request for the 2nd time after the first one returned response , it only takes <300ms .\nQdrant 1.6.0\nCollection: 55k points with 100k vector 5k dimensions\nConfig: \n +  Num of segment: 12\n +  Original Vector with memmap, on disk = true. \n +  Quantization: scalar on MEM ( 0.98)\n + HNSW on mem\nMax mem for QDrant instance is 12GB (by mean of docker compose ). Our EC2 server has 30GB Free of RAM\n\nCan anyone help me to understand this ?"
  },
  {
    "threadId": "1161991866368139395",
    "name": "Qdrant client stateful behavoir",
    "messages": "I am using Qdrant and Llama-index for my chat engine, and I have multiple collections inside. I am taking input from user to use different collections. Below is the code I am using, but the error I'm facing is the index does not change when the collection changes even when I initialize the chat again\n```\ndef initializeChat(collection, url):\n    client = qdrant_client.QdrantClient(\n        url=f\"{url}\")\n    vector_store = QdrantVectorStore(client=client, collection_name=f\"{collection}\")\n    index = VectorStoreIndex.from_vector_store(\n        vector_store=vector_store, service_context=service_context)\n    print(collection)\nreturn index\n```\nIt initialized for collection1,  but when user passes collection2 , I run the above lines of code again,(print statement prints as collection2), the index does not change and is still using previous collection. How to solve for this?\nThanks!!\nFYI more details of my code: Chat is a websocket,when user selects another collection, the websocket terminates and starts again, it triggers above initializeChat fn"
  },
  {
    "threadId": "1166012965716246628",
    "name": "Filtering points by metadata",
    "messages": "The code snippets below return `([], None)` for the 'page' and 'source'  values.\n\nExample #1 ```\nimport qdrant_client.http.models as models\n\nclient.scroll(\n    collection_name=qdrant_collection_name, \n    scroll_filter=models.Filter(\n        must=[\n            models.FieldCondition(\n                key=\"page\",\n                match=models.MatchValue(value=1),\n            ),\n        ]\n    ),\n)\n```\n\nExample 2```\nimport qdrant_client.http.models as models\n\nclient.scroll(\n    collection_name=qdrant_collection_name, \n    scroll_filter=models.Filter(\n        must=[\n            models.FieldCondition(\n                key=\"source\", \n                match=models.MatchValue(value=\"References/ALAUXs/2022/038_22_AUXSCOUT_PROGRAM_SOP_REVISION.pdf\")\n            ),\n        ]\n    ),\n    limit=1,\n    with_payload=True,\n    with_vectors=False,\n)\n```\n\n\nExample 3```\nimport qdrant_client.http.models as models\n\nclient.scroll(\n    collection_name=qdrant_collection_name,\n    scroll_filter=models.Filter(\n        must=[\n            models.FieldCondition(\n                key=\"source\",\n                match=models.MatchText(text=\"AUXSCOUT\"),\n            )\n        ]\n    )\n)\n```\n\n**Toubleshooting Steps **\n\n1. Qdrant Cloud web interface show the following metadata exists for a payload says:*\n\n<img width=\"883\" alt=\"image\" src=\"https://github.com/qdrant/qdrant/assets/20048187/77392435-29d1-4a89-97f4-76c951450c7c\">\n```\n`{\n  \"page\": 1,\n  \"source\": \"References/ALAUXs/2022/038_22_AUXSCOUT_PROGRAM_SOP_REVISION.pdf\"\n}`\n```\n2. python client.scroll method works fine and returns the same:\n\n```\nimport qdrant_client.http.models as models\n\nclient.scroll(\n    collection_name=qdrant_collection_name, \n    scroll_filter=models.Filter(\n        must=[\n            models.FieldCondition(\n                key=\"page\",\n                match=models.MatchValue(value=1),\n            ),\n        ]\n    ),\n)\n```\n\n3. I tried treating 1 as a string  and about everything else I could think of.**"
  },
  {
    "threadId": "1161742559731798076",
    "name": "Is it possible to delete (or update) points based specifically on Metadata tags?",
    "messages": "We are storing thousands of business-related documents (TXT, PDF, DOC, DOCX) in multiple collections (segregated by high-level business unit).  The text embeddings of each document (during ingestion) is pushed to Qdrant with 4 consistent metadata tags -- one of those tags is the origin file name (with full path).   THAT tag, along with one other \"grouping\" tag value, makes a document \"key\" which should always be unique (per collection).  We are running a nightly \"crawler\" process that is checking an AWS folder for new documents as well as updated documents.  For the UPDATED documents, what is the best way to manage this in Qdrant?  My \"basic\" thought is that we could simply delete any points with those 2 specific tag values and then re-push the updated document embeddings to Qdrant (with the same exact set of metadata tags again).  But I am finding different answers on the internet as to whether this is possible, or if it IS possible, whether that is the correct way to handle this.\n\nSuggestions?  (Thanks!)"
  },
  {
    "threadId": "1163806404960854026",
    "name": "Is there a qdrant GraphQL API or a Hasura integration?",
    "messages": "If not, how do you guys think is the best way to structure a query api on qdrant? Just REST endpoints?"
  },
  {
    "threadId": "1163700656448745512",
    "name": "How to efficiently create collection and update 1M records in Qdrant DB",
    "messages": "We have a database (MongoDB) collection containing around a million records. we want to implement vector search on few fields in the collection. Any suggestions on creating collection and uploading the vectors efficiently?\nThe model encoding with \"all-MiniLM-L6-v2\" itself taking 7-8 hours in CPU."
  },
  {
    "threadId": "1164151670125625354",
    "name": "Is it possible to add image file to Qdrant record metadata?",
    "messages": "Hello!\nI am using Qdrant as vector store (Docker mode) in pair with LangChain and it is really pleasant experience. But now I need to go further and provide a data collection containing texts that will be used by LLM and pictures attached them. Is it somehow possible to make a straight solution and put each picture to each record metadata field?\nI've been researching this question for a while but found no answer. Can you give me a hand with it?"
  },
  {
    "threadId": "1164208141811077230",
    "name": "Hi all - is there any way to set the vector dimensions on the qdrant cloud free version?",
    "messages": "I cant find a way to set the vector dimensions when you create a cluster. I want to use cohere embeddings at 768 thanks"
  },
  {
    "threadId": "1164394338625798185",
    "name": "qdrant-client API Documentation",
    "messages": "Hi everyone, I am quite new to qdrant and I am now trying to use Qdrant-client python package.\n\nOne problem that I am encountered with now is that where can I find the documentation for all the method and function for the qdrant-client package? I can see that there is an endpoint API documentation, but that is all I can gather. Can anyone help me point to the document regarding the qdrant-client API instead? \n\nThank you a lot in advance!"
  },
  {
    "threadId": "1164570076389851186",
    "name": "Inconsistent query result",
    "messages": "I have a collection using COSINE distance measure plus a payload of text and document_id.  \n\nI run a \"real\" query, qdrant returns only 1 result (limit=100).   If I search using a random vector, it returns 100 matches as expected.\n\nThe query is below. When I run it with a vector that is a \"real\" embedding, I get 1 match.  If i pass a random vector, it returns 100 results.  I don't set a score limit.\n\n```\nPOST collections/doc-paragraphs-openai/points/search\n{\n  \"limit\": 100,\n  \"with_payload\": true,\n  \"filter\": {\n    \"must\": [\n      {\n        \"key\": \"doc_guid\",\n        \"match\": {\n          \"any\": [\n            \"<document guid>\"\n          ]\n        }\n      }\n    ]\n  },\n  \"vector\": [...]\n}\n```\n\nCollection info:\n\n```\n{\n  \"result\": {\n    \"status\": \"green\",\n    \"optimizer_status\": \"ok\",\n    \"vectors_count\": 1054522,\n    \"indexed_vectors_count\": 1053215,\n    \"points_count\": 1054522,\n    \"segments_count\": 18,\n    \"config\": {\n      \"params\": {\n        \"vectors\": {\n          \"size\": 1536,\n          \"distance\": \"Cosine\"\n        },\n        \"shard_number\": 1,\n        \"replication_factor\": 1,\n        \"write_consistency_factor\": 1,\n        \"on_disk_payload\": true\n      },\n      \"hnsw_config\": {\n        \"m\": 16,\n        \"ef_construct\": 100,\n        \"full_scan_threshold\": 10000,\n        \"max_indexing_threads\": 0,\n        \"on_disk\": false\n      },\n      \"optimizer_config\": {\n        \"deleted_threshold\": 0.2,\n        \"vacuum_min_vector_number\": 1000,\n        \"default_segment_number\": 0,\n        \"max_segment_size\": null,\n        \"memmap_threshold\": null,\n        \"indexing_threshold\": 20000,\n        \"flush_interval_sec\": 5,\n        \"max_optimization_threads\": 1\n      },\n      \"wal_config\": {\n        \"wal_capacity_mb\": 32,\n        \"wal_segments_ahead\": 0\n      },\n      \"quantization_config\": null\n    },\n    \"payload_schema\": {\n      \"doc_guid\": {\n        \"data_type\": \"keyword\",\n        \"points\": 1054522\n      }\n    }\n  },\n  \"status\": \"ok\",\n  \"time\": 0.000054051\n}\n```"
  },
  {
    "threadId": "1164087612538105856",
    "name": "Re-index vectors in collection",
    "messages": "Is it possible to create (or re-create) a vector index after all the collection is already loaded into the database or does it have to be during upload of points?"
  },
  {
    "threadId": "1163512678300536833",
    "name": "qdrant cloud  in aws lambda fct",
    "messages": "I'm trying to run  qdrant cloud python endpoint  in aws lambda fct , i'm getting timeout in all the fct client.search or client.scroll it seem like i can't reach the endpoint, it work in local , any help please ?"
  },
  {
    "threadId": "1161057995967434853",
    "name": "Visualising the HNSW graph",
    "messages": "Hi! As the title says - is there any way to visualize the HNSW graph? Ideally all of it but even some levels of it? Is there a way to hijack the rust source code to enable this?"
  },
  {
    "threadId": "1163892856642080893",
    "name": "Unable to connect to Qdrant container defined in `docker-compose.yml` file",
    "messages": "It feels like I'm missing something when trying to define a Qdrant service within my `docker-compose.yml` file. I've had luck with doing something similar in Python, but I think something is going awry when trying to connect to port `6334`. \n\nSnippet from my `docker-compose.yml` file:\n```yaml\nversion: \"3.8\"\nservices:\n  db:\n    container_name: qdrant-db\n    image: qdrant/qdrant\n    ports:\n      - 6333:6333\n      - 6334:6334\n    volumes:\n      - ./qdrant:/qdrant/storage\n    env_file:\n      - ./.env\n    environment:\n      - QDRANT__SERVICE__GRPC_PORT=6334\n    networks:\n      - my-network\n```\nSnippet from `main.rs`:\n```rust\n#[tokio::main]\nasync fn main() -> Result<()> {\n  let qdrant_client = QdrantClient::from_url(\"db:6334\")\n        .build()?;\n  let reqwest_client = reqwest::Client::new();\n\n  util::seed_books(&qdrant_client, &reqwest_client)\n        .await?;\n  ....\n}\n```\nSnippet from `util.rs`:\n```rust\npub async fn seed_books(client: &QdrantClient, reqwest_client: &reqwest::Client) -> Result<()> {\n  let collection_name = env::var(\"COLLECTION_NAME\").expect(\"COLLECTION_NAME must be set\");\n  \n  println!(\"Creating collection: {}...\", collection_name);\n\n  client\n      .create_collection(&CreateCollection {\n          collection_name: collection_name.clone(),\n          vectors_config: Some(VectorsConfig {\n              config: Some(Config::Params(VectorParams {\n                  size: 384, // 384 is specific to the mighty model\n                  distance: Distance::Cosine.into(),\n                  ..Default::default()\n              })),\n          }),\n          ..Default::default()\n      })\n      .await\n      .unwrap();\n```\nWhenever it panics, I'm getting the error: `\"Failed to connect to db:6334/: transport error\"`. It's all on the same network, and using `db:6333` works in my Python example, so I'm not exactly sure where/why it's breaking."
  },
  {
    "threadId": "1163904680838053898",
    "name": "How can I delete specific data from Qdrant via API?",
    "messages": "Hello all,\n'I'm very new to this channel. I am upserting data to my self hosted qdrant instance, but how can I delete specific data like example.pdf from the collection via API?\nI read the docs but still not able to properly execute the delete api to delete a particular document from the collection.\n\nAny help is really appreciated ЁЯЩП"
  },
  {
    "threadId": "1156605092850585692",
    "name": "How to use Full-text search я╝Я",
    "messages": "I have add the index as below picture, but when i am searching the collection ,there is no data return. \nI think there must be something wrong with my parameters, but I can't find any help about it in the document.\nCould you provide more using doc about full-text searchя╝Я Thanks in advance~"
  },
  {
    "threadId": "1162273215167602778",
    "name": "Unable to retrieve specific keys from payload when using scroll API in v1.6.0",
    "messages": "Hi, I am trying to use the scroll API to fetch some specific data like this:\n```scroll(\n        collection_name=\"some_collection\",\n        with_payload=[\"metadata.city\"]\n    )\n```\nbut I get back empty payload in response. \n\n```([Record(id='25f7ce32-71f4-437d-81d4-1f38d5ce2d64', payload={}, vector=None),\n  Record(id='2a449f03-c10d-423f-8f37-8ecb00304d8b', payload={}, vector=None),\n  Record(id='7c9489af-1a60-4a83-8e07-c646e2c169bc', payload={}, vector=None),\n  Record(id='a26d1c9f-c33b-48f9-a40a-d75383a5733f', payload={}, vector=None)],\n None)\n```\n\nI tried using \"metadata\" only instead, and I get back results:\n```\n([Record(id='25f7ce32-71f4-437d-81d4-1f38d5ce2d64', payload={'metadata': {'city': 'New York', 'entity_type': 'skills', 'latitude': 40.7508, 'longitude': -73.9961, 'position_level_id': 6, 'resume_update_timestamp': 1688987392, 'state': 'NY', 'target_compensation': 150000, 'zip_id': 46033}}, vector=None),\n  Record(id='2a449f03-c10d-423f-8f37-8ecb00304d8b', payload={'metadata': {'city': 'Jersey City', 'desired_location_radius': 25, 'entity_type': 'skills', 'latitude': 40.7291, 'longitude': -74.0362, 'position_level_id': 6, 'resume_update_timestamp': 1691666204, 'state': 'NJ', 'target_compensation': 190000, 'zip_id': 41314}}, vector=None),\n  Record(id='7c9489af-1a60-4a83-8e07-c646e2c169bc', payload={'metadata': {'city': 'New York', 'entity_type': 'experience', 'latitude': 40.7508, 'longitude': -73.9961, 'position_level_id': 6, 'resume_update_timestamp': 1688987392, 'state': 'NY', 'target_compensation': 150000, 'zip_id': 46033}}, vector=None),\n  Record(id='a26d1c9f-c33b-48f9-a40a-d75383a5733f', payload={'metadata': {'city': 'Jersey City', 'desired_location_radius': 25, 'entity_type': 'experience', 'latitude': 40.7291, 'longitude': -74.0362, 'position_level_id': 6, 'resume_update_timestamp': 1691666204, 'state': 'NJ', 'target_compensation': 190000, 'zip_id': 41314}}, vector=None)],\n None)\n```\nAm I missing something?"
  },
  {
    "threadId": "1162271121786277888",
    "name": "Is there an API that user can unload the collections?",
    "messages": "As title mentioned."
  },
  {
    "threadId": "1163029843500355695",
    "name": "Update Quantization Configuration: Get Error of Field required (type=value_error.missing)",
    "messages": "I started with a collection with a quantization configuration.\nI use the update_collection to update the quantization configuration\n``` bash\nclient.update_collection(\n    collection_name=<collection_name>,\n    quantization_config=models.BinaryQuantization(\n        binary=models.BinaryQuantizationConfig(\n            always_ram=True,\n        ),\n    )\n}\n```\n\nAfter updating the config, I start to upsert some vectors and got the error:\n```bash\nValidationError: 4 validation errors for ParsingModel[InlineResponse2005] (for parse_as_type)\nobj -> result -> config -> quantization_config -> scalar\n  field required (type=value_error.missing)\nobj -> result -> config -> quantization_config -> binary\n  extra fields not permitted (type=value_error.extra)\nobj -> result -> config -> quantization_config -> product\n  field required (type=value_error.missing)\nobj -> result -> config -> quantization_config -> binary\n  extra fields not permitted (type=value_error.extra)\n```\nIt works fine if I use scalar quantization.\nDoes anyone know the reason?"
  },
  {
    "threadId": "1163735013729243146",
    "name": "Getting error on create collection with Qdrant version v1.6.1",
    "messages": "Hi, I'm getting the following the error when trying to create a collection on Qdrant v1.6.1. I am using the python qdrant client 1.6.2."
  },
  {
    "threadId": "1162247502452494418",
    "name": "How to do batch search with quantization",
    "messages": "For single search I am doing it like ``` search_result = self.qdrant_client.search(\n                collection_name=self.collection_name,\n                query_vector=vector,\n                query_filter=None,  \n                limit=limit,\n                search_params=models.SearchParams(\n                    quantization=models.QuantizationSearchParams(\n                        ignore=False,\n                        rescore=rescore,\n                        oversampling=oversampling,\n                    )\n                ),\n            )```\nhow to pass the SearchParams during the batch search I tried adding it in the SearchRequest but it's not working"
  },
  {
    "threadId": "1161936001313296424",
    "name": "Searched results sorted by datetime in the payload",
    "messages": "Can the results be sorted by datetime value in the payload?"
  },
  {
    "threadId": "1162043287444664382",
    "name": "Inconsistant vector count across Qdrant cluster",
    "messages": "Hello,\n\nI have a Qdrant cluster deployed:\n- 3 nodes\n- 3 shards per collection\n- 1 replica per shard\n\nWhen looking at the `/metrics endpoint`, I notice the `collections_vector_total`  value isn't consistant regarding the node that respond. I can also see this behavior when requesting `GET collections/my_collection`. \n\nNote that I'm not inserting or updating the collection at the same time. It was created this morning and not changed since this time.\n\nI would like to know if it's a normal behavior (and why) or a bug?\n\nPlease find enclosed, two different responses, from two differents nodes\n\nThanks,"
  },
  {
    "threadId": "1158701775365353472",
    "name": "Storing query embeddings using FastEmbed",
    "messages": "When using `add` method in `qdrant_client`, the `passage: ` is prefixed automatically. If I want to store `query: ` embeddings to avoid recomputing it every time, how do I go about that? Also, is there any way to use embeddings as both query as well as passage? There is a way to embed without query or passage prefix, although not recommended, how worse can the results get?"
  },
  {
    "threadId": "1158301388565139479",
    "name": "Questions regarding group search",
    "messages": "1. How do we add filter to the with_lookup part of group search?\n2. How are points retrieved in group search? We want the data to be returned based on some weighted average of top n records of each group. Is that possible? Is it on the roadmap?"
  },
  {
    "threadId": "1155837722766888991",
    "name": "Rescore + Oversampling via batch request in Python",
    "messages": "I found that the documentation contains a sample of how to specify rescoring and oversampling for single requests, But I'm not sure how to do this via batch request.\n\n\nThis is from the docs\n\nclient.search(\n    collection_name=\"{collection_name}\",\n    query_vector=[0.2, 0.1, 0.9, 0.7],\n    search_params=models.SearchParams(\n        quantization=models.QuantizationSearchParams(\n            ignore=False,\n            rescore=True,\n            oversampling=2.0,\n        )\n    )\n)"
  },
  {
    "threadId": "1154780723954319409",
    "name": "Maximum supported vector dimensionality for HNSW Index?",
    "messages": "pg_vector supports HNSW Indexing for vectors with up to 2000 dimensions. Are there any such limits (or concerns for higher dimensions) with Qdrant? (Asking for a friend who intends to use Qdrant with 5120-dimensional text embeddings for semantic search.)"
  },
  {
    "threadId": "1155804620044636213",
    "name": "Is it safe to clean storage/snapshots_temp?",
    "messages": "Basically `rm -rf storage/snapshots_temp/*` ?"
  },
  {
    "threadId": "1156179028714205184",
    "name": "What is the behavior when hybrid search is enabled?",
    "messages": "I saw the following article and recognized that qdrant uses both BM25 and vector search and re-ranks each result with an encoder.\nhttps://qdrant.tech/articles/hybrid-search/\n\nIf I want to perform a hybrid of BM25 and vector search as shown in this diagram in qdrant client in python, is MatchText equivalent to BM25 as shown in the following code?\nhttps://qdrant.tech/articles_data/hybrid-search/experiments-design.png\n\n\n```\nmodels.FieldCondition(\n    key=\"description\",\n    match=models.MatchText(text=\"good cheap\"),\n)\n```\n\n(I recognize the following as a vector search part)\n\n```\nclient.search(\n    collection_name=\"{collection_name}\",\n    query_filter=models.Filter(\n        must=[\n            models.FieldCondition(\n                key=\"city\",\n                match=models.MatchValue(\n                    value=\"London\",\n                ),\n            )\n        ]\n    ),\n    search_params=models.SearchParams(\n        hnsw_ef=128,\n        exact=False\n    ),\n    query_vector=[0.2, 0.1, 0.9, 0.7],\n    limit=3,\n)\n```\n\nAlso, can anyone tell me if reranking after extracting from both is something that needs to be written as code... (Is this handled automatically?)"
  },
  {
    "threadId": "1156514125355163689",
    "name": "Sending a text to Qdrant",
    "messages": "Good afternoon This is my first time using Qdrant. I have a question, please tell me, is it possible to send not vectorized text, but a regular dialogue to Qdrant?\nTo be more precise, I have a dialogue between two people. I vectorize this dialog, for example, using Fasttext and send it to Qdrant. But I need the vectorized dialog to be in the database along with the regular dialog text.\n\nSo is it possible to send a regular dialogue to Qdrant in addition to a vector dialogue?\n\nThanks)"
  },
  {
    "threadId": "1156580147571466360",
    "name": "Accuracy issue",
    "messages": "I have almost 200K dense vectors stored in a db. First I inserted 20k vectors and ran a test to see how many returned the \"correct\" result in the first (highest scored) position. Accuracy was over 99%. Then I inserted 180K random vectors and checked accuracy again. My \"accuracy\" measure fell to under 70% which is unusable for my use case.  I haven't used any special indexing strategy. I only carried out the benchmark after indexing was completed. \n\n{\n\"params\":{\n\"vectors\":{}\n\"shard_number\":1\n\"replication_factor\":1\n\"write_consistency_factor\":1\n\"on_disk_payload\":true\n}\n\"hnsw_config\":{\n\"m\":16\n\"ef_construct\":100\n\"full_scan_threshold\":10000\n\"max_indexing_threads\":0\n\"on_disk\":false\n}\n\"optimizer_config\":{\n\"deleted_threshold\":0.2\n\"vacuum_min_vector_number\":1000\n\"default_segment_number\":0\n\"max_segment_size\":\nNULL\n\"memmap_threshold\":\nNULL\n\"indexing_threshold\":20000\n\"flush_interval_sec\":5\n\"max_optimization_threads\":1\n}\n\"wal_config\":{\n\"wal_capacity_mb\":32\n\"wal_segments_ahead\":0\n}\n\"quantization_config\":\nNULL\n}"
  },
  {
    "threadId": "1157563603088576512",
    "name": "message\": \"cURL error 7: Failed connect to",
    "messages": "This is log {\n    \"message\": \"cURL error 7: Failed connect to d7cdce78-d9f6-4084-807b-be4328bd5e65.us-east4-0.gcp.cloud.qdrant.io:6333; Connection refused (see https:\\/\\/curl.haxx.se\\/libcurl\\/c\\/libcurl-errors.html) for https:\\/\\/d7cdce78-d9f6-4084-807b-be4328bd5e65.us-east4-0.gcp.cloud.qdrant.io:6333\\/collections\",\n    \"date\": \"2023-09-30 06:09:40\"\n}"
  },
  {
    "threadId": "1156691644519817307",
    "name": "Build from source, unable to start Qdrant",
    "messages": "Hi Folks,\nI am trying to build qdrant from sources and unable to start qdrant.\n\nEnvironment:\nUbuntu 22.04 LTS on AMD x86_64\nInstalled all libraries manually as mentioned Docker file.\nCloned qdrant master branch.\nBuild successful and I can see the binary at ./target/release/qdrant\n\nIssue:\non attempting to start the generated binary, I get \"too many open files\" error.\n\n2023-09-27T20:28:21.381383Z ERROR qdrant::startup: Panic occurred in file /home/amd/.cargo/registry/src/index.crates.io-6f17d22bba15001f/actix-server-2.1.1/src/worker.rs at line 425: called `Result::unwrap()` on an `Err` value: Os { code: 24, kind: Uncategorized, message: \"Too many open files\" }\nthread panicked while processing panic. aborting.\nAborted\n\n\nTroubleshooting:\nI have already increased the ulimit for current user in session and via the /etc/security/limits.conf file to 500000 but it did not help.\n\nI also resolved the Config WARN messages in the stack trace by copying the config directory to release directory. \n\n\nStack trace: Attached image"
  },
  {
    "threadId": "1157257110233563176",
    "name": "Vector DB Migrations",
    "messages": "I'm going to use a qdrant collection to store vectors.\nAt collection initialization (`recreate_collection()`) one can specify multiple vectors per Point, which is a feature we will need in our `stage2`.\nHowever, for the `stage1`, I currently only need to store one vector per Point.\n\nMy question is: how should migrations be handled in qdrant collections? If I initialize the collection with a single vector per point now and start populating the collection, what's the recommended flow to add a second vector per point later on? How should I back-populate the old entries?\n\nI see two obvious workarounds:\n1. initialize the collection with a single vector: `single-vector-collection`. In `stage2` to add a second vector per point each point in the `single-vector-collection` can be copied over to a new `two-vector-collection` , and the new vector populated for each Point.\n2. initialize the collection with two vectors directly and set empty entries (is this even possible?) for the second for `stage1`. Moving to `stage2`, back-populate the second vector in the same collection and add every new Point with both vectors."
  },
  {
    "threadId": "1157945500729876530",
    "name": "Incorrect RESULTS for META DATA Search",
    "messages": "I have inserted vectors with metadata such as YEAR, AUTHORS\n\n1) I have put a filter like this\nif year == '':\n        query_filter = None\n        print(\"No year\")\n    else:\n        query_filter=qmodels.Filter(\n            must= [\n                FieldCondition(\n                    key=\"year\",\n                    match=models.MatchValue(value=year),\n                )\n            ],\n        )\n        print(f\"Year : {year}\")\n\n**When I put the Year as 2023 , I see results with 2021 , 2022 and 2023 .**\n\nAm I missing something ?"
  },
  {
    "threadId": "1157039661630361710",
    "name": "Help Me With Getting Embeddings Onto Qdrant",
    "messages": "Hey Guys! I am a brand new Qdrant user and am getting used to how it works however, I need to get a txt file that is full of image embeddings into Qdrant. How would I do that in Python? Thanks"
  },
  {
    "threadId": "1158344839188979712",
    "name": "Recover collection from snapshot via API throws error",
    "messages": "Hi, I'm currently playing around with snapshots to find out how to backup and recover collections.\nFor this I want to create a snapshot on one instance, download it and upload/recreate the collection on another instance.\n\nI am using these commands:\n```bash\n# Create snapshot\ncurl -X POST ${QDRANT_URL}:6333/collections/${COLLECTION_NAME}/snapshots\n\n# Download snapshot\ncurl -X GET ${QDRANT_URL}:6333/collections/${COLLECTION_NAME}/snapshots/${SNAPSHOT_NAME}\n\n# Recover collection from snapshot by uploading\ncurl -X POST ${QDRANT_URL}:6333/collections/${COLLECTION_NAME}/snapshots/upload \\\n-H 'Content-Type:multipart/form-data' \\\n-F 'snapshot=@/path/to/snapshot_file.snapshot'\n```\n\n**This worked**: \nCreate snapshot, download it, delete snapshot and collection from the Qdrant instance, upload it again **to the same instance** from the snapshot. This worked as expected so all good there.\n\n**This did not work**:\nNow, I want to upload it to a **different instance**, where the collection never existed before, but I get this error returned:\n`{\"status\":{\"error\":\"Service internal error: Persist error: failed to persist temporary file: No such file or directory (os error 2)\"},\"time\":2.849682476}`"
  },
  {
    "threadId": "1155781256450154548",
    "name": "Can't create snapshots",
    "messages": "I tried to create snapshots and it always return Dismiss in browser and did not create any snapshots. I tried do it using python code, but it is did not work also. How can i make a snapshot?"
  },
  {
    "threadId": "1158410242342801469",
    "name": "Group search",
    "messages": "Possible to have the same collection referencing itself in with_lookup??"
  },
  {
    "threadId": "1156861310110289951",
    "name": "Does Qdrant have some caching mechanism for queries?",
    "messages": "Hello, \n\nI'm currently trying to evaluate the performance of a solution we built with Qdrant. For that purpose, I would like to measure the execution time for x requests to our service in different situations (change the pool size returned, some parameters etc...). But I'm afraid of potential caches in Qdrant.\n\nCan you tell me if such caches exist (that would make the first request run slower than the subsequents) and if yes, can (and how) I disable them for testing?"
  },
  {
    "threadId": "1158409684865257562",
    "name": "Filter based on ids",
    "messages": "How to filter based on a set of ids which should NOT match?"
  },
  {
    "threadId": "1158423609174413393",
    "name": "RAM usage for 10 million embeddings",
    "messages": "For one of my use case, search speed is not important. I want to store embedding and its metadata on disk as I have huge amount of documents. My assumption is that if I store everything on disk, it should take very less RAM. In my experiment, I have found that it is taking around ~4 GB RAM for 10 Million embeddings for this configuration which is quite large. \nThis is my configuration:\n\n```\n    client.recreate_collection(\n        collection_name=collection_name,\n        vectors_config={\n            IMAGE_VECTOR_NAME: VectorParams(size=IMAGE_EMBEDDING_SIZE, distance=Distance.COSINE, on_disk=True),\n            TEXT_VECTOR_NAME: VectorParams(size=TEXT_EMBEDDING_SIZE, distance=Distance.COSINE, on_disk=True),\n            LLM_VECTOR_NAME: VectorParams(size=LLM_EMBEDDING_SIZE, distance=Distance.COSINE, on_disk=True),\n        },\n        optimizers_config=OptimizersConfigDiff(\n                        flush_interval_sec=10\n            ),\n        hnsw_config=models.HnswConfigDiff(on_disk=True),\n        on_disk_payload=True,\n    )\n```\nAm I missing something? What is the configuration for the minimum usage of RAM considering the search speed is not important."
  },
  {
    "threadId": "1158155847692713994",
    "name": "Hello!",
    "messages": "Please help me with this problem:\nAfter updating the python client I have the following error when trying to install the project at AWS EB (Linux):\n\n```\nERROR: Could not find a version that satisfies the requirement pywin32==306 \n```\nThe installation failed."
  },
  {
    "threadId": "1153624037415714846",
    "name": "load saved vectors on disk",
    "messages": "IтАЩm using qdrant deplyed through docker deployment on azure app services. IтАЩve saved the vectors using the тАЬon_diskтАЭ = True. How to load these vectors in new session? Where are these vectors saved? Can we specify location"
  },
  {
    "threadId": "1156253958470385704",
    "name": "Unable to install FastEmbed on intel mac mini(2018)",
    "messages": "I am trying to use FastEmbed to potentially replace OpenAi's ada-002. I am currently unable to install it on my 2018 mac mini with `poetry add fastembed`\nI see the following error:\n```Unable to find installation candidates for onnxruntime-silicon (1.15.0)```\nI tried updating poetry, and qdrant version to the latest, and retried this, but to no avail.\n\nI am using python version `3.11.2`, poetry version `1.6.1`, qdrant_client version `1.5.4`\n\nI tried installing fastembed on a clean directory with a virtual env activated using `pip install fastembed` and that failed as well:\n```\nERROR: Cannot install fastembed==0.0.1, fastembed==0.0.2, fastembed==0.0.3 and fastembed==0.0.4 because these package versions have conflicting dependencies.\n\nThe conflict is caused by:\n    fastembed 0.0.4 depends on onnxruntime-silicon<2.0.0 and >=1.15.0; sys_platform == \"darwin\"\n    fastembed 0.0.3 depends on onnxruntime-silicon<2.0.0 and >=1.15.0; sys_platform == \"darwin\"\n    fastembed 0.0.2 depends on onnxruntime-silicon<2.0.0 and >=1.15.0; sys_platform == \"darwin\"\n    fastembed 0.0.1 depends on onnxruntime-silicon<2.0.0 and >=1.15.0; sys_platform == \"darwin\"\n\nTo fix this you could try to:\n1. loosen the range of package versions you've specified\n2. remove package versions to allow pip attempt to solve the dependency conflict\n\nERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\n```\n\nI looked into the fastembed's `pyproject.toml` and i see:\n\n```[tool.poetry.dependencies.onnxruntime-silicon]\nversion = \"^1.15.0\"\nmarkers = \"sys_platform == 'darwin'\"  # This makes it macOS specific\n```\nIs this supposed to be installed on `darwin` systems that are silicon based only, and not intel based?"
  },
  {
    "threadId": "1158777570125422602",
    "name": "Advice on getting better matches with FastEmbed on long sequences",
    "messages": "I am using fastembed right now to generate embeddings for sanitized job descriptions (passage) and resume (query) to create different collections in Qdrant vector database.  Since the model(BAAI/bge-small-en) I am using has a max_length = 512 limit, I am forced to split job descriptions and resume content into multiple chunks of 512 characters using RecursiveTextSplitter.\nQdrant has a `query_batch` method that takes multiple vectors as query embeddings and finds matches for each of them. The job matches I got back were poor, and I think that is because of loss of context because of 512 character limit of the model.\n\nI was previously using OpenAI embeddings which has a limit of 8k, and I do get decent job matches with that.\n\nWhat can I do to get better matches using fastembed, given it has a 512 char limit ?\n\nDisclaimer: I am not an ML engineer, and none of my peers at my company are, so what I am preferably looking for is something that works out of the box. If not, something that i can learn and build in 2 weeks. I'd appreciate any help I can get in this regard ЁЯЩП"
  },
  {
    "threadId": "1159240493973110814",
    "name": "Update collection with additional vector and retrieve multiple vectors?",
    "messages": "Hi there,\n\nIs it possible to update a collection with a new vector? My current collection already has a default vector (unnamed) and I want to add a new vector to a point (with a name).\n\n```PUT /collections/{collection_name}\n\n{\n    \"vectors\": {\n        \"image\": {\n            \"size\": 4,\n            \"distance\": \"Dot\"\n        },\n        \"text\": {\n            \"size\": 8,\n            \"distance\": \"Cosine\"\n        }\n    }\n}```\n\nWould this overwrite all the vectors I have, given that the current one doesn't have a name?\n\nMy second question is is it possible to retrieve multiple vectors with an API call? Would:\n\n```client.retrieve(\n    collection_name=\"{collection_name}\",\n    ids=[0, 3, 10],\n    with_vectors=True\n)```\n\nreturn all vectors or just the default vectors? Do I need to specify the vector name?\n\nThank you!"
  },
  {
    "threadId": "1159112900913410048",
    "name": "Latency spike",
    "messages": "Hello there, \n\nWe've succesfully launch our app built on qdrant but I noticed something weird.\nSometimes, I can see huge latency spikes on the log dashboard (and also feel them in the app) but I can't explain why.\n\nAny idea?"
  },
  {
    "threadId": "1159395074073952338",
    "name": "Apache Beam pipeline for cleaning , transforming uploading vectors to qdrant.",
    "messages": "Hi guys,\nHas anyone setup such pipeline for automating uploading new vectors in qdrant. I am currently facing issue uploading part , how it would fit into pipeline. Need some guidance , new to Apache Beam."
  },
  {
    "threadId": "1151909233793704068",
    "name": "Change storage type for vectors",
    "messages": "Hello!\nI have a collection with over 80 million vectors (almost 15GB) and I need to change on_disk mode for vectors from false to true to free up RAM. It seems like I cannot do it with update collection API. There is very poor description for update collection vector params on the API reference page. Python qdrant-client also fails to update on_disk param with error \"vectors -> on_disk value is not a valid dict (type=type_error.dict)\" Is there a way to migrate collection from non-on_disk storage type to on_disk type?\nI use qdrant with version 1.5.1, cluster mode, 5 virtual machines."
  },
  {
    "threadId": "1158960425455599677",
    "name": "Qdrant slowing down",
    "messages": "This is my log when qdrant is started..collection has about 450 million entries of a vector of length 1024\n2023-10-04T02:44:52.123154Z  INFO actix_web::middleware::logger: 172.17.0.1 \"PUT /collections/us_fto_collection_v2/points?wait=false HTTP/1.1\" 200 99 \"-\" \"python-httpx/0.24.1\" 0.002257    \n2023-10-04T02:44:52.123157Z  INFO actix_web::middleware::logger: 172.17.0.1 \"PUT /collections/us_fto_collection_v2/points?wait=false HTTP/1.1\" 200 100 \"-\" \"python-httpx/0.24.1\" 0.002263    \nThis is after a few 1000 inserts\n2023-10-04T02:53:27.499729Z  INFO actix_web::middleware::logger: 172.17.0.1 \"PUT /collections/us_fto_collection_v2/points?wait=false HTTP/1.1\" 200 100 \"-\" \"python-httpx/0.24.1\" 1.251968    \n2023-10-04T02:53:27.554544Z  INFO actix_web::middleware::logger: 172.17.0.1 \"PUT /collections/us_fto_collection_v2/points?wait=false HTTP/1.1\" 200 100 \"-\" \"python-httpx/0.24.1\" 1.048006    \n\nWhy is there a slowdown?"
  },
  {
    "threadId": "1159786856557318174",
    "name": "Loading snapshot",
    "messages": "I have new qdrant client and a local snapshot file but I'm unable to load that. The error I get is \nb'{\"status\":{\"error\":\"Bad request: Snapshot file \\\\\"/collection-7536304922932827-2023-10-06-11-53-38.snapshot\\\\\" does not exist\"},\"time\":0.002067665}'\nClient is new so the collection does not exist but even after creating the collection I cant load it using \nclient.recover_snapshot(\"test\",\"file:///collection-7536304922932827-2023-10-06-11-53-38.snapshot\")"
  },
  {
    "threadId": "1160694576265297922",
    "name": "Disparate data in collections - best practices",
    "messages": "I noticed there are no compound indices. From the docs it says the best practice is to make one collection or one collection per customer - not one collection per entity type. This is very abnormal for me as I come from a background of multi collections in noSQL. \n\nI have 10 different entity types that i am vectorizing and storing datapoints for. My data also is of the format [ clientId, entityId, knowledgeTypeId] - so it's 3 layers of depth. If I were doing a single collection per clientId i would still need a compound index to query by entityId and knowledgeTypeId. \n\nIs this setup a candidate for multiple collections per client - 1 per entity Type? I may have 1 client with 10 entity types and each entity type may have 100 knowledge types, and for each knowledge type there may be 10 data points. I expect to have clients in the hundreds.\n\nSoo 100 clients * 10 entity / client - 100 knowledge types / entity * 10 data points / knowledge type = 100 collections of 10,000 vectors of 100 types/payload formats. \n\nWhat are the best practices in this situation?"
  },
  {
    "threadId": "1156216258929377340",
    "name": "Unable to take snapshot of one collection, despite being able to take snapshots of other collections",
    "messages": "Hello, we are trying to take a snapshot of one of our collections `gte-embedding3`, but the snapshot fails with the error below. Meanwhile, if we try to take a snapshot of a different collection `gte-embedding-test3`, it successfully creates it.\n\nI have tested creating the snapshot via the dashboard console as well as the dashboard GUI button \"Take Snapshot.\" In both cases, the `gte-embedding3` fails but the `gte-embedding-test3` succeeds.\n\nI know that the `gte-embedding3` exists, becuase I am able to query it via similiarity search, and I can add new vectors to it. \n\nOne of the main differences between these two collections is the size. `gte-embedding3` is likely around 1GB, while `gte-embedding-test3` is 67 MB.\n\nWe are using Qdrant 1.5.0"
  },
  {
    "threadId": "1161191827366494249",
    "name": "Connection Pool",
    "messages": "How many user connection can handle (is there any connection pool )?"
  },
  {
    "threadId": "1160843813460385813",
    "name": "Snapshots and Sharding",
    "messages": "Hi, I wanted to ask how snapshots behave if a collections is sharded across multiple nodes.\nEspecially, can I create a snapshot on a single node instance and then upload the snapshot to a sharded instance? And vice-versa?\nOr am I limited to only upload snaphots to instances that have the exact same configuration as the one where the snapshot was created?"
  },
  {
    "threadId": "1160918302130974740",
    "name": "Handle timeout in python client",
    "messages": "Hi, i want to create a json to list all by datas from my db in order to do some semantic search on them. \n\nI try to list all the point with the python-cli like that : \n```\n    test = client.scroll(\n    collection_name=\"legal_bot_30_08_23\",\n    limit=1000000,\n    with_payload=True,\n    with_vectors=True,\n    )```\n\nAnd i get a timeout error : qdrant_client.http.exceptions.ResponseHandlingException: timed out\n\nIs there a way to modify this timeout ? \n\nThanks"
  },
  {
    "threadId": "1160926528591368343",
    "name": "Payload error: JSON payload (77382309 bytes) is larger than allowed",
    "messages": "UnexpectedResponse: Unexpected Response: 400 (Bad Request)\nRaw response content:\nb'{\"status\":{\"error\":\"Payload error: JSON payload (77382309 bytes) is larger than allowed (limit: 33554432 bytes).\"},\"time\":0.0}'\n\nShould I go for inserting one by one ? \nIm inserting list of embeddings like this"
  },
  {
    "threadId": "1161004480767217724",
    "name": "Handling ScrollPoints returns in Rust",
    "messages": "Hello, i am creating an api call where i get a specific amount of points back, but i would like to parse the return into a JSON object, how is it possible to do it? i could not find a solution to it Thanks in advance"
  },
  {
    "threadId": "1158920966722834554",
    "name": "Debugging pod startup issue on GKE kubernetes when deploying qdrant using helm",
    "messages": "Hi, I am trying to debug a pod startup issue when deploying qdrant using helm on a GKE kubernetes cluster. Pod events show:\n\nWarning  Unhealthy               1s (x98 over 9m51s)  kubelet                  Startup probe failed: Get \"http://10.224.171.11:6333/readyz\": dial tcp 10.224.171.11:6333: connect: connection refused\n\nWhen I look at the logs for the pod, all I see is:\n\n2023-10-04T00:08:23.491615Z  INFO storage::content_manager::consensus::persistent: Loading raft state from ./storage/raft_state.json\n2023-10-04T00:08:23.493903Z  INFO storage::content_manager::toc: Loading collection: knowledge-base\n\nAfter sometime the pod restarts and the same thing repeats. I set the recovery mode env var but that doesn't seem to stop this never ending restarts.\n\nQDRANT_ALLOW_RECOVERY_MODE:  true\n\nAny suggestions on how to debug and resolve this issue?"
  },
  {
    "threadId": "1159597347999850637",
    "name": "Understanding how to create qdrant infrastructure",
    "messages": "So I would like to deploy an app which allows searches of documents using qdrant. I would like it to be serverless. My thinking was: when the app runs, it gets the persisted database from s3 bucket, by calling QdrantClient(path=persistent_db_folder). But in this way i cannot use payload indexes. What should i do instead? I am a bit confused. I want to keep my qdrant application serverless, having always the last persisted state"
  },
  {
    "threadId": "1159393387519160351",
    "name": "How to change the distance metrics during search?",
    "messages": "Hi, may I know how can I switch to other distance metrics when performing search?"
  },
  {
    "threadId": "1157160099354247188",
    "name": "Issues updating a collection's HnswConfig to have on_disk=True",
    "messages": "Am I doing something wrong here?"
  },
  {
    "threadId": "1157045942340571206",
    "name": "Too many open files",
    "messages": "Hi all,\nWe are running qdrant as a hosted service in our environment and the server has been running for quite some time now.\nToday we got this error when creatig a new collection.\n\ngrpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n    status = StatusCode.INTERNAL\n    details = \"Service internal error: RocksDB open error: IO error: While open a file for appending: ./storage/collections/ai_assistant_9a320b07_e7ba_415d_82fe_365b95627c3d$cmfp$documentation/0/segments/3b17ef49-5d33-4306-b1dd-7f386bd3042c/payload_index/000000.dbtmp: Too many open files\"\n    debug_error_string = \"UNKNOWN:Error received from peer  {created_time:\"2023-09-28T19:53:34.30880701+00:00\", grpc_status:13, grpc_message:\"Service internal error: RocksDB open error: IO error: While open a file for appending: ./storage/collections/testsfds3421124123$$doc/0/segments/3b17ef49-5d33-4306-b1dd-7f386bd3042c/payload_index/000000.dbtmp: Too many open files\"}\"\n\nHow to resolve this error and how to avoid this error in the future?\nCan anyone please help resolving this error?"
  },
  {
    "threadId": "1156939096212652145",
    "name": "upload_collection warning",
    "messages": "Im getting a lot these messages using upload_collection \nWARNING:root:Batch upload failed 3 times. Retrying..."
  },
  {
    "threadId": "1156871826333507604",
    "name": "Pagination",
    "messages": "Can we use pagination with Qdrant ?"
  },
  {
    "threadId": "1156551249420898334",
    "name": "Qdrant Upgrade",
    "messages": "Hi, I am planning upgrade Qdrant DB which is installed on my AKS, can you please let me know the step/process for upgrade or if there is any document to refer"
  },
  {
    "threadId": "1150533408947175585",
    "name": "Qdrant Docker UI dashboard and other doubts on project specific. how Qdrant will suit my project",
    "messages": "Please confirm when the qdrant image is installed in the companies production environment, will i have an dashboard to see what all clisters and collections created during the chatbot creation. similar to this"
  },
  {
    "threadId": "1155965183794086000",
    "name": "x86 to ARM Migration",
    "messages": "Do I need to do anything special when switching out the instance type and image to ARM or should it just boost up fine with the existing collections created on x86? The persistent data is all on an attached EFS volume on the cluster."
  },
  {
    "threadId": "1152604870545776670",
    "name": "scoring points using filters",
    "messages": "IтАЩm trying to see if there is a way to perform a query that is essentially тАЬhow far are these points?тАЭ rather than тАЬwhat is near?тАЭ. I know I can use filter on the search endpoint but even with a payload index, it is very slow (relatively speaking). Without payload index, it takes 5-6s and with the index it takes 500-600ms. \n\nSearch without filter is very efficient and we are happy with that. It would be amazing if we could leverage qdrant for our other use case as well all our data is in it already.\n\nI understand that qdrant doesnтАЩt do pre-filtering. It makes sense for the common use case it is built for. However, I believe pre-filtering would make my use case work. Any ideas how I may be able to achieve that?"
  },
  {
    "threadId": "1154295667427123240",
    "name": "Hi  everyone,",
    "messages": "I am getting the below-mentioned error. Is it because of too many indexing calls or something else?\nCan you help me with this or how to handle the below error?\n\n```Panic occurred in file lib/collection/src/update_handler.rs at line 243: Optimization error: Service internal error: Removing proxy segment which is still in use: \"./storage/collections/library-paragraph-embeddings/64/segments/1573345f-88d3-4a2c-b5f5-98b3a747e344\"```"
  },
  {
    "threadId": "1154701551303135243",
    "name": "Langchain PQ",
    "messages": "Hello Team, someone already use Product Quantization with Qdrant in Langchain? If yes, can you share a example ?"
  },
  {
    "threadId": "1155057398529146900",
    "name": "Is there a way to stop a Snapshot process?",
    "messages": "Once I trigger a snapshot with `POST /snapshots` on a node, is there a way to stop it?"
  },
  {
    "threadId": "1155437328593072199",
    "name": "filter on payload using > and < operators",
    "messages": "I have a duration field in payload which is integer/float. I want to retrieve only those vectors whose duration is larger than a predefiend value. \nHow can I do it in python?"
  },
  {
    "threadId": "1155483170284785774",
    "name": "How do I check if a collection already exists?",
    "messages": "Hey folks,\nI was wondering how I am supposed to check whether a collection already exists in python.\nCurrently I am doing:\n`\n        try:\n            # The next line throws an exception if the collection does not exist (is there a better way to do that?)\n            self.client.get_collection(final_name)\n        except qdrant_client.http.exceptions.UnexpectedResponse as e:\n            if e.status_code == 404:\n`\nwhich on one hand seems a pretty inefficient way to do that and on the other hand seems quite brittle (404 might not always mean that the collection simply does not exist?)\n\nThanks for your help!"
  },
  {
    "threadId": "1155166407504429157",
    "name": "What http server library should i use with rust qdrant",
    "messages": "I was making it in python flask, but i needed async support for a couple tasks, i thought of switching over to actix rust but i couldnt get it to work any suggestions?"
  },
  {
    "threadId": "1154895558578811000",
    "name": "embedding generation with Rust",
    "messages": "Hi, since you are building with Rust may I ask if you have come across text and image embedding generation with Rust? \n\nI have found rust-bert for text embeddings. And recently huggingface released candle. For generating text embeddings, there are some options like rust-bert or rust-sbert. But couldn't find anything related to image embedding generation. \n\nAny advice on that?"
  },
  {
    "threadId": "1154855118785744966",
    "name": "Create a collection compatible with new fastembed simpler API",
    "messages": "Hi all,\nI am trying to use the new simpler API: https://github.com/qdrant/qdrant-client/blob/master/README.md#fast-embeddings--simpler-api\nUnfortunately I cannot find any documentation on how to create a collection that is compatible with this API.\nI tried:\n`\n                embedding_size, embedding_distance = self.client._get_model_params(\n                    self.client.embedding_model_name\n                )\n                self.client.create_collection(\n                    collection_name=final_name,\n                    vectors_config={\n                        \"size\": embedding_size,\n                        \"distance\": embedding_distance,\n                    },\n                )\n`\nthis already seems fishy as I have to use an underscore method to get the parameters. Also when I then call the add method I get:\n`\nAssertionError: Collection have incompatible vector params: size=384 distance=<Distance.COSINE: 'Cosine'> hnsw_config=None quantization_config=None on_disk=None\n`\nCan anyone point me to the documentation explaining how to create collections that are compatible with the new API?\n\nThanksЁЯШГ"
  },
  {
    "threadId": "1154733734633943040",
    "name": "Find the Leader node?",
    "messages": "Is there a way to know/find out which node/peer is the (RAFT) leader?"
  },
  {
    "threadId": "1153340130808037549",
    "name": "Improving search performance",
    "messages": "I'm having issues retrieving the most relevant documents from my vectorstore (hosted on Qdrant Cloud). \n\nFor example, the embedding of the text `What is the elongation of Docol 1100M?` is being used as input and the most relevant documents are:\n\n#0 (0.85021555)\n```This is part of a document with the title Docol 1100M \n# Chemical Composition (ladle analysis)```\n\n#1 (0.8468354) \n```\"This is part of a document with the title Docol 1000DP\"```\n...\n\n#6 (0.8426687) \n```Row 0 in a table from a document with the title Docol 1100M\n\nDocol CR860Y 1100T-MS\nStandard : SSAB\nCoating : UC, EG\nTest direction : L\nYield strength Rp0.2(MPa) : 860 - 1100\nTensile strength Rm (MPa) : 1100 - 1300\nElongation A80 (min %) : 3\nMin. inner bending radius for a 90┬░ bend : 3.5 x t```\n...\n\nThe document holding the answer to the question is only ranked 6, which is not good enough for my application. It feels like it shouldn't be that hard to get a higher rating since it contains both the words `Docol1100m` and `Elongation`. \n\nI'm using OpenAI's text embedding model `text-embedding-ada-002`. The search method for this collection is `cosine`. I assume there is no point of trying Euclidean distance since OpenAI's embedding vectors are normalized? \n\nI've tried using `models.SearchParams(hnsw_ef=128, exact=True)` without seeing any improvement. \n\nIs there anything else I can do to try to improve the result using Qdrant or is the next step to look at other embedding functions? I could definitely afford to lower the lookup speed if it meant improving the result."
  },
  {
    "threadId": "1153697861389791282",
    "name": "exact matches not returned by similarity search",
    "messages": "Hi there, I have a similarity search use case, where the queried vector may already exists in the collection. In the search result, this should  appear as a neighbor with distance=0, but is often missing. Increasing the hnsw_ef parameter does not help. Current workaround is to use exact search - but I was wondering if there is an explanation for this behaviour, and a solution using hnsw-based search?"
  },
  {
    "threadId": "1150318385444429844",
    "name": "Finetuning or RAG",
    "messages": "Hi. Do I need vector database to do finetuning or RAG of an LLM we are using? If yes, how? Thank you."
  },
  {
    "threadId": "1150691372303667220",
    "name": "\"Stream removed\" Error",
    "messages": "Hey guys! We're using qdrant cloud and our API went temporarily down for around a minute due to an error we were receiving from qdrant. When using the search method in the python library, it failed, and this was included in the stack trace:\n```\n    2023-09-09T02:11:49.062-07:00    File \"/usr/local/lib/python3.11/site-packages/qdrant_client/qdrant_client.py\", line 310, in search    \n    2023-09-09T02:11:49.062-07:00    return self._client.search(\n    2023-09-09T02:11:49.062-07:00    ^^^^^^^^^^^^^^^^^^^^    \n    2023-09-09T02:11:49.062-07:00    File \"/usr/local/lib/python3.11/site-packages/qdrant_client/qdrant_remote.py\", line 421, in search    \n    2023-09-09T02:11:49.062-07:00    res: grpc.SearchResponse = self.grpc_points.Search(\n    2023-09-09T02:11:49.062-07:00    ^^^^^^^^^^^^^^^^^^^^^^^^\n    2023-09-09T02:11:49.062-07:00    File \"/usr/local/lib/python3.11/site-packages/grpc/_channel.py\", line 1161, in __call__\n    2023-09-09T02:11:49.062-07:00    return _end_unary_response_blocking(state, call, False, None)\n    2023-09-09T02:11:49.062-07:00    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    2023-09-09T02:11:49.062-07:00    File \"/usr/local/lib/python3.11/site-packages/grpc/_channel.py\", line 1004, in _end_unary_response_blocking\n    2023-09-09T02:11:49.062-07:00    raise _InactiveRpcError(state) # pytype: disable=not-instantiable    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    2023-09-09T02:11:49.062-07:00    grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n    2023-09-09T02:11:49.062-07:00    status = StatusCode.UNKNOWN\n    2023-09-09T02:11:49.062-07:00    details = \"Stream removed\"\n    2023-09-09T02:11:49.062-07:00    debug_error_string = \"UNKNOWN:Error received from peer {grpc_message:\"Stream removed\", grpc_status:2, created_time:\"2023-09-09T09:11:49.057623957+00:00\"}\"\n```\nDoes anyone know what can lead to this kind of error?"
  },
  {
    "threadId": "1150745707137998848",
    "name": "I've been trying to use Qdrant for my chatbot which I'm still working on but I'm getting this error",
    "messages": ""
  },
  {
    "threadId": "1149449744171671662",
    "name": "Local Qdrant Snapshot throwing  No such file or directory (OS error 2)",
    "messages": "I have an instance of qdrant running locally on docker using the qdrant/qdrant:latest image. I have 11 collections, all of which are empty except for rrdb_matter_2. I haven't experimented with sharding or clusters yet, I am simply trying to test snapshots at scale.\nThe collection's stats are as follows:\n\"status\": \"green\",\n\"optimizer_status\": \"ok\",\n\"vectors_count\": 227400,\n\"indexed_vectors_count\": 194100,\n\"points_count\": 75800\nWhen I attempt to make a POST request to create a new snapshot (/collections/rrdb_matter_2/snapshots), I am eventually met with the following error:\n[2023-09-07T20:37:53.799Z WARN  qdrant::actix::helpers] error processing request: Error while copy WAL \"./storage/snapshots_temp/rrdb_matter_2-1016472092926465-2023-09-07-20-37-31.tmp/0\" No such file or directory (os error 2)\n[2023-09-07T20:37:53.799Z INFO  actix_web::middleware::logger] 172.17.0.1 \"POST /collections/rrdb_matter_2/snapshots HTTP/1.1\" 500 195 \"-\" \"Apache-HttpClient/4.5.13 (Java/17.0.2)\" 21.905568\n\nI will say that my machine is a bit low on RAM (27.6GB/32GB). Would this affect/restrict the ability for Qdrant to store files in temp_path?\nPlease let me know if you need any more information."
  },
  {
    "threadId": "1151006478891749498",
    "name": "Get unique values in payload fields",
    "messages": "Is it possible to use the API to search for unique values that appear in a certain payload field? We are hoping to create a drop-down for some categorical fields that should only contain a handful of values and it would be helpful to be able to make an API call to get all the possible unique values before applying a filter."
  },
  {
    "threadId": "1151464043673034822",
    "name": "Silent crashloop after restart",
    "messages": "Our node which was hosting a single instance qdrant cluster was restarted as part of routine management from GCP. This resulted in qdrant entering an endless silent crashloop cycle. The logs got as far as \"rebuilding collection\" then the process would exit without any obvious message as to what or why caused it to panic.\n\nI tried increasing the size of the node to have 128GB RAM - but this didn't work. I increased the log level to DEBUG, but no additional helpful logs were reported. I increased the timings for the readiness / liveness probes in case it was external termination from kubernetes - agian this didn't help.\n\nConfig settings are as per the defaults in the helm chart deployment. Unfortunately I really needed the service running again, and so the only way I found to get it to successfully launch was by deleting the persistent volume (and thus the backing data for the collections). This the allowed the service to start without any issues. I think at the time of failure we had around 10,000 - 15,000 vectors of length 768, including some metadata.\n\nI guess the main request of this, is what else should I be trying to do if this happens again to understand what is going wrong, and hopefully recover without dropping the whole database, requiring a lengthy rebuild of the system."
  },
  {
    "threadId": "1152371945560608798",
    "name": "I have a 300M vector collection and it stopped indexing after 20M points indexed. The status is",
    "messages": "\"Green\". Has anyone encountered similar issue before? The detailed collection information:\n{\n  \"result\": {\n    \"status\": \"green\",\n    \"optimizer_status\": \"ok\",\n    \"vectors_count\": 286852373,\n    \"indexed_vectors_count\": 20640999,\n    \"points_count\": 286852373,\n    \"segments_count\": 44,\n    \"config\": {\n      \"params\": {\n        \"vectors\": {\n          \"size\": 512,\n          \"distance\": \"Dot\"\n        },\n        \"shard_number\": 11,\n        \"replication_factor\": 1,\n        \"write_consistency_factor\": 1,\n        \"on_disk_payload\": false\n      },\n      \"hnsw_config\": {\n        \"m\": 64,\n        \"ef_construct\": 256,\n        \"full_scan_threshold\": 10000,\n        \"max_indexing_threads\": 32,\n        \"on_disk\": false\n      },\n      \"optimizer_config\": {\n        \"deleted_threshold\": 0.2,\n        \"vacuum_min_vector_number\": 1000,\n        \"default_segment_number\": 2,\n        \"max_segment_size\": 8000000,\n        \"memmap_threshold\": null,\n        \"indexing_threshold\": 40000,\n        \"flush_interval_sec\": 5,\n        \"max_optimization_threads\": 1\n      },\n      \"wal_config\": {\n        \"wal_capacity_mb\": 32,\n        \"wal_segments_ahead\": 0\n      },\n      \"quantization_config\": null\n    },\n    \"payload_schema\": {\n      \"created_at_msec\": {\n        \"data_type\": \"integer\",\n        \"points\": 286852373\n      }\n    }\n  },\n  \"status\": \"ok\",\n  \"time\": 0.001240511\n}"
  },
  {
    "threadId": "1153360883595083817",
    "name": "Suggestions on upsert without point IDs",
    "messages": "I'm managing a Qdrant collection containing over 200,000 records, which are periodically refreshed. Each update batch consists of approximately 500 records, including both vectors and payloads. \n\nThe issue arises because these update requests lack point IDs, which are mandatory for the `upsert` API. \n\nConsequently, I have to use the `scroll` API first to retrieve these point IDs using unique payload identifiers, before I can execute the `upsert` operation. This adds considerable latency to the entire process. I'd be grateful for any insights or recommendations to streamline this workflow.\n\nI am using python qdrant_client."
  },
  {
    "threadId": "1153374069664784496",
    "name": "Status red: how to restore qdrant full operativity?",
    "messages": "The current status of my collection is red.\nWith a detail like the following:\n```\nService internal error: Can't remove segment data at ./storage/collections/sentences/0/segments/cc12d213-4616-4ee3-b7ba-b7289a88e5a6.deleted, error: Resource temporarily unavailable (os error 11)\n```\nI am still getting search results, but not the ones I would be expecting (in particular, looks like at least a few records are missing).\n\nThe error 11 is a network related error, because the storage folder is on a azure file share.\n\nHow can I restore full operativity? (reboot? delete the file in the log?)\n\nShall I reindex everything? How can I know which records should be reindexed?\n\nThe qdrant version is 1.3.0"
  },
  {
    "threadId": "1150928949166227466",
    "name": "Problem of retrieval that respects mentioned ids/terms",
    "messages": "So far I am having huge problems with embeddings (tried a lot of them, from ada-2, to BGE-large and biolinkbert) that when user asks question about specific gene, or gene variant or any other entity with id the similarity search returns passages with totally different genes/variants/whatever. I wonder what would be a strategy to mitigate this? So far I am thiking if it is possible to combine fulltext search with embedding one as fulltext at least respects terminology and does not give a lot of top results for totally unrelated genes. Another can be NER recognition, the problem is that the number of NER is so huge that it is impossible to put them to metadata. What other soolutions you would suggest? So far this problem makes vector databases totally useless for me for almost a half user queries."
  },
  {
    "threadId": "1149330276124725258",
    "name": "Did you already upgrade to v1.5? ЁЯШЙ",
    "messages": "This is the first post in our new forum-style channel. The new version v1.5 is already available on GitHub https://github.com/qdrant/qdrant/releases/tag/v1.5.0 The official announcement will come soon. \nWelcome to discuss new new features in this thread."
  },
  {
    "threadId": "1153993573092565052",
    "name": "Migration to the new model, in the prod without downtime",
    "messages": "Are there any known ways/tools to migrate vectors to the new version of the model, maybe integrated to qdrant?"
  },
  {
    "threadId": "1154091342826320003",
    "name": "Estimated memory size is half of the real amount - why?",
    "messages": "I read in the binary quantization post the famous formula\n```\nmemory_size = 1.5 * number_of_vectors * vector_dimension * 4\n```\nSo, in my case I have 442k vectors with a dimension of 1536.\n\nThe real amount of memory is 6.67 GiB (observed through Datadog metrics, for the container executing qdrant).\nThe formula, in GiB, returns 3.64.\n\nCould the difference in memory be the fruit of the three keyword/integer indexes I have asked to build? Is there a formula to estimate their size?"
  },
  {
    "threadId": "1150132494100869180",
    "name": "Using Qdrant for pdf vs pdf(s) comparison.",
    "messages": "Hi! This is not related to Qdrant, but an overall question. I would appreciate your help.\n\nI have one pdf (pdfA) on one side and multiple pdfs (pdfsN) on another side.\n\nI need to scan those pdfs and list down all the contradictions between pdfA and pdfsN.\n\nFor example, If somewhere in pdfA it is written that the company made $30mil revenue in 2021, but somewhere in pdfsN it is written that the company made revenue of $40mil (or even made loss), I need to output it.\n\nCould you please help me with some ideas on how to develop it? How can I utilize Qdrant for that?"
  },
  {
    "threadId": "1154006765302710333",
    "name": "grpc._channel._InactiveRpcError",
    "messages": "Hi, we're ocassionally getting this error in production while using the Python client to search for embeddings (using the `search()` method).  We're using qdrant's cloud offering. I've added logic to re-instantiate the qdrant client in the event that there's an error, and to try the request again up to 3 times, but it's failing for all retries. Can someone help diagnose the issue and/or offer suggestions on how to avoid it? Thank you. Here is the full error:\n```\n2023-09-20T02:45:41.647-07:00    grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n    2023-09-20T02:45:41.647-07:00    status = StatusCode.UNKNOWN\n    2023-09-20T02:45:41.647-07:00    details = \"Stream removed\"\n    2023-09-20T02:45:41.647-07:00    debug_error_string = \"UNKNOWN:Error received from peer {created_time:\"2023-09-20T09:45:41.644852315+00:00\", grpc_status:2, grpc_message:\"Stream removed\"}\"\n    2023-09-20T02:45:41.647-07:00    >\n```"
  },
  {
    "threadId": "1151726069385273425",
    "name": "Multitenancy: Collections Vs. Payload Partition",
    "messages": "https://qdrant.tech/documentation/tutorials/multiple-partitions/\n\nI read through the Multitenancy setup advice in the docs, but still have some questions, hope someone can help to clarify.\n\nIn my use case I have multiple users which have separated datasets. (chat and memory for a chatbot. Chat over your data & memory  with one chatbot for each dataset). \nSo each query as well as indexing operation for adding new vectors will only apply to one isolated dataset. I will never query all datasets, so all datasets are isolated.\n\nQuestions:\n- In the docs its says, only use collections if you have limited amount of users. How much is limited? For me it will be a couple hundred to thousands\n- there is a mention that collections create an overhead and thate one needs to ensure that they do not affect each other in any way, including performance-wise. What does that mean? Can I use a couple thousand collections if they are all isolated, or is that a no go because the overhead would kill the performance?\n\n- for using partition by payload to optimize performance it is adviced to set\n\n    \"hnsw_config\": {\n        \"payload_m\": 16,\n        \"m\": 0\n    }\n\nI am trying to understand these, settings, the explenation in the docs is rather short.\n\nits says: \"By adopting this strategy, Qdrant will index vectors for each user independently, significantly accelerating the process.\"\n\nIn my limited understanding m:0  limits the number of nodes neighbors nodes to 0, so this wont build a global graph as there are no neighbors nodes.\n\npayload_m:\nCustom M param for additional payload-aware HNSW links. If not set, default M will be used.\nWhat are  payload-aware HNSW links? A payload index created with \n\n{\n    \"field_name\": \"group_id\",\n    \"field_schema\": \"keyword\"\n}\n\nwould normaly build an index for that field_name. But the docs suggest that this now builds a separate index for each value of the field_name\nso one separate index for user_1, user_2,....etc.\nis that correct?"
  },
  {
    "threadId": "1153617148233662484",
    "name": "qdrant-txtai for large scale similarity search",
    "messages": "Hi, I am already using qdrant locally for one of my LLM application. I recently came across about qdrant-txtai. I am working on a new usecase finding similarity on large-scale doc list. Is it the standard tool if I want to build a solution finding large-scale doc similarity or are there already some favoured approaches? (as I see qdrant-txtai repo is couple months old, but things happening quite fast in the field)"
  },
  {
    "threadId": "1151097729645297684",
    "name": "qdrant web UI on azure ml studio",
    "messages": "Is there a way to view the qdrant web Ui inside azure machine learning studio?"
  },
  {
    "threadId": "1151914008861999204",
    "name": "creation of snapshot, run out all of the disk space (other than location of my persistent disk loca)",
    "messages": "docker run -p 6333:6333 \\\n     -v /media/.../nvme2_db/pubmed_search_vector_payload_on_disk:/qdrant/storage:z \\\n     qdrant/qdrant\n\nI use the above command to start my qdrant server in my local settings. I click the button in the web UI to create snapshots., my server create a snapshot at \n/media/.../nvme2_db/pubmed_search_vector_payload_on_disk/tmp (as mentioned in your documentation)\n\nbut during the process, it generate a .tmp file in the docker overlay2 folder as well in my system\n/var/snap/docker/common/var-lib-docker/overlay2/32af7a450e31049....\n/diff/qdrant/snapshots/pubmed_search_vector_payload_on_disk/pubmed_search_vector_payload_on_disk-8968763481894006-2023-09-12-22-21-29.tmp\n\nThis poses a problem because my vector db is so large (persistent disk /media/.../nvme2_db is like 220 gb with 20 million vector/payloads data) and at /var/snap/docker/common/ i simply have only 30gb mounted.\nIs there any approach that I can customized the location of .tmp file in this case ? or is there other method to deal with this ?\n\nThanks a lot  ЁЯШЖ"
  },
  {
    "threadId": "1153689011588698232",
    "name": "API for local Qdrant",
    "messages": "Hello So i want to set up API for my qdrant which i have hosted in my AWS server So the problem which i am facing is if i pull the docker image using this cmd  docker pull qdrant/qdrant i am not able to make changes in the that config file so that i can setup up API for it \nSo I tried the other way like Git clone the repo made the changes in config and then started building the image using sudo docker -t build qdrant/qdrant:v1.5.1 but my server keeps crashing \nSo any suggestions how can i solve this issue in the step 1 itself\nand also any advice like if any updates comes in the qdrant how can i update it without losing the data or having a down time."
  },
  {
    "threadId": "1154245098482176020",
    "name": "Vector missed when importing into conllection",
    "messages": "hi team, I encountered vector loss when importing data into the collection. is there any vector limitя╝Я that is my collection setting below : \n\n{\n  \"result\": {\n    \"status\": \"yellow\",\n    \"optimizer_status\": \"ok\",\n    \"vectors_count\": 1048903,\n    \"indexed_vectors_count\": 1024557,\n    \"points_count\": 1048903,\n    \"segments_count\": 13,\n    \"config\": {\n      \"params\": {\n        \"vectors\": {\n          \"size\": 1536,\n          \"distance\": \"Cosine\"\n        },\n        \"shard_number\": 3,\n        \"replication_factor\": 1,\n        \"write_consistency_factor\": 1,\n        \"on_disk_payload\": true\n      },\n      \"hnsw_config\": {\n        \"m\": 0,\n        \"ef_construct\": 100,\n        \"full_scan_threshold\": 10000,\n        \"max_indexing_threads\": 0,\n        \"on_disk\": false,\n        \"payload_m\": 16\n      },\n      \"optimizer_config\": {\n        \"deleted_threshold\": 0.2,\n        \"vacuum_min_vector_number\": 1000,\n        \"default_segment_number\": 0,\n        \"max_segment_size\": null,\n        \"memmap_threshold\": null,\n        \"indexing_threshold\": 20000,\n        \"flush_interval_sec\": 5,\n        \"max_optimization_threads\": 1\n      },\n      \"wal_config\": {\n        \"wal_capacity_mb\": 32,\n        \"wal_segments_ahead\": 0\n      },\n      \"quantization_config\": null\n    },\n    \"payload_schema\": {\n      \"group_id\": {\n        \"data_type\": \"keyword\",\n        \"points\": 1047673\n      }\n    }\n  },\n  \"status\": \"ok\",\n  \"time\": 0.00095162\n}"
  },
  {
    "threadId": "1149776391596945428",
    "name": "Qdrant Cloud Update Stuck Initializing",
    "messages": "Hey guys, we have an **emergency**. How long does it typically take to upgrade a qdrant cluster in qdrant cloud? I recently updated a dev cluster, and it took a minute or two at most. I'm now updating our production cluster, and it's been \"initializing\" for some time now. I'm upgrading to v1.5.0 for reference."
  },
  {
    "threadId": "1154374440042381312",
    "name": "Suggestions on maintaining Vector Embeddings",
    "messages": "We have a MongoDB collection containing around 200K records, each with a specific text field. The goal is to implement a vector similarity search replacing text keyword search on this text field to provide relevant search results to users. Currently, the MongoDB collection does not contain any vector embeddings for the text field. We are considering using Qdrant to store these embeddings and perform the similarity search. \nAny advice on implementing this vector similarity search feature while ensuring data consistency between MongoDB and Qdrant?"
  },
  {
    "threadId": "1153588396942438420",
    "name": "Difference between vectors_count and points_count",
    "messages": "Hello there, \n\nI'd like to know what could make vectors_count different from points_count?\n\n```json\n\"vectors_count\": 158624,\n\"indexed_vectors_count\": 158624,\n\"points_count\": 158222,\n```"
  },
  {
    "threadId": "1153209242745053194",
    "name": "Consistency parameter",
    "messages": "Greetings, all!\n\nI encounter this Query Parameter in numerous endpoints, but to be frank, I'm struggling to grasp its precise purpose:\n\n```\nRead consistency parameter\n\nDefines how many replicas should be queried to get the result\n\n    N - send N random request and return points, which present on all of them\n\n    majority - send N/2+1 random request and return points, which present on all of them\n\n    quorum - send requests to all nodes and return points which present on majority of them\n\n    all - send requests to all nodes and return points which present on all of them\n\nDefault value is Factor(1)\n```\n\nCould someone please furnish me with a real-world example? This would greatly assist me in comprehending how to effectively utilize this parameter."
  },
  {
    "threadId": "1153273246490312705",
    "name": "Similar cars model input?",
    "messages": "I have been following the \"Similar cars search with similarity learning\" tutorial and I was just wondering what type of input the finetuned model expects? I was trying to do something like this ```tuned_cars_model = SimilarityModel.load(load_path)``` ```tuned_cars_model.encode(Image.open(filepath).convert(\"RGB\"))``` but I get a ```TypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>```. I suspect I need to convert the image to NumPy array? Am I using the intended way of using the model?"
  },
  {
    "threadId": "1153299351569957007",
    "name": "flat index in qdrant?",
    "messages": "Can we implement flat indexes in qdrant ?"
  },
  {
    "threadId": "1149385435399868426",
    "name": "Is there any way to stop indexing of the points, if im adding new fields into payload object?",
    "messages": "So im trying to update the payload of each point, adding new field \"metadata\". It's not index. But qdrant seems to reindex all data anyway, and it take really a lot of time, is there any way to prevent it?\n\nAlso, there's an index on other fields from the payload."
  },
  {
    "threadId": "1153311464619257917",
    "name": "QdrantClient(\":memory:\") does not provide metadata",
    "messages": "Hi, we want to test, that a collection does have expected configuration values. In my `pytest` test cases I used \n```\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import VectorParams, Distance,\n\nclient = QdrantClient(':memory:')  # same with client = QdrantClient(path=\"qdrant_data\")\nclient.create_collection(\n  collection_name=\"my_collection\", \n  on_disk_payload=False, \n  vectors_config=VectorParams(size=100, distance=Distance.COSINE)\n)\ncollection = client.get_collection(\"my_collection\")\n\nassert collection.config.params.on_disk_payload == False  # Fails\nassert collection.config.params.on_disk_payload == None   # Succeeds\n```\n\nThe first assertion fails, since `on_disk_payload` is `None`.\n\nWhen using `client = QdrantClient(url='localhost')` with `docker compose` setup, it works.  But I want to have simple unit tests for CI without the need for Docker in Docker. \n\nIs there any chance to test for real CollectionConfig values with in memory QdrantClient?"
  },
  {
    "threadId": "1153696035529564223",
    "name": "Connect client to Kubernetes",
    "messages": "Hello, i have qdrant cluster hosted on Kubernetes, i am able to query it in postman just fine (screenshot below), however i am trying to connect to it via the QdrantClient in a flask project, whatever method i try it always returns with a 404, here is a piece of my code below"
  },
  {
    "threadId": "1150101840990589029",
    "name": "Help to update ram usage after upgrade collection settings",
    "messages": "I changed settings collection and removed many unnecessory collection, but RAM usage not changed. Should i do something more to update RAM usage?"
  },
  {
    "threadId": "1153268080106020924",
    "name": "several disks",
    "messages": "There are several disks in a server. How do I configure Qdrant to use them?"
  },
  {
    "threadId": "1151181985646125058",
    "name": "Version support",
    "messages": "How long are versions supported on the cloud? I see 1.4.1 and 1.5.1, and it looks like the cadance is every month, so trying to understand upgrade cycles."
  },
  {
    "threadId": "1151441213308809267",
    "name": "Indexes creation before or after bulk upload",
    "messages": "hello! i am have collections with indexes based on different attributes and would like to ask what would be the best practice on indexes for a bulk load, should I create then BEFORE or AFTER upserting the points? Thanks!"
  },
  {
    "threadId": "1153053353522237670",
    "name": "how to use qdrant in cloud services like colab or kaggle",
    "messages": "I am trying to use qdrant on some datasets and models from transformers but couldn't use them in my labtop because of my low resources\nhow to use qdrant in cloud services and they didn't allow deploying docker within it! \ni"
  },
  {
    "threadId": "1153236110076563537",
    "name": "Where did my base go?",
    "messages": "Today i look in cloud dashboard and see that resource usage updated and my base is empty. Check pls\nhttps://674aff54-3b7d-4ccd-87c5-3fca54d06e34.us-east-1-0.aws.cloud.qdrant.io"
  },
  {
    "threadId": "1152188410337964102",
    "name": "Data Syncing delay between Replica nodes in the cluster",
    "messages": "I have started a qdrant cluster with 2 nodes and have enabled replication across the 2. While writing data i can see data gets synced between both the nodes and its working as expected. However if i bring down the secondary node and write date and then bring back the secondary node the newly written data is not getting synced to the secondary node. Also i dont see any logs when one of the node goes down in active node. When i hit the /cluster endpoint i do see an error but could get much details on it. PFB the response from /cluster endpoint\n\n{\"result\":{\"status\":\"enabled\",\"peer_id\":3211210833194464,\"peers\":{\"1906721573434346\":{\"uri\":\"http://172.25.0.2:6335/\"},\"3211210833194464\":{\"uri\":\"http://qdrant_primary:6335/\"}},\"raft_info\":{\"term\":773,\"commit\":13,\"pending_operations\":0,\"leader\":0,\"role\":\"Candidate\",\"is_voter\":true},\"consensus_thread_status\":{\"consensus_thread_status\":\"working\",\"last_update\":\"2023-09-15T10:17:55.401259881Z\"},\"message_send_failures\":{\"http://172.25.0.2:6335/\":{\"count\":48,\"latest_error\":\"Error in closure supplied to transport channel pool: status: Unavailable, message: \\\"Failed to connect to http://172.25.0.2:6335/, error: transport error\\\", details: [], metadata: MetadataMap { headers: {} }\"}}},\"status\":\"ok\",\"time\":0.000114459}"
  },
  {
    "threadId": "1153434470414831779",
    "name": "Batch search does not return payload",
    "messages": "Hey! Is there payload support for the search_batch function in the python API? When pulling from the API payload returns as None.\n\nThis is a snippet of code I'm using and getting the below result:\n\nCode:\n`        results = q_client.search_batch(\n            collection_name=\"ticket_classification\",\n            with_payload=True,\n            requests=[models.SearchRequest(\n                vector=encoder.encode(val_document).tolist(),\n                limit=3\n                )\n                for idx, val_document in enumerate(val_documents)\n            ]\n        )`\n\nResult:\n`[ScoredPoint(id=228001, version=0, score=1.0000002, payload=None, vector=None), ScoredPoint(id=227772, version=0, score=1.0000002, payload=None, vector=None), ScoredPoint(id=227807, version=0, score=1.0000002, payload=None, vector=None), ScoredPoint(id=227799, version=0, score=1.0000002, payload=None, vector=None)]`\n\nThanks!"
  },
  {
    "threadId": "1153138427856957510",
    "name": "Retrieve the whole collection",
    "messages": "you can retrive point from a collection using :\n```\nclient.retrieve(\n    collection_name=\"{collection_name}\",\n    ids=[0, 3, 10],\n)\n```\nbut how can i retrieve all the points from that collection with there id's"
  },
  {
    "threadId": "1152483920806412338",
    "name": "qdrant_client.http.exceptions.ResponseHandlingException: timed out",
    "messages": "Hi, Any thoughts why I'm getting the following exception? One off searches are still working in most cases. When I run 2-3 searches using my script it is throwing this error. I did a docker restart of qdrant.\n\nEverything was working fine earlier. I just created a new collection. Now I have total 5 collections with altogether ~3M vectors. Qdrant is taking 14G RAM from my 32G RAM machine!\n\nPlease let me know if any other information will help. My pipeline is stuck due to this. Thanks so much"
  },
  {
    "threadId": "1152288172613582909",
    "name": "Search params and search_batch",
    "messages": "Hello, how to specify `search_params` when using `search_batch?` I want to set `exact=False` and `hnsw_ef=myval`"
  },
  {
    "threadId": "1152232130101907506",
    "name": "Difference between collection vs collections in the volume",
    "messages": "Hello, I have just started Qdrant to store my embeddings. I am just exploring it locally and I was not able to access any collection after creating them with python client. Thats the code I use to create the collection \n\n```\n        client = OpenAITextEmbeddingsQdrant.get_qdrant_client()\n        vectors_config = models.VectorParams(size=1536, distance=models.Distance.COSINE)\n\n        client.recreate_collection(collection_name=\"qa_tool\", vectors_config=vectors_config)\n```\nSo I just had a look into docker volume and if I create the collection by python client it creates a  collection under collection path in volume as an sqlite file. But If I create a collection by api with http request, it adds the collection under collection**s** folder in the volume. Which I can see by get endpoint of collections.\n\nWhat is the difference between them? I am also not able to see anything created with python client when I call get endpoint of collections."
  },
  {
    "threadId": "1150678777077190696",
    "name": "Deadlock during insertion",
    "messages": "Im trying to bulk insert data and from time to time basis i get the below deadlock error. As of now i restart the container to get it fixed which is not the right way to resolve the error. Can you please guide me on how to debug the issue and how it can be handled in a better way.\n\n`qdrant-qdrant-1  | 2023-09-11T06:13:41.382011Z  WARN collection::collection_manager::holders::segment_holder: Trying to read-lock all collection segments is taking a long time. This could be a deadlock and may block new updates.    `\n\nQdrant version: 1.5.0\nRunning on a local container"
  },
  {
    "threadId": "1150962611467329636",
    "name": "is it able to just copy the folder where the persistent disk located to another computer and use it?",
    "messages": "hi\nI am using docker version of qdrant with version 1.5. I am a newbie to vector db.\n\nI already got a collection with 20million vectors in it. use without problem.\n\nhowever due to large size, I was unable to take a snapshot of it (fail for many times and my disk space is full)\n\nI am just wondering if I can just copy the persistent disk folder (in the following itis the /path/to/data) on-the-go and paste it on another computer with same docker environment and qdrant installrd\n\nthanks\n\nps. \nI am using the following command to start my qdrant sever\n\ndocker run -p 6333:6333 \\\n    -v $(pwd)/path/to/data:/qdrant/storage \\\n    qdrant/qdrant"
  },
  {
    "threadId": "1151109567862816808",
    "name": "Appropriate number of vectors in a collection",
    "messages": "Dear team, I have configured multitenancy with the field group_id and queried with group_id filter, but I having such a large vector in the collection (about 3000W) , could it lead to other bottlenecks? May i konw what's an appropriate value for the number of vectors in a collection?ЁЯлб"
  },
  {
    "threadId": "1150892322054230056",
    "name": "Why is qdrant recreating index every time I launch the app?",
    "messages": "Hi there, \nI'm using the following code and expecting qdrant not to recreate index everytime I launch the app as per the documentation on langchain (https://python.langchain.com/docs/integrations/vectorstores/qdrant) .   But I see everytime qdrant taking time to recreate the index ( 3 mins per html doc) and slowing the launch. Any help is appreciated.\n\n\n```\nqdrant = Qdrant.from_documents(\n    all_doc_splits,\n    ch_embed,\n    path=\"./local_qdrant\",\n    ## location=\":memory:\",  # Local mode with in-memory storage only\n    collection_name=\"html_documents\"\n)\n```"
  },
  {
    "threadId": "1151223542705299536",
    "name": "Filter: if doesn't match",
    "messages": "Hi, \n\nIs there a way to retrieve points with filtering where a specific index **doesn't** match with a categorical value.\n\nLet's say:\n\nindex \"city\" can be : \"Paris\", \"London\", \"New York\" and I want every points except those whose index city is equal to Paris."
  },
  {
    "threadId": "1151197326627569774",
    "name": "How are scores calculated?",
    "messages": "Hi all, what goes into the \"score\" calculation and what are the min/max values I can expect? I expected between 0 and 1 but I'm seeing \"scores\" between 40 and 112 - I would like to set `score_threshold` but not sure what a good value might be\n\nI'm seeing this on two collections, both scalar quantized, one with Euclidian distance, the other with Dot product"
  },
  {
    "threadId": "1151572622576979998",
    "name": "We are facing an issue with Qdrant DB Access - 503 service unavailable error.",
    "messages": "We have our Qdrant DB hosted on AKS cluster. Currently the cluster looks working fine. However, we are unable to reach Qdrant DB via the URL. Also, the Qdrant DB state is in \"waiting\"."
  },
  {
    "threadId": "1151857640826212413",
    "name": "metadata parameter in QdrantClient",
    "messages": "IтАЩm using below code to create qdrant client but there doesnтАЩt seem to be metadata parameter in the function. I need to pass token obtained from azure managed identity to authenticate . Pls help\n\nfrom qdrant_client import QdrantClient\n\nfrom azure.identity import ManagedIdentityCredential, DefaultAzureCredential\n\nQDRANT_APP_ID = \"your_qdrant_app_id\"\nQDRANT_APP_NAME = \"your_web_app_name\"\n\ndef get_user_token():\n    credential = DefaultAzureCredential(exclude_interactive_browser_credential=False)\n    result = credential.get_token(f\"api://{QDRANT_APP_ID}/.default\")\n    return result.token\n\nclient = QdrantClient(\n    url=f\"https://{QDRANT_APP_NAME}.azurewebsites.net\",\n    port=443,\n    metadata={\"Authorization\": f\"Bearer {get_user_token()}\"},\n)\n\nresult = client.get_collections()\nprint(result.collections)"
  },
  {
    "threadId": "1151909486953504779",
    "name": "how to color vectors in qdrant dashboard?",
    "messages": "I added a color key in payload and in dashboard IтАЩm using below command to color them but itтАЩs not showing all vectors"
  },
  {
    "threadId": "1149342980092743771",
    "name": "Does qdrant_client  python library expect sqlite3 to be installed",
    "messages": "Does qdrant_client  python library expect sqlite3 to be installed as well, even when you point to the cloud instance, connection seems to fail\n\n(venv3.10-snap) ubuntu@ip-172-31-39-30:~/content/content-service$ vi qdrant_db.py \n(venv3.10-snap) ubuntu@ip-172-31-39-30:~/content/content-service$ python3 qdrant_db.py \nTraceback (most recent call last):\n  File \"/home/ubuntu/content/content-service/qdrant_db.py\", line 1, in <module>\n    from qdrant_client import QdrantClient\n  File \"/home/ubuntu/content/content-service/venv3.10-snap/lib/python3.10/site-packages/qdrant_client/__init__.py\", line 1, in <module>\n    from .qdrant_client import QdrantClient as QdrantClient\n  File \"/home/ubuntu/content/content-service/venv3.10-snap/lib/python3.10/site-packages/qdrant_client/qdrant_client.py\", line 7, in <module>\n    from qdrant_client.local.qdrant_local import QdrantLocal\n  File \"/home/ubuntu/content/content-service/venv3.10-snap/lib/python3.10/site-packages/qdrant_client/local/qdrant_local.py\", line 16, in <module>\n    from qdrant_client.local.local_collection import LocalCollection\n  File \"/home/ubuntu/content/content-service/venv3.10-snap/lib/python3.10/site-packages/qdrant_client/local/local_collection.py\", line 17, in <module>\n    from qdrant_client.local.persistence import CollectionPersistence\n  File \"/home/ubuntu/content/content-service/venv3.10-snap/lib/python3.10/site-packages/qdrant_client/local/persistence.py\", line 5, in <module>\n    import sqlite3\n  File \"/home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/sqlite3/__init__.py\", line 57, in <module>\n    from sqlite3.dbapi2 import *\n  File \"/home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/sqlite3/dbapi2.py\", line 27, in <module>\n    from _sqlite3 import *\nModuleNotFoundError: No module named '_sqlite3'"
  },
  {
    "threadId": "1150476707061575831",
    "name": "separate database for IDs",
    "messages": "Hi. I watched a semantic kernel with Qdrant db on YouTube earlier. The dev advocate of Qdrant said that normally there's another database for keeping IDs? I am new to this. I just want a better explanation why do we need another database. Thank you."
  },
  {
    "threadId": "1150649736362217595",
    "name": "Can I put another unique string in the Point IDs instead of a UUID?",
    "messages": "I am an user who is using qdrant so well.\nOnce again, I'd like to take this opportunity to thank everyone for making this a great product to use...\nI know that you put a UUID String in the IDs item of the Points that make up the Collection, but I would like to ask if it is possible to put a 14 Byte String other than the UUID.\n\nThe other String is the _id String used by MongoDB, which is called ObjectID.\n\nex) ObjectId(\"542c2b97bac0595474108b48\")\n\nI want to use that String part instead of the UUID.\n\nThanks in advance\n\nBest Regards..."
  },
  {
    "threadId": "1149449498314166362",
    "name": "AWS Helm Deploy stalled",
    "messages": "I was able to helm install qdrant-release into my local k8s as per the steps in this guide.  However, running the same commands on a cluster running in amazon EKS, the pods are not coming up.  Containers seem blocked, but I'm EKS and helm noob.\n\nhttps://qdrant.tech/documentation/guides/installation/?selector=aHRtbCA%2BIGJvZHkgPiBkaXY6bnRoLW9mLXR5cGUoMSkgPiBzZWN0aW9uID4gZGl2ID4gZGl2ID4gZGl2ID4gYXJ0aWNsZSA%2BIGgyOm50aC1vZi10eXBlKDQp\n\nDoes this screenshot mean anything to anyone?"
  },
  {
    "threadId": "1149655113242263552",
    "name": "Minimum configurations to be included in SLA",
    "messages": "( continuation of <#1148856312587898920> )\nRegarding SLA mentioned the previous thread, is there any requirement in cluster configurations (e.g. number of nodes) to get the SLA?\nGCP Matching Engine requires 2 replicas at least to be included in their SLA."
  },
  {
    "threadId": "1149672027880427591",
    "name": "How to upgrade qdrant docker container?",
    "messages": "Is there any additional step needed than pull newer image and recreate container with the volume that stores data?"
  },
  {
    "threadId": "1149334605879586926",
    "name": "How to optimize memory",
    "messages": "Hey, I'm using memmap_threhold=1 along with hnsw_config on disk while writing large amount of data into qdrant, however the ram usage keeps increasing and the container eventually crashes. Can you please help me on how to limit the ram usage"
  },
  {
    "threadId": "1302580787790745600",
    "name": "Clarification on point deletion",
    "messages": "Hi everyone, \n\nWhen deleting a point, does it's vectors also get deleted? I'm using this API: \n\n```POST /collections/{collection_name}/points/delete```\n\nThis issue seems confusing in clarifying the above: https://github.com/qdrant/qdrant/issues/2550\n\nPS: Would you recommend a certain approach if trying to delete about 100 million points in a large production collection?"
  },
  {
    "threadId": "1302425234963562496",
    "name": "Issue with Hybrid Search and Langchain",
    "messages": "I followed this Qdrant hybrid search article (https://qdrant.tech/documentation/tutorials/hybrid-search-fastembed/#) to index some data with dense and sparse embeddings. I then wanted to create a RAG chatbot in chainlit using langchain over this data.\n\nAre HuggingFaceEmbeddings the correct way to include the dense and sparse vectors with the Qdrant langchain vector store?\n\nI am getting a 'list' object has no attribute 'indices' error with the sparse embeddings in the similarly search with score method. I have the code and stack trace details in the following gist.\n\nhttps://gist.github.com/hodgesz/a671fca85a1fabeb6dc543b8db329ebc\n\nAny ideas?"
  },
  {
    "threadId": "1302306239468011530",
    "name": "Index structuring efficiency question",
    "messages": "Let's say we want to index a piece of the payload that is only present in a subset (e.g. 20%) of the total documents. Which method is most efficient computationally for index building/retrieval in Qdrant:\n\n1. ensure that 20% of the documents have this metadata field, and the other 80% *do not* have this field\n2. ensure that 20% of the documents have this metadata field, and the other 80% have the field, but with an empty object (empty list/empty dict)\n3. ensure that 20% of the documents have this metadata field, and the other 80% have the field, but with an null value.\n\nIs there any difference? \n\nCheers,\n\nRob"
  },
  {
    "threadId": "1302205224198602763",
    "name": "Move collection to another cluster (min downtime)",
    "messages": "Hey guys, could you tell me the best practices for moving a collection to another cluster (region)? \nI currently have 120 million vectors and 26 million points with 106 segments_count.\nI need to move my collection to another cluster with minimal downtime. Could you give me advice on how to do this in the best way? Thank you in advance!"
  },
  {
    "threadId": "1302184738223816734",
    "name": "ApacheAirflow with Qdrant",
    "messages": "Hello everyone, i am making a project with apache airflow, qdrant and astro. But i am going to do translate function from English to Vietnamese and I don't know how to do that. Please help me! Thanks!"
  },
  {
    "threadId": "1301990618637275217",
    "name": "Unable to filter search between a date range.",
    "messages": "Hello, My name is Deva. I'm trying to build a project using qdrant.\n\nI ran a script to upload monthly data but at the end, my laptop died and I couldn't upload the entire month data for Sepetember 2024. I am trying to filter and check how many records of september are present and then delete all of them so I could start fresh with the month of september.\n\nWhile I was able to filter out using a specific timestamp, using the following query.\n\n```JSON\nPOST collections/alpaca_news/points/scroll\n{\n  \"filter\": {\n    \"must\": [\n      {\n        \"key\": \"date\",\n        \"match\": {\n          \"text\": \"2024-08-01T00:52:42+00:00\"\n        }\n      }\n    ]\n  }\n}\n```\n\nBut I'm not able to figure out a date range, this is my curent one and it fails\n\n```JSON\nPOST collections/alpaca_news/points/scroll\n\n{\n  \"filter\": {\n    \"must\": [\n      {\n        \"key\": \"date\",\n        \"range\": {\n          \"gte\": \"2024-09-01T00:00:00+00:00\",\n          \"lt\": \"2024-10-01T00:00:00+00:00\"\n        }\n      }\n    ]\n  }\n}\n\nor\n\nPOST collections/alpaca_news/points/scroll\n{\n  \"filter\": {\n    \"must\": [\n      {\n        \"key\": \"date\",\n        \"match\": {\n          \"text\": \"2024-09\"\n        }\n      }\n    ]\n  }\n}\n```\n\nCan you help me with the same, I would like to first check how many records are present from the mont of sepetember and then delete all those records.\n\nThanks,\nDeva."
  },
  {
    "threadId": "1301748095247450152",
    "name": "high memory usage",
    "messages": "I am currently upgrading the version from 1.10.0 to 1.12.1 and inserting new vectors.\n- 768-dimensional dense vectors\n- Sparse vectors\n- The index threshold for dense vectors is set to 0, and the index for payload, vectors, and sparse vectors are set to on_disk=true.\n\nIтАЩve inserted approximately 30 million entries. Previously, memory usage was pretty low with the same settings, but now when I check with kubectl top, I see that itтАЩs using around 90% of memory.\n\nIs this just a cache? Currently, thereтАЩs no OOM, but is there a risk that OOM might occur?\n\nI thought it might be garbage, so I restarted the k8s pod, but the memory usage increased even further.\n\nAny cache optimization has been placed in newer version?"
  },
  {
    "threadId": "1301600528400126043",
    "name": "is a return static in similar vector search?",
    "messages": "Hi guys, I recently migrated VDB from another service to Qdrant. I found the results of a similar vector search were not quite stable in Qdrant. I got slightly different results with the if  vector. I am not sure it's bc the nature of the HNSW or it's to do with my parameters? Could anybody tell me what's going on? Many thanks!"
  },
  {
    "threadId": "1301507542182137937",
    "name": "Issue with Deleting Points Using /points/delete Endpoint",
    "messages": "Hello guys!\n\nI have approximately 300k records in qdrant and here is my multi-vector collection config:\n\n```\n(\n    [params] => Array\n        (\n            [vectors] => Array\n                (\n                    [image] => Array\n                        (\n                            [size] => 512\n                            [distance] => Cosine\n                        )\n\n                    [text] => Array\n                        (\n                            [size] => 1536\n                            [distance] => Cosine\n                        )\n\n                )\n\n            [shard_number] => 1\n            [replication_factor] => 1\n            [write_consistency_factor] => 1\n            [on_disk_payload] => 1\n        )\n\n    [hnsw_config] => Array\n        (\n            [m] => 16\n            [ef_construct] => 100\n            [full_scan_threshold] => 10000\n            [max_indexing_threads] => 0\n            [on_disk] => \n        )\n\n    [optimizer_config] => Array\n        (\n            [deleted_threshold] => 0.2\n            [vacuum_min_vector_number] => 1000\n            [default_segment_number] => 0\n            [max_segment_size] => \n            [memmap_threshold] => \n            [indexing_threshold] => 20000\n            [flush_interval_sec] => 5\n            [max_optimization_threads] => \n        )\n\n    [wal_config] => Array\n        (\n            [wal_capacity_mb] => 32\n            [wal_segments_ahead] => 0\n        )\n\n    [quantization_config] => \n)\n```\n\nI'm attempting to delete around 100,000 points using the /collections/{collection_name}/points/delete endpoint. The deletion is based on a filter applied to an indexed payload key (batch_id). Here is the body of my POST request:\n\n```\n{\n  \"filter\": {\n    \"must\": [\n      {\n        \"key\": \"batch_id\",\n        \"match\": {\n          \"value\": 2\n        }\n      }\n    ]\n  }\n}\n```"
  },
  {
    "threadId": "1301474997843529779",
    "name": "Looking for advise on how to group dataset of social media posts into series.",
    "messages": "Hey,\n\nI am working on a project where I want to create a dataset of social media posts with their transcripts and various custom categories (e.g., format type, tone, CTA, etc.).\n\nBased on this, I plan to organize them into different series of post types.\n\nFor example, a brand might have four types of posts: event updates, feature updates, community posts, and AI news.\n\nI was thinking about doing this with an LLM, but I was also wondering if this is a good use case for Qdrant?\n\nThanks in advance!"
  },
  {
    "threadId": "1301474373919707188",
    "name": "Facet┬атАУ does approximate facet counting affect value completeness?",
    "messages": "Hi, I have a question regarding Facet counts (https://qdrant.tech/documentation/concepts/payload/#facet-counts)\n\nIn Qdrant's facet count feature with default approximate counting:\n(a) Are we guaranteed to see *all* existing values in the results (even if their count numbers are approximate)?, or\n(b) Is it possible that some values that exist in the data might be completely missing from the results due to the approximation?\n\nThank you for clarification"
  },
  {
    "threadId": "1301104067107491912",
    "name": "POINT ID",
    "messages": "Can I avoid creating a vector id? In some other posts it seems possible but when I create pointstructs id is mandatory"
  },
  {
    "threadId": "1301103214736969759",
    "name": "Need Help with implementing Autoscaling with AKS",
    "messages": "I have successfully used the official HELM chart to deploy qdrant to AKS. However, I am struggling to figure out how to implement autoscaling with the statefulset. I had attempted to use HPA, but it seems to only monitor the pod's CPU and/or memory usage which doesn't really seem to \"matter\" as much when it comes to storage spaces. Is there any way I can set it up so that it will scale up when the PV is almost filled up? I apologize for this seemingly trivial question as I am quite new to the whole LLM tech...\n\nCurrent Setup:\n3 Services including the public-facing loadbalancer\nStatefulset: Currently I have set the request for storage to 10Gi within volumeClaimTemplate\n\nThank you in advance!"
  },
  {
    "threadId": "1300956928846856234",
    "name": "Vector Search with Multiple Embeddings Using .query_points_groups",
    "messages": "How can I perform a search using .query_points_groups to obtain relevant results based on multiple embedding vectors at the same time?"
  },
  {
    "threadId": "1300932035208155136",
    "name": "Snapshot recovery of index with custom sharding succeeded but errors on read",
    "messages": "Hello, I'm testing snapshot recovery of collection with custom sharding and getting `ERROR qdrant::actix::helpers: Error processing request: Service internal error: 0 of 0 read operations failed` error when trying to get collection information from `/collections/{collectionName}`\nCollection set up:\nCollection is created with `sharding_method: custom`  and multiple shards created separately through `/collections/{collectionName}/shards` api. \n\nAs result of collection recovery the collection is created without any shards, therefore I've created shards separately through `/collections/{collectionName}/shards` api and then applied shards recovery. Shards recovery succeeded and shows that shards have number of points \n\nThe issue: Once I'm trying to get collection information through `/collections/{recoveredCollectionName}` I'm getting `ERROR qdrant::actix::helpers: Error processing request: Service internal error: 0 of 0 read operations failed`. I've tried to enable debug logging but did not get any data beyond error message I mentioned above"
  },
  {
    "threadId": "1300890324394180680",
    "name": "Where do I report errors with the online examples and documentation?",
    "messages": "As I work with Qdrant, I see errors with the online examples and documentation.  Would you like me to share them with you?  If so, please elaborate.\n\nExample:\nThe integration example at : https://qdrant.tech/documentation/embeddings/ollama/  is missing the step to create a qdrant collection prior to calling the .upsert() method.  \n\nPrior to the line: qdrant_client.upsert()\nAdd:\n    from qdrant_client.models import Distance, VectorParams\n    client.create_collection(\n        collection_name=\"demo_collection\",\n        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),       \n    )\n\nNote also that the 'size=' argument for VectorParams needs elaboration.  It took much research and trial and error until I figured that out."
  },
  {
    "threadId": "1300863281874862150",
    "name": "Define Sharding Value",
    "messages": "Hey guys! I would like to know how you pick the optimal sharding value. \nDo we have any best practices? Correct me if I'm wrong, but for optimal performance (based on an old message):\n1. Number of shards = number of nodes\n2. Increase replicas as much as can fit in memory\n3. Verify on each node that no optimization is pending\nThank you in advance!"
  },
  {
    "threadId": "1300852763793821726",
    "name": "How to see all the embeddings inside qdrant_client?",
    "messages": "I have created persistent qdrant client and collection using this:\n```from llama_index.core import VectorStoreIndex, StorageContext\nfrom llama_index.core.storage.docstore import SimpleDocumentStore\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\nfrom qdrant_client import QdrantClient\n# import nest_asyncio\n\n# nest_asyncio.apply()\n\ndocstore = SimpleDocumentStore()\ndocstore.add_documents(nodes)\n\nqdrant_client = QdrantClient(\n    path = \".\"\n)\n\nvector_store = QdrantVectorStore(client=qdrant_client, collection_name=\"temp\")\n\nstorage_context = StorageContext.from_defaults(\n    docstore=docstore, vector_store=vector_store\n)\n\nindex = VectorStoreIndex(nodes=nodes, storage_context=storage_context)``` \nNow, I want to see dense embeddings generated in this process but when I run this,\n```response = qdrant_client.scroll(collection_name=\"legal_v1\")\nresponse```\nI get `vector=None` under `payload` section. Why? I can query through this index fyi."
  },
  {
    "threadId": "1300814366014308434",
    "name": "Partial update of nested fields.",
    "messages": "Hello! I'm working on a new Qdrant collection which will have the following structure in the points:\n\ncase_id (unique identifier)\ndescription (full text)\ndescription_embedding\nparty_id\noutcome\nletters (list of letters)\n    - id\n    - html_body\n        - case_id\n\nWe are using Estuary for CDC that streams the changes from two tables (cases and letters). We want to have these two joined in Qdrant.\n\nI'm currently working on our own connector to handle the upsert/delete/update logic as it receives updates from Estuary via webhook.\n\nI have most of the logic in place but I'm unsure of how I can make update to only one letter? I understand the API usage of updating with \"set_payload\", but this would currently overwrite existing letters for a given point, correct?\n\nLets say I have one point with 3 letters. One of the letter's html_body is updated and must be updated in Qdrant. How can I make an api call that only updates that letter's (based of the id) html_body?\n\nI have seen in the documents of \"set_payload\" that it is possible to update nested structures, but the example is not super clear to me and I wonder if someone can confirm if it can work for my case? \n\nE.g.\n\n{\n    \"case_id\" : \"1\",\n    \"description\": \"Some text\",\n    \"description_embedding\": [...],\n    \"party_id\" : \"1\",\n    \"outcome\" : \"Win\"\n    \"letters\" : [\n         {\n            \"id\" : \"1\",\n             \"html_body\": \"Some body 1\",\n                 \"case_id\" : \"1\"\n         },\n         {\n            \"id\": \"2\",\n            \"html_body\": \"Some body 2\" (only update this html_body, based on matching with id)\n            \"case_id\": \"1\"\n         },\n        ]\n}"
  },
  {
    "threadId": "1300794686222372896",
    "name": "Migration from float32 to float16",
    "messages": "I would like to migrate the existing collection to float16 vector. Is the recreation  the only option or could I just modify current collection or ~~drop current and load from the snapshot (float32) to empty (float16)~~(doesn't work)?"
  },
  {
    "threadId": "1300793074166923316",
    "name": "How to increase segments count",
    "messages": "Hi everyone!\nI want to update collection parameters and increase segments count. I send PATCH request to /collections/books with body:\n```\n{\n    \"optimizers_config\": {\n        \"default_segment_number\": 32,\n        \"max_segment_size\": 2147483647,\n        \"max_optimization_threads\": null\n    }\n}\n```\nand get success response:\n```\n{\n    \"result\": true,\n    \"status\": \"ok\",\n    \"time\": 0.005131243\n}\n```\nbut segments count is still 9 and collection status is green. I tried different `max_segment_size` from small to big, but nothing trigger index rebuilding. How to start it?\n\nSetup:\n- docker v1.11.5 (recently upgraded from v1.9.7)\n- optimizer config:\n```\n{\n  \"deleted_threshold\": 0.2,\n  \"vacuum_min_vector_number\": 1000,\n  \"default_segment_number\": 32,\n  \"max_segment_size\": 2147483647,\n  \"memmap_threshold\": 20000,\n  \"indexing_threshold\": 20000,\n  \"flush_interval_sec\": 5,\n  \"max_optimization_threads\": 16\n}\n```\n- database config:\n```\n{\n    \"result\": {\n        \"status\": \"green\",\n        \"optimizer_status\": \"ok\",\n        \"indexed_vectors_count\": 332598669,\n        \"points_count\": 332598669,\n        \"segments_count\": 9,\n        ...\n```"
  },
  {
    "threadId": "1300759980357718036",
    "name": "How to update sparse vector (BM25) ?",
    "messages": "hi. I would ask regarding sparse vectors. While we're talking about BM25, it's based on frequency of word, right?\n\nWith that say, \n1. when we inserting new data the frequency will always change, is it correct?\n2. if the frequency change, means that the embedding will be also affected, i mean the embedding that's already stored in qdrant would changed too, is it correct?\n3. if so how qdrant handle this? i mean does this process is run automatically in the background or how?\n\nTry to find the answer in qdrant documentation but couldn't find it. Thank you so much for your answer"
  },
  {
    "threadId": "1300669248879722547",
    "name": "Docker compose takes a long time to load collections on restart",
    "messages": "Have around 8 million embeddings takes 15 minutes to load the collection on restart. Using NVME ssd and on disk vectors config"
  },
  {
    "threadId": "1300456459162746910",
    "name": "Empty response from grpc search query",
    "messages": "Hello everyone,\n\nI recently find out about an uncommon error using qdrant, sometimes the search query does not return any vector (and no errors either) ! Here are some context:\n- We are using qdrant on a single machine (16vcpu and 120Go of ram)\n- We are working on a single collection (today it is 40Millions points)\n- The payload of our points look like this: (a lot of points share the same content_id)\n```\n{\n    array_ids: string[];\n    content_id: string;\n    type: string;\n}\n```\n- The configuration of our collection: I will put it on the first message because of free discord limitation\n\nWe do a first grpc search request to get a specific content_id, then we do a second grpc batch request with a filter on content_id so we retrieve only vectors on a single content. Lately we found traces of second calls that returned no points but with a first call. I will describe the interaction:\n- A user created the 24/10 x vectors with the content_id: 13\n- The first query returned a point with the content_id: 13, so we went on the second query with filter on content_id: 13 and the searchBatch array was empty\n- Last night at 21H44 and for more thant 10mins each time he queried he only got empty response (empty array) on the batchPart but still got points on the first query\n- Today I logged on and tried the exact same query (re-used the vector values of the failed one) on qdrant dashboard and got all the response as normal behavior\n\nWe are sure that absolutly nothing happened on qdrant (no update on the points, no change of payload, nothing) so we are completely at a loss on this error that seems to appear randomly.\n\nI know that the database was never optimised, so I wanted to check if it may be because the database is under a lot of stress ? \n(not enough shards, a wrong HNSW config, optimization threads low, implement cluster mode)"
  },
  {
    "threadId": "1300431812266557561",
    "name": "batch hybrid search",
    "messages": "Is there any way to make a batch hybrid search using dense+sparse vectors?"
  },
  {
    "threadId": "1300411811677339739",
    "name": "Collection Storage",
    "messages": "How do I check my collection storage? How do I know how much space I have left?"
  },
  {
    "threadId": "1300404421921996822",
    "name": "When Index rebuilding gets triggered?",
    "messages": "Indexing threshold is set to 20000 when a collection is first created. \nI am adding points regularly into the collection. let's say once everyday.\n\nMy questions is,\n\nBefore adding point If indexing is stopped by setting threshold 0, then insert/upsert the points and then again resume indexing with `PATCH /collections/{collection_name}` \n```json\n{\n    \"optimizers_config\": {\n        \"indexing_threshold\": 20000\n    }\n}\n```\nDoes this trigger a Index rebuilding? \n\nTo be precise, does patching the collection with `indexing_threshold` triggers a index rebuilding?"
  },
  {
    "threadId": "1300348172207652894",
    "name": "Multinode Connection issue on ec2",
    "messages": "Below are the two dockerfile which are running on 2 ec2 machines.\n\nDockerfile 1 :\n\n# Use the official Qdrant image\nFROM qdrant/qdrant:latest\n \n# Set the URI and bootstrap for the second node\nENV QDRANT__CLUSTER__ENABLED=true\nENV QDRANT_URI=http://<ip1>:6335\n \n# Expose API and internal cluster communication ports\nEXPOSE 6333 6335\n \n# Command to run Qdrant with the bootstrap configuration\nCMD [\"./qdrant\", \"--uri\", \"http://<ip1>:6335\"]\n\n\nDockerfile 2 : \n\n# Dockerfile node2\n \n# Use the official Qdrant image\nFROM qdrant/qdrant:latest\n \n# Set the URI and bootstrap for the second node\nENV QDRANT__CLUSTER__ENABLED=true\nENV QDRANT_URI=http://<ip2>:6335\nENV QDRANT_BOOTSTRAP=http://<ip1>:6335\n \n# Expose API and internal cluster communication ports\nEXPOSE 6333 6335\n\n# Command to run Qdrant with the bootstrap configuration\nCMD [\"./qdrant\", \"--bootstrap\", \"http://<ip1>:6335\", \"--uri\", \"http://<ip2>:6335\"]"
  },
  {
    "threadId": "1300158529772064778",
    "name": "Max number of non-zero values in sparse vector",
    "messages": "Hi, is there a maximum number of non-zero dimensions for the sparse vector type? pgvector has a limit of 1,000 and we need a better solution but couldn't see Qdrant's documentation covering this."
  },
  {
    "threadId": "1300017221732536391",
    "name": "Creating a collection of sparse embeddings only",
    "messages": "when trying to create a collection of sparse embeddings as per the websites documentation ( https://qdrant.tech/documentation/concepts/vectors/ ) \n\n```\nfrom qdrant_client import QdrantClient, models\n\nclient = QdrantClient(url=\"http://localhost:6333\")\n\nclient.create_collection(\n    collection_name=\"{collection_name}\",\n    sparse_vectors_config={\n        \"text\": models.SparseVectorParams(),\n    },\n)\n```\n\ni am getting an error of TypeError: QdrantClient.create_collection() missing 1 required positional argument: 'vectors_config'\nI am using the hosted version of qdrant if that should make a difference also using fastembed to generate the sparse embeddings"
  },
  {
    "threadId": "1299933492049870888",
    "name": "scalar quantization vs binary quantization",
    "messages": "We have around 2M vectors (384 in dimension) in our QDrant cluster. Original, we did a scalar quantization for these vectors. We also created a separate collection and  applied binary quantization to the same set of vectors. \nFor each collection, we did a bench mark and simply just called the recommendation API to retrieve recommendations. We measured the rps. And we found that for binary quantization we reached 550 rps (50% latency 59 ms) while for scalar quantization we reached 1800 rps (50% latency 7 ms). Why did scalar quantization outperformed binary quantization by this much? One caveat is that we do have filter condition set but we created the same payload index for those filter in both collection. We did not set the 'oversampling' and 'rescore' parameters when doing search with binary quantization so  they are left at default values."
  },
  {
    "threadId": "1299723342789738577",
    "name": "Cost-efficient cluster scaling recommendations?",
    "messages": "I am currently pushing 60M+ documents in my 4-nodes Qdrant Cloud cluster.\nSo far i have inserted 3M documents, but it would exceed the storage limit soon.\nI think I should do horizontal/vertical scaling, but I want to know what would be the best cost-efficient approaches to handle this upcoming problem.\n\nThis is my current cluster state:\n- 4 nodes\n- Storage usage: 7GB of 32GB (28GB of 128GB on 4 nodes) (3M docs currently)\n- On-demand cost: approx. $230/mo\n\nCluster seems to be out of storage after 9M documents added.\n\n- Quantization: Binary\n- HNSW index is always on RAM.\n\nI am waiting for your recommendations!"
  },
  {
    "threadId": "1299388503691366484",
    "name": "Limiting Result Types via Payload Filtering? (Max of n per payload/metadata type)",
    "messages": "I'm working on a product recommender system. I am hoping to limit results such that in a returned sample of 10, there are a max of 2 products per brand. Is this possible to achieve in the initial query via payload filtering? Or some other approach? Right now the only way I can think to achieve this is returning a much larger sample, sya 30-50, and limiting results after that via an additional script. But this approach is much slower than a payload filtered approach would be.\n\nAny ideas?"
  },
  {
    "threadId": "1299385930213490718",
    "name": "Duplicate points",
    "messages": "Hi, how do you solve duplicate points storing? The points have the same vectors but different payload. So imo storing the same vectors would just bias results towards these points, however, i need to keep the distinguished payloads. What are recommended strategies to that? I have read this: https://qdrant.tech/documentation/concepts/search/#lookup-in-groups but that brings some complexity when doing updates and deletes, plus for each search I would need to use Group API - so even points which are unique would fall to some group. I was also thinking to store the unique payloads into some field which would be list of objects-the payloads, but that is also added complexity for updates/deletes.\nAny suggestion is welcomed.\nThank you."
  },
  {
    "threadId": "1299175703413129256",
    "name": "Cloud API 401 Unauthorized",
    "messages": "I am attempting to use the cloud api to create an api key for my cluster (https://cloud.qdrant.io/pa/v1/docs#/Authentication/create_api_key).\n\nI'm sending a post request to https://cloud.qdrant.io/pa/v1/accounts/{account_id}/auth/api-keys where the account id is the uuid of my account (the owner/admin account), and I am including my cloud api key in the Authorization header (I have tried both just raw `{api_key}` and `Bearer {api_key}`), but I get 401 unauthorized in the response .\n\nThis api does seem slightly sketchy -- is it well maintained? If it is, then what is the correct way to authenticate with it?"
  },
  {
    "threadId": "1299169105101721702",
    "name": "updating collection vectors / altering collections / migration ?",
    "messages": "hey ! ive been working on a project where we frequently add new vectors, which necessitates creating a new collection. i was just wondering if theres any way to add vectors to a collection after its created ? if not, is there any way to create a new collection and easily readd all the points from my previous collection? i guess i could do it via snapshots? thanks !"
  },
  {
    "threadId": "1299099202068156539",
    "name": "HTTP instead of gRPC Go client",
    "messages": "We have QDrant cluster running inside K8s. Is it possible to use Golang client to use only REST instead of gRPC? We find it tricky to do load balancing of gRPC in K8s. Is the trick is to pass the port of 6333 instead of 6334?"
  },
  {
    "threadId": "1299051248447656091",
    "name": "FP16 Dense Vectors",
    "messages": "Is it possible to convert an existing collection from FP32 to FP16? , Also how much of a throughput difference might I expect to see from disabling indexing during upload?"
  },
  {
    "threadId": "1299026140572160010",
    "name": "Shard number, Replication Factor, and Search",
    "messages": "In the QDrant doc (https://qdrant.tech/documentation/guides/distributed_deployment/#sharding), it stated \"when you send a search request to one Qdrant node, it automatically queries all other nodes to obtain the full search result\". We have a 3-node cluster and the replication factor is 3. So it means that any node in the cluster should have complete shards of the collection. If that is the case then a search request and respond is handled by only one node without spanning to the entire cluster. Is this correct?"
  },
  {
    "threadId": "1298753376669925387",
    "name": "Is there a way to get hybrid search working within a docker, running on a mac?",
    "messages": "I am trying to get hybrid search working, but seems to fail on a mac running within docker."
  },
  {
    "threadId": "1298682813507178586",
    "name": "payload list filtering",
    "messages": "Hi! I have the following qdrant with the following payload fields: \n\nchunk_summary: xxxxx\nchunk_summary_id: \n[\n0:\"0a39947f3a35a9caf89075a71333eedf78399aac5f26389e6\"\n1:\"0a39947f3a35a9caf89075a71333eedf78399aac5f26389e7\"\n]\ndoc_url:  xxx\ndocument_id: xxx\nfile_path: xxx\n\ni would like to write a scrypt in python that checks which point in this collection, has chunk_id \"0a39947f3a35a9caf89075a71333eedf78399aac5f26389e6\". However, given that chunk_id is a list in qdrant payload, ive tried all the filtering functions in the documentation and it doesnt work. I want to have one single chunk_summary_id and check if that chunk_summary_id is inside the list of any of the points... \n\nIs this possible? is there a different way? maybe if it was a dictioanry itd be easier? \n\nPlease, any help is very welcome ЁЯШж \n\nIve tried this and returns None:\n\nchunk_id = \"0a39947f3a35a9caf89075a71333eedf78399aac5f26389e6\"\n\nqd_client.scroll(\n    collection_name=intermediate_summaries_collection_name,\n    scroll_filter=models.Filter(\n        should=[\n            models.FieldCondition(\n                key=\"chunk_id\",\n                match=models.MatchValue(value=chunk_id),\n            ),\n        ],\n    ),\n)"
  },
  {
    "threadId": "1298606515208458240",
    "name": "Too many opened files during shards transfer",
    "messages": "Hi, \n\nI have three node cluster and after one node fail I create new node. I start trasfering shards to the new node.  I have three shards each with 3M points . I get this error:\n\n```\nFailed to acknowledge WAL version: Can't truncate WAL: failed to write first-index file: Io(Os { code: 24, kind: Uncategorized, message: \"Too many open files\" }\n```\n\nAfter that shards have status Dead.\n\nI increase the limit from 1M, 2M and even on 8M opened files.\n```\nulimit -S -n\n8097152\n```\n\nDo you have any idea what I should change?\n\nThe result is the same. \nThank you"
  },
  {
    "threadId": "1298553479530283041",
    "name": "Need help with Hybrid Search (Dense + Sparse) implementation",
    "messages": "Hi, I am trying to implement hybrid search for information retrieval. I am using Dense vector (Openai) and Sparse vectore (Fastembed), can anyone provide a detailed blog/page/tutorial, all the available doesn't teach all the parts.\n\nAny help will be appreciated."
  },
  {
    "threadId": "1298317652841861222",
    "name": "RAG for forum",
    "messages": "Hi everyone,\n\nI'm currently in the process of indexing an internal forum to build a proper chatbot that can access the knowledge of the forum and I'm wondering about the best way to store the vectors for retrieval. Has anyone done something similar? \n\nI have every post as a JSON and was planning to insert them with dense and sparse embeddings. \n\nHere is the current structure:\n\n`    {\n        \"thread_id\": 100001,\n        \"thread_title\": \"RandomThreadTitle\",\n        \"subforum\": \"tips\",\n        \"post_id\": \"557089\",\n        \"author\": \"randomauthor\",\n        \"date\": \"2024/05/10\",\n        \"content\": \"Content of the post.\",\n        \"reactions\": \"\",\n        \"is_reply\": false,\n        \"reply_to\": null,\n        \"page\": 1\n    }`\n\nSince some of the posts are quite large, I need to chunk them and was planning to include the thread title to the payload to keep the chunks together as standard recursive chunking might not work as well for this? How would you approach this?\n\nThanks everyone ЁЯЩВ"
  },
  {
    "threadId": "1298287849866793073",
    "name": "Single Collection clarifications",
    "messages": "I see that QDrant recommends one single collection for eveything via payload filtering. Right now we have multiple collections to organize the content per subjects. Why is the recommendation? Does not affect the quality of the results once we may get more irrelevant answers if we put everything in the same collection?"
  },
  {
    "threadId": "1298209179991015425",
    "name": "qdrant with databricks",
    "messages": "hey! \ni've read the guide about connecting databricks and qdrant but i have a couple of questions \ndoes qdrant spark data sink supports streaming writes ?"
  },
  {
    "threadId": "1298157224740393010",
    "name": "Sorting by payload",
    "messages": "Hello, I use qdrant for image-search,  when  searching by vector and I want sort by  a payload (such as image_grade)\nIs there any way to achieve this?"
  },
  {
    "threadId": "1298010139634176080",
    "name": "Qdrant Connection Issue Configuration Coolify/Self-Hosted",
    "messages": "Hey guys. I'm encountering an issue while trying to connect to my self-hosted Qdrant instance using the Python SDK. While the initial connection seems successful, I run into problems when attempting to create collections. Here's what I'm experiencing:\n## The Problem\nWhen I try to create a collection like this:\n```python\nqdrant_client.create_collection(\n    collection_name=\"test_collection_2\",\n    vectors_config=VectorParams(size=4, distance=Distance.DOT),\n)\n```\nI receive the following error `ResponseHandlingException: Connection refused`\n## My Connection Configuration\n```python\nqdrant_client = QdrantClient(\n    host=\"XXXX\",  # Your server's IP or hostname\n    port=6333,  # The port Qdrant is running on (default is 6333)\n    grpc_port=6334,  # The gRPC port Qdrant is running on (default is 6334)\n    # prefix=\"\",  # URL prefix if any\n    timeout=10.0,  # Timeout for requests\n    prefer_grpc=False, # Use gRPC instead of HTTP\n    api_key=\"<YOUR_API_KEY>\",  # Your API key\n    https=False,  # Use HTTPS instead of HTTP\n)\n```\nThis error probably indicates that the client can reach the server, but the server is not accepting the connection on the specified port.\n## Possible TLS/HTTPS Issue\nI suspect this might be related to **TLS/HTTPS** configuration because I receive this warning after connecting: `UserWarning: Api key is used with an insecure connection.`\nI have a local instance running, and I don't encounter any connection issues with it.\n## Troubleshooting Steps\nI've already tried the following:\n- тЬЕ Verified that my instance is running and healthy\n- тЬЕ Checked the Firewall settings\n- тЬЕ Reviewed the Docker compose configuration\n## Questions\n1. Is the lack of **TLS/HTTPS** configuration likely to be the root cause of this issue?\n2. If not, could there be a problem with how I'm setting up the connection?\n3. Are there any additional troubleshooting steps you'd recommend?"
  },
  {
    "threadId": "1298008792067342356",
    "name": "i am facing an error as:",
    "messages": "<_InactiveRpcError of RPC that terminated with:\n        status = StatusCode.UNIMPLEMENTED\n        details = \"\"\n        debug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"\", grpc_status:12, created_time:\"2024-10-21T19:33:32.3761695+00:00\"}\""
  },
  {
    "threadId": "1297919541564936304",
    "name": "Was able to fix this bug locally but any updates on it?",
    "messages": "https://github.com/qdrant/qdrant-web-ui/issues/94\n\nI tested locally that changing package.json:\n\"build-qdrant\": \"vite build --base '/dashboard'\",\nto\n\"build-qdrant\": \"vite build --base '/'\",\n\nsomehow made it work with subpaths.. if it was that simple I'm sure it would have been fixed, but any chance we can get a proper fix for running the UI in support of subpaths? e.g\nhttps://company.com/8b449c1e-bdfd-4d87-b6f5-70b31bd03ae5/qdrant-dev-master-qdrant/dashboard"
  },
  {
    "threadId": "1297869924160765982",
    "name": "New to qdrant - replicas for fault tollerance",
    "messages": "Hello, as a company we want to utilize qdrant as vector store. We are concerned about data replication in qdrant. Is going for cloud the only option? Does qdrant provide some configuration script or expose API to save data from our on-premise VM to another machine periodically?\n\nI found this in the possibility offered by the service but it is not stated if this two types of support are offered also on premise\n> Qdrant offers comprehensive horizontal scaling support through two key mechanisms:\n> \n>     Size expansion via sharding and throughput enhancement via replication\n>     Zero-downtime rolling updates and seamless dynamic scaling of the collections"
  },
  {
    "threadId": "1297328958123151370",
    "name": "Help with splade vectors/hybrid search",
    "messages": "Hi guys, i was reading about splade vectors and i have a question, itтАЩs possible to increase the weight of a token?\n\nSuppose i have: тАЬHP SLIM 14 HDтАЭ\n\nCan i give more relevance to тАЬHPтАЭ by increasing its weight? And how do i decide how much to increase it?"
  },
  {
    "threadId": "1296936795052707871",
    "name": "Fastembed Image embeddings without reading from disk?",
    "messages": "Hello o/\nI wanted to try and use the fastembed python library in my application to generate embeddings of images, but as far as I can see the ImageEmbedding class supports generating embeddings only from files that reside on disk.\nIs it possible to use in-memory image data instead?\nThanks!"
  },
  {
    "threadId": "1296878231403757800",
    "name": "Updating batch points and waiting for the result",
    "messages": "Hello guys, long term user here was there since the begining after a migration from pinecone 1 year ago ! We went a long way with qdrant and using most of its capacity. Today I am strugling with some optimizations:\nThe solution we are implementing avoid duplication of vectors by setting a payload of ids to each vector, since this day we had hundreds to couple thousands documents each representing 10 to 30 vectors. As the number of client keep growing, I am facing today a batch update of 5M of vectors in one time ! batching it document by document would take 9H, I reduced this time by batching 1000 per 1000 but the time is still very long. This is  only a part of the whole process and I wanted to launch it on the early stage so that other unrelated things can be treated while Qdrant finish its operation. We got the very nice wait: false and I gathered all the operationIds from those batch and I wanted at the end of the process to circle on them every 5s to wait for the final completion. But nower in the api in rest or grpc can I found an endpoint that takes a operation id and give back the current status.. Am I missing something obvious ? Am I condemned to the loooooong wait: true to be sure that my payload is updated ?"
  },
  {
    "threadId": "1296805275289915464",
    "name": "I am facing an error, While using Search method in the qdrant.",
    "messages": "Please someone helpme."
  },
  {
    "threadId": "1296789189215387719",
    "name": "Getting all points through scrolling with some filtering is really slow",
    "messages": "Hey guys,\nI am fairly new to rag databases. We try to put all our files into a combined database. The results are 1.1m embeddings.\nWe already filtered with having a Llama 3.1 look at all the data and giving us a score on how useful the texts are.\n\nAs embedding model I use BAAI/bge-m3 with 1024 dimension.\n\nI started with using a FAISS Database with LangChain as a wrapper. Which was working great. It was  really fast. The generated database file with metadata and texts was around  10gb.\nWe tried to implement filtering by metadata which FAISS did not really support. So we looked for alternatives.\n\nNow I am working with Qdrant on a small scale. everything was fine. But with a big full database (sqlite file around 15gb) the loadtime alone was 10 minutes with the default settings.\nI am currently trying to load all documents. With FAISS, I loaded everything, filtered and paginated the results manually and it took seconds. With client.scroll with the qdrant DB. It takes minutes.\nIs there an easy way to just get all texts with metadata from the vector_store?\n\nThis is how my code currently looks:\n```python\n# Use scroll method to retrieve all documents\n    all_docs = []\n    scroll_id = None\n    total_loaded = 0\n\n    while True:\n        logger.debug(f\"Loading documents with scroll_id={scroll_id}. Documents loaded so far: {total_loaded}\")\n        res = vector_store.client.scroll(\n            collection_name=vector_store.collection_name,\n            scroll_filter=qdrant_filter,\n            limit=100000,\n            with_payload=True,\n            with_vectors=False,\n            offset=scroll_id\n        )\n        points, scroll_id = res\n        if not points:\n            logger.debug(\"No more points returned from scroll.\")\n            break\n        for point in points:\n            doc = Document(page_content=point.payload.get('page_content', ''),\n                           metadata=point.payload.get(\"metadata\", {}))\n            doc.metadata[\"id\"] = point.id\n            all_docs.append(doc)\n        total_loaded += len(points)\n        if scroll_id is None:\n            break\n\n    logger.info(f\"Total documents loaded: {total_loaded}\")\n```\nFilter include but are not limited too:\nallowed_folders, and document type..\n\nI am already looking at options to improve the query resultus with quantization options, but it's a dealbreaker that loading all documents takes 5 minutes with scrolling.\nI must be doing something wrong here right? ЁЯШД\nI tried do search the discord for an answer but only found an unanswered thread."
  },
  {
    "threadId": "1296771846250958862",
    "name": "Unable to build from source with Kaniko",
    "messages": "Trying to build from source using googles kaniko builder and it's getting broken/stuck on this line:\n> COPY --from=xx / /\nIs there an alternative we can change it to to be more specific to resolve the issue?\nSomething like this?\n\nCOPY --from=xx /usr/local/cargo/bin /usr/local/cargo/bin\nCOPY --from=xx /usr/local/rustup/toolchains /usr/local/rustup/toolchains\nCOPY --from=xx /usr/local/rustup/update-hashes /usr/local/rustup/update-hashes"
  },
  {
    "threadId": "1296472709903159316",
    "name": "Is it possible to load llama_index locally and save the index into Qdrant with QdrantVectorStore?",
    "messages": "I want to make sure the embeddings and other metadata are the same after transfer."
  },
  {
    "threadId": "1296408978535743498",
    "name": "Batch search API single wrong request discards whole batch search",
    "messages": "Hi, \n\nI've been happily using Qdrant for quite a while now. I just noticed that we can improve our performance by switching to batch search api together with point ids. However, in my application, there is a chance that an id is requested that is not in Qdrant. This means the whole batch request is rejected and no data is sent back. Is there a possibility to set these to ignore so the existent chunks are queried? Or at least return the results that were captured untill the wrong request?\n\nSteps to reproduce:\n```\n\n# Insert two points into the collection\npoints = [\n    {\"id\": 1123456780001, \"vector\": [0.9]*vector_length, \"payload\": {\"type\": 1, \"doc_id\": 12345678}},\n    {\"id\": 1876543210002, \"vector\": [0.9]*vector_length, \"payload\": {\"type\": 1, \"doc_id\": 87654321}}\n]\n    \nqdrant_handler._client.upsert(collection_name=collection_name, points=points)\n# Test with some chunk IDs that exist and some that don't\nchunk_ids = [1123456780001, 1123456780002, 1123456780003]  # Only 1123456780001 exists\ntarget_doc_ids = [87654321]\nquery_requests = [\n        models.QueryRequest(\n            query=chunk_id, \n            limit=10,\n            score_threshold=0.8, \n            with_payload=True\n        )\n        for chunk_id in chunk_ids\n    ]\n# query with only existent point returns the result\nresults = qdrant_handler._client.query_batch_points(collection_name=collection_name, requests=query_requests[:1])\nprint(results[0].points[0].id)\n# --> 1876543210002\n\n# query with a non existent point returns no results and an error\nresults = qdrant_handler._client.query_batch_points(collection_name=collection_name, requests=query_requests)\nE       qdrant_client.http.exceptions.UnexpectedResponse: Unexpected Response: 404 (Not Found)\nE       Raw response content:\nE       b'{\"status\":{\"error\":\"Not found: No point with id 1123456780002 found\"},\"time\":0.000044833}'```"
  },
  {
    "threadId": "1296259135234379837",
    "name": "Does Qdrant has a documentation RAG for the community?",
    "messages": "Hey, I'm comming from Weaviate. Looking for an alternative. I'm trying to learn from the community and get familiar with the tools available and the devex. Thanks."
  },
  {
    "threadId": "1296183313429692507",
    "name": "Suspend Qdrant",
    "messages": "How do I suspend my cluster (not delete) from the https://cloud.qdrant.io/"
  },
  {
    "threadId": "1296170327084367904",
    "name": "search on filtered points",
    "messages": "If you see in the code for graph layers there is one function search_on_level in that functions first points are filtered using point_scorer.score_points() and then it processes the candidates, right??"
  },
  {
    "threadId": "1296127303818022982",
    "name": "Filter by times",
    "messages": "I have this structure in my database\n```{\n  \"Mo\": [\n    {\n      \"opening\": \"10:00\",\n      \"closing\": \"20:00\"\n    },\n    {\n      \"opening\": \"21:00\",\n      \"closing\": \"23:59\"\n    }\n  ]\n}```\n\nIs there a possibility to search by comparing the saved time with my current time?\nEx:\nIt's monday at 8:00, is closed\nIt's monday at 13:00, is open\nIt's monday at 20:30, is closed"
  },
  {
    "threadId": "1296122827753259149",
    "name": "Qdrant generated UUID on creation returns without \"-\" but on reading the points it gets \"-\"",
    "messages": "Qdrant generated UUID on creation returns without \"-\" but on reading the points it gets \"-\". This fails my testing asserts. Though I am removing the \"-\" to proceed ahead but is there some cleaner solution or am I missing something. Is the str representation for UUID during processing issue?"
  },
  {
    "threadId": "1296087588322676827",
    "name": "How to use qdrant for anomalies detection with vector search for IoT sensors data",
    "messages": "Is it even possible for anomalies detection in qdrant at vector search runtime ? If yes how can we do it ? \n\nWhat are different methods or techniques that need for anomalies detection using vector search ?"
  },
  {
    "threadId": "1296039319848353873",
    "name": "how to upgrade cluster from free tier to paid?",
    "messages": "How can I do it?"
  },
  {
    "threadId": "1296033686797553664",
    "name": "ValueError: Unsupported embedding model but the same value is present as a valid model in the desc!",
    "messages": "```\nValueError: Unsupported embedding model: \"BAAI/bge-small-en-v1.5\". Supported models: {'BAAI/bge-base-en': (768, <Distance.COSINE: 'Cosine'>), 'BAAI/bge-base-en-v1.5': (768, <Distance.COSINE: 'Cosine'>), 'BAAI/bge-large-en-v1.5': (1024, <Distance.COSINE: 'Cosine'>), 'BAAI/bge-small-en': (384, <Distance.COSINE: 'Cosine'>), 'BAAI/bge-small-en-v1.5': (384, <Distance.COSINE: 'Cosine'>), 'BAAI/bge-small-zh-v1.5': (512, <Distance.COSINE: 'Cosine'>), 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2': (384, <Distance.COSINE: 'Cosine'>), 'thenlper/gte-large': (1024, <Distance.COSINE: 'Cosine'>), 'mixedbread-ai/mxbai-embed-large-v1': (1024, <Distance.COSINE: 'Cosine'>), ... snip ... 'Qdrant/clip-ViT-B-32-text': (512, <Distance.COSINE: 'Cosine'>), 'sentence-transformers/all-MiniLM-L6-v2': (384, <Distance.COSINE: 'Cosine'>), 'jinaai/jina-embeddings-v2-base-en': (768, <Distance.COSINE: 'Cosine'>), 'jinaai/jina-embeddings-v2-small-en': (512, <Distance.COSINE: 'Cosine'>), 'jinaai/jina-embeddings-v2-base-de': (768, <Distance.COSINE: 'Cosine'>), 'jinaai/jina-embeddings-v2-base-code': (768, <Distance.COSINE: 'Cosine'>), 'nomic-ai/nomic-embed-text-v1.5': (768, <Distance.COSINE: 'Cosine'>), 'nomic-ai/nomic-embed-text-v1.5-Q': (768, <Distance.COSINE: 'Cosine'>), 'nomic-ai/nomic-embed-text-v1': (768, <Distance.COSINE: 'Cosine'>)}\n```\n `BAAI/bge-small-en-v1.5` is a valid model. \nThe funny thing is I get this error only when running the program within the docker container. \nRunning this on the host machine (my ubuntu laptop) has no issues at all. \nWhat can be the problem when running within the container?"
  },
  {
    "threadId": "1296025507032465439",
    "name": "Reuse PVC  for qdrant",
    "messages": "I am moving my qdrant cluster to new subscription in Azure. I am using AKS to deploy my cluster. I am trying to maintain the same data for new deployment, hence, I have made a PVC from the other subscription and trying to use in the new deployment. However, I don't see that it working as expected since the collections are empty.\n\nSnippet of my helm chart\n\npersistence:\n  existingClaim: \"cross-sub-azurefile-claim\"\n  # Remove or comment out the following lines:\n  accessModes: [\"ReadWriteMany\"]\n  size: 64Gi\n  storageClassName: azurefile-csi-premium\n\nAny help is much appreciated"
  },
  {
    "threadId": "1295995174551556107",
    "name": "Memory overheads for many small payloads",
    "messages": "Hey folks! I work at Discord. I'm evaluating Qdrant for a new recsys use case involving many small points/vectors in memory (due to high retrieval volume). I've noticed in testing that there's a quite high storage overhead for in-memory payload/index storage; when I attempt to store trivially small documents I am seeing 500B+ of RAM overhead per point.\n\nTo demonstrate the issue, I ran a trivial test - details below. I'm willing to make code changes to qdrant, any pointers appreciated!\n\n====\n\nDeplyment configuration:\n```\nQdrant version: v1.11.1\n12x sharding\n3x replication\n9x hosts on GKE - c3d-standard-30 w/ 120GB RAM each\n```\n\nAfter sidecars, OS overheads, and 3x replication factor, I expect to be able to store ~300GB of stuff.\n\nCollection configuration: \n```\n{\n  \"params\": {\n    \"vectors\": {\n      \"size\": 2,\n      \"distance\": \"Cosine\",\n      \"on_disk\": true\n    },\n    \"shard_number\": 12,\n    \"replication_factor\": 3,\n    \"write_consistency_factor\": 1,\n    \"on_disk_payload\": false\n  },\n  \"hnsw_config\": {\n    \"m\": 16,\n    \"ef_construct\": 100,\n    \"full_scan_threshold\": 10000,\n    \"max_indexing_threads\": 0,\n    \"on_disk\": true\n  },\n  \"optimizer_config\": {\n    \"deleted_threshold\": 0.2,\n    \"vacuum_min_vector_number\": 1000,\n    \"default_segment_number\": 0,\n    \"max_segment_size\": null,\n    \"memmap_threshold\": null,\n    \"indexing_threshold\": 0,\n    \"flush_interval_sec\": 5,\n    \"max_optimization_threads\": null\n  },\n  \"wal_config\": {\n    \"wal_capacity_mb\": 32,\n    \"wal_segments_ahead\": 0\n  },\n  \"quantization_config\": null\n}\n```\n\n**Test setup:**\n\nExample point:\n\n```\n{\n  \"id\": 0,\n  \"payload\": {\n    \"c\": \"e\",\n    \"i\": 10909558\n  },\n  \"vector\": [\n    0.08657566,\n    0.9962453\n  ]\n}\n```\n\nI inserted ~488M of these points before the cluster OOMed at ~105G memory used per host; for a ballpark 600B per item.\n\nOnce again, any guidance appreciated! I'm hoping someone has a quick answer or pointer, before I dig in and start reading qdrant code."
  },
  {
    "threadId": "1295826484388167750",
    "name": "Filter by missing named vector",
    "messages": "Hi! I'd like to find records that are missing one of my named vectors. Is there a filter I can use to find these? I cannot find anything on this in the documenation. Im using the python client."
  },
  {
    "threadId": "1295824389279055952",
    "name": "Group_by in FastEmbed and H3 Index",
    "messages": "Hi team!\nCan I use group_by with FastEmbed? I'm working with some H3 indexes and would like to achieve the best results in a specific area, grouped by that index. My goal is to display 3 or 4 results within each hexagon. Additionally, does it support nested objects? I have different resolutions inside H3, each with varying values."
  },
  {
    "threadId": "1295719036449194089",
    "name": "Hey!",
    "messages": "I'm seeing latencies of around ~200ms on a collection that just has 3 points (384 embedding size)\nHowever on looking at cluster's metrics, the latency is just ~3ms \nI'm using a free tier cluster (4gb disk, 1gb ram) hosted in europe-west3 location. \n\nCan someone help me understand why this is happening?"
  },
  {
    "threadId": "1295703557093326889",
    "name": "storage for qdrant",
    "messages": "hey!!\nwe started integrating qdrant on our system but we want to figure out the storage cost and how large will our database in order to estimate costs \nour usecase that we will dump into it alot of data (~6-8 tb) \n\nand are there any tips about handling this amount of data? like offloading it into the disk and using quantization?"
  },
  {
    "threadId": "1295676624280358973",
    "name": "Multi Node Issue",
    "messages": "If I am having 4 nodes while spinning up qdrant, and by any case if one of my nodes goes down then , I am not able to access the collection present.\n\nIt gives an error : \nError: Service internal error: 1 of 1 read operations failed: Service internal error: Tonic status error: status: Unavailable, message: \"Failed to connect to http://qdrant-node2:6335/, error: transport error\", details: [], metadata: MetadataMap { headers: {} }\n\nAm I doing something wrong or it can be handled?"
  },
  {
    "threadId": "1295665296702439517",
    "name": "Read operations failed.",
    "messages": "I'm getting timeout error during read operations from Qdrant. Got the same error a couple of days ago as well. It resolved on its own. \nHere's the collection configuration :\n\n{\n  \"params\": {\n    \"vectors\": {\n      \"size\": 384,\n      \"distance\": \"Cosine\",\n      \"hnsw_config\": {\n        \"m\": 32,\n        \"ef_construct\": 100,\n        \"full_scan_threshold\": 10000,\n        \"on_disk\": true\n      }\n    },\n    \"shard_number\": 2,\n    \"replication_factor\": 1,\n    \"write_consistency_factor\": 1,\n    \"on_disk_payload\": true\n  },\n  \"hnsw_config\": {\n    \"m\": 32,\n    \"ef_construct\": 200,\n    \"full_scan_threshold\": 10000,\n    \"max_indexing_threads\": 0,\n    \"on_disk\": true\n  },\n  \"optimizer_config\": {\n    \"deleted_threshold\": 0.2,\n    \"vacuum_min_vector_number\": 1000,\n    \"default_segment_number\": 0,\n    \"max_segment_size\": null,\n    \"memmap_threshold\": 20000,\n    \"indexing_threshold\": 20000,\n    \"flush_interval_sec\": 5,\n    \"max_optimization_threads\": null\n  },\n  \"wal_config\": {\n    \"wal_capacity_mb\": 32,\n    \"wal_segments_ahead\": 0\n  },\n  \"quantization_config\": {\n    \"binary\": {\n      \"always_ram\": true\n    }\n  }\n}\n\nTest Cluster Configuaration:\n\nRAM: 8 GB\nvCPUs: 1.0 vCPU\nDisk space: 32 GB\nNodes: 1 Node\n\nFYI <@1097578530931482664>"
  },
  {
    "threadId": "1295339863578640425",
    "name": "points/search is not returning payload data on search",
    "messages": "on search pyload data is not returning \nexample query:\n```POST collections/sacredohms/points/search \n{\n  \"limit\": 1,\n  \"vector\": [\n     0.037236378,-0.041108675, ...... -0.059867077,0.03547037\n  ]\n}```\noutput\n```{\n  \"result\": [\n    {\n      \"id\": \"8264279a-03c4-4f3c-8e4c-b2e75b935398\",\n      \"version\": 13,\n      \"score\": 0.5711459\n    }\n  ],\n  \"status\": \"ok\",\n  \"time\": 0.000428472\n}```\n\nexpected:\n```{\n  \"result\": [\n    {\n      \"id\": \"8264279a-03c4-4f3c-8e4c-b2e75b935398\",\n      \"version\": 13,\n      \"score\": 0.5711459,\n      \"payload\": {\n        \"content\": \"some data from storage.......\"\n      }\n    }\n  ],\n  \"status\": \"ok\",\n  \"time\": 0.000428472\n}```"
  },
  {
    "threadId": "1295275295691112513",
    "name": "Missing Payload",
    "messages": "I'm unsure about where to best adress this, as it is more of a question than a specifi bug(i also logged this as an issue, however it didnt seem to really be the best place for this).\n\nWhen using the similarity search, i tried using the payload for some additional tagging information. However, if i try to retrieve the payload from the search, the result is an empty payload. Os there anything i have to enable to allow the retrievel of a payload or some other mechanism that might prevent me from fetching the payload?\nThanks in advance for any help\n\nCode for reference\n```rust\nUpsert:\nlet payload: Payload = json!(\n{\"patient_id\": entry.patient_id}\n)\n.try_into()\n.unwrap();\nlet point = PointStruct::new(id, vectors, payload);\nupsert_point(&vectordb, point).await?;\n\nSearch:\nqdrant\n.search_points(&SearchPoints {\ncollection_name: \"patientdata\".to_string(),\nfilter: None,\nparams: Some(SearchParams {\nhnsw_ef: Some(128),\nexact: Some(false),\n..Default::default()\n}),\nvector: queryvector.clone(),\nscore_threshold: Some(0.665),\nlimit: 1000000,\n..Default::default()\n})\n.await;\n```"
  },
  {
    "threadId": "1295019662139981967",
    "name": "collection in a chatbot",
    "messages": "hi i am making a chatbot and want to keep the mesasges of use in rag should i have just one collection or multipule colliction and when should i do the split"
  },
  {
    "threadId": "1294993921453326427",
    "name": "Why indexed_vectors_count == 0?",
    "messages": "I running the Qdrant on self hosted k8s cluster with 3 pods\nsomehow, the point counts drop dramaticlly - maybe because I have a bug because we add and delete points every hour.\nor maybe because the cluster run on preemtible machines and once one pod id down it create a problem?\nanother question,\nWht does it means when the indexed_vectors_count == 0\nhere is the details of my collection\n'{\n    \"status\": \"green\",\n    \"optimizer_status\": \"ok\",\n    \"vectors_count\": 8537,\n    \"indexed_vectors_count\": 0,\n    \"points_count\": 8503,\n    \"segments_count\": 12,\n    \"config\": {\n        \"params\": {\n            \"vectors\": {\n                \"size\": 768,\n                \"distance\": \"Cosine\",\n                \"hnsw_config\": null,\n                \"quantization_config\": null,\n                \"on_disk\": null,\n                \"datatype\": null\n            },\n            \"shard_number\": 3,\n            \"sharding_method\": null,\n            \"replication_factor\": 1,\n            \"write_consistency_factor\": 1,\n            \"read_fan_out_factor\": null,\n            \"on_disk_payload\": true,\n            \"sparse_vectors\": null\n        },\n        \"hnsw_config\": {\n            \"m\": 16,\n            \"ef_construct\": 100,\n            \"full_scan_threshold\": 10000,\n            \"max_indexing_threads\": 0,\n            \"on_disk\": false,\n            \"payload_m\": null\n        },\n        \"optimizer_config\": {\n            \"deleted_threshold\": 0.2,\n            \"vacuum_min_vector_number\": 1000,\n            \"default_segment_number\": 0,\n            \"max_segment_size\": null,\n            \"memmap_threshold\": null,\n            \"indexing_threshold\": 20000,\n            \"flush_interval_sec\": 5,\n            \"max_optimization_threads\": null\n        },\n        \"wal_config\": {\n            \"wal_capacity_mb\": 32,\n            \"wal_segments_ahead\": 0\n        },\n        \"quantization_config\": null\n    },\n    \"payload_schema\": {}\n}'"
  },
  {
    "threadId": "1294944046644592641",
    "name": "Adding quantization during bulk upload in production",
    "messages": "Hi everyone, \nQdrant is proving to be cool day after another, and most importantly scalable.\n\nI have a production environment of 4 Qdrant nodes and 2 collections, each collection has 4 shards. Each node has about 24 GB of RAM. \n\nIn one collection, let's call it collection A, which is heavily used, I have about 41 mil points. I'm inserting about 9 million points per day. \n\nFor collection A, I have dense and sparse vectors both saved on disk, moreover the hnsw index is also saved on disk. \n\nMy questions:\n\nCan I add scalar quantization, placed on disk, without impacting the insert speed? \nAlso, could I decrease the memmap_threshold to a lower value without impacting the insert speed?"
  },
  {
    "threadId": "1294941562832420894",
    "name": "I have issue with the data stored in Qdrant",
    "messages": "I have 30G data in the collection but when I hit this below query it takes a lot of time and my server gets down but when I have a small collection of data is working fine \n\n# Initialize Qdrant client\nqdrant_client = QdrantClient(path=save_directory)\ncollection_name = \"smart_search_collection\""
  },
  {
    "threadId": "1294772653500006555",
    "name": "Client closes out while uploading snapshot",
    "messages": "I have two clusters hosted in AWS. I am trying to migrate a collection in a cluster that is using version 1.11.3 to the other, which is on version 1.12.0. I am able to create a 125GB snapshot of the collection in the cluster using 1.11.3 and download it to an ec2 using wget. When I attempt to upload the snapshot to the 1.12.0 cluster by running curl in that ec2, I get a 'Client Closed Request' message.  The logs on the 1.12.0 cluster say 2024-10-12T18:52:20.431267Z INFO actix_web::middleware::logger: 10.10.54.240 \"POST /collections/pharmaspectra-vectors/snapshots/upload HTTP/1.1\" 400 50 \"-\" \"curl/8.5.0\" 606.464478.\n\nI am able to upload a snapshot of a small (84 MB) test collection to the 1.12.0 cluster, which makes me think the problem is that I am trying to upload a snapshot that is too large. Is there something I can do to either break up the snapshot into smaller pieces or force qdrant to keep the connection alive?"
  },
  {
    "threadId": "1294407801694584853",
    "name": "RAG: Hybrid vs mult. collections",
    "messages": "I've got a RAG system of PDFs. It's currently in one collection of dense vectors. Each chunk is a page and each page has lots of metadata used for filtering. The filtering allows me to limit documents by audience and for the audience to be able to query over a single document if they like.\n\nTwo issues I'm trying to address:\n1. The metadata is crammed into the payload of each record, which adds a lot of overhead.\n2. The system sometimes struggles with relevance since each page lacks the context of the document it's in. I find myself enriching prompts by hand through a complex inference chain. Not a best practice.\n\nWhat's the best path forward?\nA. Rebuild as two collections: pdf_docs and pdf_pages with an identifier in pdf_pages payload that references parent document? Then run query that filters based on one collection and then semantic search on the other\nB. Have have one collection and use hybrid search with the document level info as sparse vectors and the pdf page content as dense? Does this help the overhead problem?\nC. Have one collection but get greater context into my chunks by using unstructured.io's open-source library or model-based API to serialize the pdf before chunking\nD. Something else?\n\nFeel free to challenge any of my assumptions\n<#1244504266496806994>"
  },
  {
    "threadId": "1294254195217530901",
    "name": "Creating Positive Negative Pairs for Discovery & Context Search",
    "messages": "I have gone through the examples shared in the documentation for Discovery and context search where we are passing hard-coded vectors, but how do we form positive negative pairs where input is just a text and lets assume we have a function that converts string to vectors\nIn this case, vector values are not know before, how do we form positive vectors which are closer to target"
  },
  {
    "threadId": "1294234780107542609",
    "name": "Performant index to check for non null values",
    "messages": "Having issues with a query using a lot of CPU for a fairly simple lookup\n\nQdrant cloud ID: 9d7d7a7a-9526-42c3-b797-8a67b0c97103\n\nCurrently using this payload schema\n```\n{\n  \"createdAt\":{\n    \"data_type\":\"datetime\"\n    \"points\":101426\n  }\n  \"projectId\":{\n    \"data_type\":\"keyword\"\n    \"points\":101406\n  }\n  \"annotation\":{\n    \"data_type\":\"keyword\"\n    \"points\":2933\n  }\n}\n```\n\nAnd my query looks like\n```\nPOST collections/library/points/query\n{\n  \"limit\": 10,\n  \"query\": [\n    -0.16943237,\n    ...\n  ],\n  \"filter\": {\n    \"must\": [\n      {\n        \"key\": \"projectId\",\n        \"match\": {\n          \"value\": \"feb1a14d-969f-4a1c-933d-4d3adaafb792\"\n        }\n      }\n    ],\n    \"must_not\": [\n      {\n        \"is_null\": {\n          \"key\": \"annotation\"\n        }\n      },\n      {\n        \"key\": \"source\",\n        \"match\": {\n          \"value\": \"Semantic cache\"\n        }\n      }\n    ]\n  },\n  \"with_payload\": true\n}\n```"
  },
  {
    "threadId": "1294230794277097535",
    "name": "Index for an isNull condition?",
    "messages": "What is the best way to set up an index if I want to filter for values that are not null? I currently use a must_not clause with an IsNullCondition but it seems to be very slow even though I have created a keyword index on the field."
  },
  {
    "threadId": "1294194295623909410",
    "name": "Can I add a new embedding to existing collection?",
    "messages": "Hi! Is it possible to add a new named embedding to an already existing collection? I would like to add a new embedding when I update my model and roll over to the new embedding once all records have been updated. It would be nice to be able to compare them as well before I push it to prod. I already have 2 named embeddings in my collection. Can I update it to contain a third and then populate it or is it better to create a new collection every time I need to add a new embedding?"
  },
  {
    "threadId": "1294177987746336901",
    "name": "variables in the console",
    "messages": "Is it possible to use variables in the web console? Something like:\n\n```@name=\"MyCollection\"\n\nGET collections/{{name}}```"
  },
  {
    "threadId": "1293926502554927144",
    "name": "Don't build an index for one config",
    "messages": "Hey I am trying to have multiple vector configs. One dense and one multi vector. How can I disable building an index for the multi vector (I want to use it purely for reranking). I tried it with m=0 and setting the full_scan_threshold to extremely high. But is there any way to do disable it for any case."
  },
  {
    "threadId": "1293895977257795665",
    "name": "Error: Too Many Open Files",
    "messages": "I am receiving this error in qdrant, preventing me from adding any more documents to the vector store:\n\n\"Service internal error: Failed to flush id_tracker versions: Service runtime error: RocksDB flush_cf error: IO error: While open a file for appending: ./storage/collections/privacylaw_ai/0/segments/1401e3ab-27f2-4f77-a534-8758758bce55/000369.sst: Too many open files\"\n\nI found the following information:\nhttps://qdrant.tech/documentation/guides/common-errors/\n\nWhich states I should run: ulimit -n 10000\n\nBut where do I run this command? I am using the qdrant binary built from source on MacOS Sequoia."
  },
  {
    "threadId": "1293892305098506272",
    "name": "Sparse Vectors on Rails",
    "messages": "Hey team. Just wondering if you have any insights on setting up hybrid search with Qdrant through Rails.\n\nWe have dense vectors already linked up to Qdrant (with dense vectors through Vertex embeddings). I'm wondering if there's an embedding API that you are aware of that I can use to generate sparse vectors through Rails?\n\nI'm aware of Python libraries supported by Qdrant that I could use to create my own API, but just looking for an existing API first."
  },
  {
    "threadId": "1293865349413994516",
    "name": "High-availability Qdrant deployment",
    "messages": "Hi! Firstly, Qdrant is cool. Secondly, I'm setting up Qdrant in cluster mode in k8s using the Qdrant helm chart. My deployment is not behaving as I expect (or as documented) and I am wondering if I have done something wrong.\n\n## The deployment:\nCluster size: 3\nShards per collection: 12\nReplication factor: 3\nWrite consistency factor: 2\n\nI've confirmed that these values are set correctly by querying the `/cluster` and `collections/:collection_name/cluster` endpoints. I see 12 shards as expected, and that these shards are evenly distributed amongst the nodes. I also see that replication factor is set to 3, although I've found no way to confirm that these replicas actually exist. The cluster started empty, and I added data to it without changing any configuration.\n\n## The problem:\nWhen I kill a pod in the cluster, all queries (default read `consistency`) to the Qdrant service fail until the pod recovers. It's as if the data isn't replicated across the cluster.\n\nDoes anyone know what might be wrong, or have suggestions for how I could debug this further? ЁЯЩП"
  },
  {
    "threadId": "1293751363054080101",
    "name": "qdrant_client.http.exceptions.ResponseHandlingException",
    "messages": "Summarize the issue: \nWe are upserting million vectors into collection via python stack which is optimized to read thousands of dynamodb items (via create_task & gather())\nWhen the pipeline scales up, qdrant starts to throw\n\nqdrant_client.http.exceptions.ResponseHandlingException\n\nCPU monitoring reports that we still have room and networking limits are set like this:\n\nlimits = Limits(max_connections=1000, max_keepalive_connections=40)\n\n    ASYNC_QDRANT_CLIENT = AsyncQdrantClient(\n        host, port=port, timeout=timeout, api_key=api_key, limits=limits\n    )\n\nNote: When we scale down the pipeline  and run the same code with few thousand items then no exception are raised and vectors gets added/updated/deleted properly\n=======\n\nAre you using Qdrant in local mode, with Docker or Qdrant Cloud? \nQdrant Cloud\n\n======\nWhich Qdrant version are you running? Sometimes the issue happens because you are not running the latest version.\n1.7.0\nWe upgraded the qdrant client to 1.12.0 and issue still persists\n\n=====\nHow are you connecting to Qdrant? Are you using a client?\n\nThis is how are connecting : \nASYNC_QDRANT_CLIENT = AsyncQdrantClient(\n        host, port=port, timeout=timeout, api_key=api_key, limits=limits\n    )\nQDRANT_HOST=\"https://58a61b37-d775-44b3-8347-d221acb0cc9d.us-east-1-0.aws.cloud.qdrant.io\"\nQDRANT_PORT=6333\ntimeout=30\n\n======\nProvide your current database configuration. What are the current collection settings? How many vectors in a collection? In case of a distributed setup, describe shards, replication factor etc.\n\nCollection config: Provided at the end.\n# vectors : 3.8 million\nNo distributed setup, replication or shards\n\n\n\n======\nWhat is your server configuration? If your question is related to performance, we need to know the operating system version, CPU, RAM size and storage details.\nServer config screenshot attached\n\n\nIs there anything showing in the server logs? Also, provide screenshots of monitoring or log excerpts that illustrate the issue."
  },
  {
    "threadId": "1293690997116764261",
    "name": "Embeddings don't match",
    "messages": "Hi there,\n\nI'm using the python API and I run into problems uploading simple dense-vector points. I generate the embeddings first and then upload them using upsert. \n\nMy generated embeddings are:[ -0.30374687910079956, -0.9881866574287415, 0.13102030754089355, 0.1470603048801422, 0.443134605884552, ....]\n\nWhat is stored in Qdrant is: [-0.025181504,-0.08192357,0.010861967,0.01219173,0.036737155,-0.023033412,....] (the \"default vector\")\n\nWhat  are the reasons for this?\n\n\nEdit: I run qdrant locally via docker\n\n\nMany thanks"
  },
  {
    "threadId": "1293612735585583144",
    "name": "What exactly is getting embedded",
    "messages": "the content coming in _node_content is quite a lot and contains a lot of metadata as well as nearby nodes, i want to know if only the text in this is getting embedded or all the information available in the _node_content"
  },
  {
    "threadId": "1293582631065227376",
    "name": "Qdrant Cloud Service Pricing Information",
    "messages": "Hi everyone,\n\nI would like to get more detailed information about the pricing for the different Qdrant Cloud services. I looked through the information on the Qdrant website, but it lacks detail.\n\nI would appreciate it if someone could help me with this.\n\nI used this resource:  https://qdrant.tech/pricing/"
  },
  {
    "threadId": "1293553557970223125",
    "name": "German language",
    "messages": "I try to embedd german language text using llama-index. After the embedding all of the german language special characters are gone from the embedded text. what can cause this and how to prevent this from happen. I did see that prior to the embedding those characters where unicode some \\u00e4 like things, so I added some function to translade those back top the right character and made the outcoming text UTF8 to be sure it would be part of the encoding. But still after the embedding process qdrant shows text missing all those characters Like \"T├╝re\" comes out as \"T re\" (T├╝re is the german word for door if that matters ЁЯШЙ ).\nIm pretty sure it does not have to be caused by qdrant but maybe someone knows the possible cause as I feel a bit lost after the  UTF8 trick which I was expecting to solve the issue"
  },
  {
    "threadId": "1293267030904012803",
    "name": "Move points id from one collection to another",
    "messages": "I want to apply multitenancy because I have my collections so bad managed\nTo do that I need to create a single collection and modify the payload of each point id of other collections to have the user id, and other important information. But how can i move the point from one collection to another without duplicating it?\n\nThanks"
  },
  {
    "threadId": "1293245051396292668",
    "name": "Specifying vector params in config after collection is already filled?",
    "messages": "Hi, I keep running into the following error when trying to query points (especially using Llamaindex retreivers but even when using the lone `query_points()` call):\n`b'{\"status\":{\"error\":\"Wrong input: Vector params for  are not specified in config\"},\"time\":0.000192497}'`\n\nLooking through the history of the error in the rest of the discord, I see people mentioning that the way the points are uploaded may not specify this in the config. Here's how I upload my points:\n```client.add(\n            collection_name=collection_name,\n            documents=documents,\n            metadata=metadata,\n            ids=ids,\n        )```\n\nWe have about a few months worth of data in our prod cluster now, and we were wondering if instead of reuploading all the points, is there a way to just specify the vector params for the collection? \n\nFor reference, I can attach some photos of our existing config in the qdrant cloud. It's weird cause to me it looks like there are some pretty specific vector names there already."
  },
  {
    "threadId": "1293226268397146183",
    "name": "Is there way of using keywords that are in payload to do hybrid search?",
    "messages": "My current method holds keywords in payload, and I want to use them to do hybrid search, by fusion-ing them\n\nIs there any way of doing this?"
  },
  {
    "threadId": "1293221020743762022",
    "name": "Memory consumption, \"On Disk\" vs \"on_disk\"...",
    "messages": "I'm a bit challenged on memory consumption.\nтАв I have 600k points with a vector size of 1536 dimensions (5,5GB). \nтАв Each point has a quite large json payload, around 15k each (9GB or is it 13,5GB?)\nтАв I have four payload indices, two of which are on disk (datetime and keyword)\n\nMy cluster currently use around 7GB of RAM, and I expect to increase the size of the collection with around 50%.\n\nI could obviously offload the bulk of the payload from the collection to some external storage, and only keep the indexed fields in the payload, but I would like to explore other options if possible.\n\nтАв I have been reading the storage docs, which talks about offloading the payload to disk, except for the payload indices.\nтАв I have allready marked the two less frequently used indices as on_disk.\nтАв Should I just toggle the \"On Disk\" checkbox in the cluster configuration, to offload the rest of the payload to disk?\n\nAny suggestions?"
  },
  {
    "threadId": "1293207750565953607",
    "name": "hybrid search not filtering by score_threshold?",
    "messages": "Hi there!\nI'm working with a Qdrant cluster that has sparse and dense vectors, and I'm using FastEmbed. I'm trying to filter results based on a `score_threshold >= 0.5`, but the results seem to be incorrect (all shown).\n\nHereтАЩs the query IтАЩm using:\n```\nquery(\n    collection_name=collection_name,\n    query_text=text,\n    limit=limit,\n    query_filter=filter,\n    offset=offset,\n    score_threshold=minScore,\n)\n```\n\nCould you help me understand why the score_threshold filter might not be working as expected? Any insights would be greatly appreciated!"
  },
  {
    "threadId": "1293130790691016786",
    "name": "Export to Bigquery?",
    "messages": "Is there a simple way you would recommend to export the data from a collection to an external data warehouse like eg Bigquery or GCS/S3? Preferrably a streaming solution like Dataflow/Fivetran/Airbyte?"
  },
  {
    "threadId": "1293122031843409940",
    "name": "Issue with Qdrant not retrieving metadata on AWS",
    "messages": "Hey everyone, IтАЩm working with Qdrant and LangChain to retrieve documents with metadata (including IDs), and everything works fine on my local machine. However, when I run the same setup on AWS, the metadata is missing in the search results (only page_content is returned). The documents seem to be indexed correctly, and IтАЩve checked for any version mismatch or network issues. Has anyone faced a similar problem or have any suggestions? Thanks in advance!"
  },
  {
    "threadId": "1292955720622538923",
    "name": "an I specify a dict key with arbitrary JSON-legal characters in a FieldCondition?",
    "messages": "I am storing unique tags in my payload as the keys of a dict (metadata.tags), rather than a list, because that enables me to append a new tag idempotently to multiple points at once:\n\n`  client.set_payload(payload = { tag: True }, key = \"metadata.tags\", points = points)`\n\nwhich is very clean and efficient. And I can query them with:\n\n`filter = QdrantFilter(\n           must = [\n               FieldCondition(\n                   key = f\"metadata.tags.{tag}\",\n                   match = MatchValue(value = True)\n               )\n           ]\n       )\n`\n\nwhich also works great. However, a problem arises when my tags include legal (JSON) characters such as \"/\", because the \"foo.bar.baz\" key syntax breaks when you include certain characters in a naked key like that. For instance:\n\n` key = \"metadata.tags.tag-with/slash\"`\n\ngenerates a parsing error, even though \"tag-with/slash\" is a JSON dict key.\n\nI would like to be able specify my key in such a way that non-compatible (but JSON-legal) characters can be escaped. For instance:\n\n`    key = f'metadata.tags[\"{tag}\"]'`\n\nwhich would echo jq's alternate query syntax. But I'd be happy with any solution."
  },
  {
    "threadId": "1292909659010633768",
    "name": "how to store short term memory",
    "messages": "I am trying to make a simple chat bit i wan touse qdrant for long term memory can i also use it for short term memory and should i even do that"
  },
  {
    "threadId": "1292819499627577376",
    "name": "Discovery API Giving Empty Results",
    "messages": "I just tried out a basic sample which creates a collection and inserts 4 vectors with ids starting from Integer 1, Now I am performing discovery search with target closest to two vectors, I have given positive negative pairs as   point ids itself \n\n1st, 3rd vectors are in decimals\n2nd , 4th vectors are in 10's and 100\nAll vectors of 4 dimension \nTarget vectors are also in decimals, so comparatively 1,3 vectors should be returned as output"
  },
  {
    "threadId": "1292752835364130817",
    "name": "Batching uploading without waiting error",
    "messages": "Hi all, \nThank you for the amazing work being done. \n\nI am batch uploading points to qdrant without waiting for the results to be processed as could be seen in the following code. \n\n```\n        for i in range(0, len(points), 1000):\n            await self.qdrant_client.upsert(\n                collection_name=self.collection_name,\n                wait=False,\n                points=points[i:i+1000]\n            )\n```\nIn some cases, I get error of ```grpc.aio._call.AioRpcError``` where the error message is ```    status = StatusCode.UNAVAILABLE\n    details = \"recvmsg:Connection reset by peer\" ```. \n\nMy interpretation that the nodes did not complete processing the previous upserts and hence are unavailable.  \n\nWithout changing WAL configurations nor setting wait to true, how to check from the code using qdrant client if the nodes are available to accept new points and finished processing the previous points? \n\nOr is there another way to check the WAL size currently used? \nDo you recommend another approach?\n\nThank you all"
  },
  {
    "threadId": "1292333671285788733",
    "name": "Hi, All",
    "messages": "I want to do like Private Chatbot for Interactive Learning, but i have bug with HuggingFaceTGIGenerator importing. url i follow: https://qdrant.tech/documentation/examples/rag-chatbot-red-hat-openshift-haystack/"
  },
  {
    "threadId": "1291783150799945870",
    "name": "Binary Build Issues",
    "messages": "Hi everyone, I recently installed Qdrant via the binary build instructions (because I have no desire to add Docker to my existing toolchain) and have found a number of issues.\n\nI have successfully built Qdrant on my Mac Studio Ultra and it is running and is connectable - but it did not build it with the dashboard functionality (so I have installed qdrant-dashboard project from github, which does interact with qdrant with no issues) and I do not see any way to create API keys (so I can include Qdrant in my n8n workflows.)\n\nIs anyone able to help with these issues?\n\nMany thanks."
  },
  {
    "threadId": "1290938845600284716",
    "name": "When changing replica numbers in helm chart",
    "messages": "2024-10-02T07:27:28.390928Z  WARN storage::content_manager::consensus_manager: Failed to send message to http://qdrant-demo-8.qdrant-demo-headless:6335/ with error: Error in closure supplied to transport channel pool: status: Unavailable, message: \"Failed to connect to http://qdrant-demo-8.qdrant-demo-headless:6335/, error: transport error\", details: [], metadata: MetadataMap { headers: {} }    \n2024-10-02T07:27:28.392269Z  WARN storage::content_manager::consensus_manager: Failed to send message to http://qdrant-demo-3.qdrant-demo-headless:6335/ with error: Error in closure supplied to transport channel pool: status: Unavailable, message: \"Failed to connect to http://qdrant-demo-3.qdrant-demo-headless:6335/, error: transport error\", details: [], metadata: MetadataMap { headers: {} }    \n2024-10-02T07:27:28.393602Z  WARN storage::content_manager::consensus_manager: Failed to send message to http://qdrant-demo-5.qdrant-demo-headless:6335/ with error: Error in closure supplied to transport channel pool: status: Unavailable, message: \"Failed to connect to http://qdrant-demo-5.qdrant-demo-headless:6335/, error: transport error\", details: [], metadata: MetadataMap { headers: {} }"
  },
  {
    "threadId": "1290007121429008507",
    "name": "Benchmark results way too bad",
    "messages": "I use https://github.com/qdrant/vector-db-benchmark/ and have set up the Python client and the VDBMS (sequentially) with two AWS instances t3a.2xlarge. Here you can see the test results (text is in German, sorry!).\n\nhttps://i.postimg.cc/3J8pkxKn/grafik.png\n\nIn general, my result is only about half as good as the results from https://qdrant.tech/benchmarks/. There it says Standard D8s v3 and Standard D8ls v5 were used by Azure. Is this still up to date?"
  },
  {
    "threadId": "1288029074056282182",
    "name": "Nested Payload indexing throwing error in latest Qdrant docker image.",
    "messages": "Hi team, my payload looks something like this:\n\"general\": {\n\"a\": \"test a\",\n\"b\": \"test b\",\n},\n\"image_ids\": [],\n\"chunk_ids\": [],\n\"ocr_chunk_ids\": [],\n\"_id\": \"038e21b5-5469-444a-841e-4e60e1bd152b\",\n\"parent_id\": \"038e21b5-5469-444a-841e-4e60e1bd152b\",\n\"path\": \"/home/ubuntu/pdf_storage/038e21b5-5469-444a-841e-4e60e1bd152b/ALCHG0006DS.pdf\"\n}\n\nEarlier in Qdrant i needed to index the keys inside general key and i used to index or even search within that key with:\n\nresults = client.scroll(\n    collection_name=collection_name,\n    scroll_filter=models.Filter(\n        should=[\n            FieldCondition(\n                key=\"general.a\", match=models.MatchText(text=\"a\")\n            ),\n        ],\n    ),\n)\n\nand index it like:\nclient.create_payload_index(collection_name=collection_name,\n                                             field_name=\"general.a\",\n                                             field_schema=models.TextIndexParams(\n                                                    type=\"text\",\n                                                    tokenizer=models.TokenizerType.PREFIX,\n                                                    min_token_len=1,\n                                                    max_token_len=15,\n                                                    lowercase=True,\n                                                ),\n                                            )\n\nbut in the latest qdrant version i am neither able to search nor index. Has there been any changes in the schema to achieve the same?"
  },
  {
    "threadId": "1275002311541788713",
    "name": "Slow Upload of Sparse Vectors",
    "messages": "Background:\n\nI am trying to create a collection with almost 60M sparse vectors. For huge collections, turning off indexing during the initial upload is recommended. However, on setting indexing_threhold = 0, I cannot disable sparse vector indexing. This might be causing the upload process to be slow compared to dense vectors. It takes way less time with the same collection configuration to upload dense vectors. Is there a way to disable sparse vector indexing? \n\nConfiguration:\n\"config\": {\n      \"params\": {\n        \"vectors\": {},\n        \"shard_number\": 1,\n        \"replication_factor\": 1,\n        \"write_consistency_factor\": 1,\n        \"on_disk_payload\": true,\n        \"sparse_vectors\": {\n          \"bm25\": {\n            \"index\": {\n              \"on_disk\": true\n            },\n            \"modifier\": \"idf\"\n          }\n        }\n      },\n      \"hnsw_config\": {\n        \"m\": 16,\n        \"ef_construct\": 100,\n        \"full_scan_threshold\": 10000,\n        \"max_indexing_threads\": 0,\n        \"on_disk\": true\n      },\n      \"optimizer_config\": {\n        \"deleted_threshold\": 0.2,\n        \"vacuum_min_vector_number\": 1000,\n        \"default_segment_number\": 8,\n        \"max_segment_size\": 4000000,\n        \"memmap_threshold\": null,\n        \"indexing_threshold\": 0,\n        \"flush_interval_sec\": 5,\n        \"max_optimization_threads\": null\n      },\n      \"wal_config\": {\n        \"wal_capacity_mb\": 32,\n        \"wal_segments_ahead\": 0\n      },\n      \"quantization_config\": null\n    },\n    \"payload_schema\": {}"
  },
  {
    "threadId": "1273693677553717309",
    "name": "Clustering with Qdrant",
    "messages": "Hi team,\nsay suppose i have 1000 products which i want to group based on their similarity into several groups. These products have an image and few metadata like Price, Category etc. Can it be possibe with Qdrant? I have thought of few approaches like having the image embeddings of all products and then doing similar search with some filters from metadata. The thing is it can be possible that all 1000 products may be similar and should be in a single group. Is there any API for such similar use case?\nAnother approach could be using DBSCAN clustering algorithm. Anyone from the team who could suggest which could be the best way?"
  },
  {
    "threadId": "1266641422711263244",
    "name": "Re-index after  deletion?",
    "messages": "I deleted all the points in a collection manually.\nNow points_count is 0.\nBut indexed_vectors_count remains 3 and segments_count is 2.\n\nShould I do anything about this? Or is it handled automatically?"
  },
  {
    "threadId": "1261947292475330621",
    "name": "Failed to load local shard (MacOS) on local qdrant server",
    "messages": "Hi I'm getting this error when running the qdrant server locally with Docker.\n\nMacOS Sonoma 14.4.1\n\nHow did I ran the qdrant server?\n\n```\ndocker run -p 6333:6333 -v $(pwd)/qdrant_storage:/qdrant/storage:z qdrant/qdrant\n```\n\nit has 1 collection called \"test1\", it has like 2gb of vectors which is about $50 in embeddings using the voyage-large-2 model, so I wouldn't like to lose those.\n\nIs there a way I can recover this vectors? Or just get the server to run?\n\nI remember running the server and stopping it multiple times before this happened, and it worked fine, I did look around inside the `qdrant_storage` folders and subfolders just to see how it was inside, but that's it, I didn't changed anything, I think macos does something to the folders when you just look around and may be the cause of this.\n\nI already tried deleting the .DS_STORE files, didn't helped."
  },
  {
    "threadId": "1239456143701708810",
    "name": "GET collection size (bytes)",
    "messages": "Hi do you have or can you improve the GET Collection Info to add the collection size in bytes?\n\nCurrently the GET Collection Info only shows number of points. We want to know the total amount of bytes used up by the collection (including all metadata)"
  },
  {
    "threadId": "1238089523817349191",
    "name": "Vectors count and points count is inconsistent",
    "messages": "I use one thread to upsert points to qdrant cluster,  upsert one point at a time, and a point contains a vector.\n\nThe qdrant cluster has 3 instances that will be randomly killed for testing.\n\nAfter running for several hours, I found that vector count and point count are not equal, but search query looks fine.\n\nMy question is why is the inconsistency happening and what are the possible reasons?\n\n\nCheck points count on each shard and replicas get from API collections/test_2_1/cluster:\nNode1: \n```{\n  \"result\": {\n    \"peer_id\": 1090326398130707,\n    \"shard_count\": 3,\n    \"local_shards\": [\n      {\n        \"shard_id\": 0,\n        \"points_count\": 131218,\n        \"state\": \"Active\"\n      },\n      {\n        \"shard_id\": 2,\n        \"points_count\": 139601,\n        \"state\": \"Active\"\n      }\n    ]\n  }\n}\n```\nNode2:\n```{\n  \"result\": {\n    \"peer_id\": 7762661076067225,\n    \"shard_count\": 3,\n    \"local_shards\": [\n      {\n        \"shard_id\": 1,\n        \"points_count\": 151332,\n        \"state\": \"Active\"\n      },\n      {\n        \"shard_id\": 2,\n        \"points_count\": 139601,\n        \"state\": \"Active\"\n      }\n    ]\n  }\n}\n```\nNode3:\n```{\n  \"result\": {\n    \"peer_id\": 663719077956453,\n    \"shard_count\": 3,\n    \"local_shards\": [\n      {\n        \"shard_id\": 0,\n        \"points_count\": 131218,\n        \"state\": \"Active\"\n      },\n      {\n        \"shard_id\": 1,\n        \"points_count\": 151332,\n        \"state\": \"Active\"\n      }\n    ]\n  }\n}\n```\n\nBelow is the collection info get from API collections/test_2_1:\n```{\n  \"result\": {\n    \"status\": \"green\",\n    \"optimizer_status\": \"ok\",\n    \"vectors_count\": 422152,\n    \"indexed_vectors_count\": 0,\n    \"points_count\": 422151,\n    \"segments_count\": 24,\n    \"config\": {\n      \"params\": {\n        \"vectors\": {\n          \"size\": 4,\n          \"distance\": \"Dot\"\n        },\n        \"shard_number\": 3,\n        \"replication_factor\": 2,\n        \"write_consistency_factor\": 1,\n        \"on_disk_payload\": true\n      }\n    }\n  },\n  \"status\": \"ok\",\n  \"time\": 0.000462343\n}\n```"
  },
  {
    "threadId": "1205026760295452692",
    "name": "Wrong input: Vector params for  are not specified in config",
    "messages": "Running Docker build **qdrant/qdrant:v1.7.4**\n\nUsing the examples from:\n**http://0.0.0.0:6333/dashboard#/tutorial/quickstart**\n\nRunning (after upserts):\n```bash\nPOST collections/test/points/search\n{\n  \"vector\": [0.2, 0.1, 0.9, 0.7],\n  \"limit\": 3,\n  \"with_payload\": true\n}\n```\n\nReturns:\n```json\n{\n  \"error\": \"Wrong input: Vector params for  are not specified in config\"\n}\n```\n\nRunning a simple search returns this error, I am unable to find similar posts on this nor does this error help me debug what the issue is.\n\nIs this a config setup issue (my docker config is the default one other than grpc being disabled)?"
  },
  {
    "threadId": "1185581267320852540",
    "name": "helm upgrade failure",
    "messages": "Hello, I am trying to use helm to upgrade an existing installation of qdrant with:\n\n```\nhelm upgrade qdrant qdrant/qdrant --version 0.7.3 -n qdrant\n```\n\nBut I am presented with:\n\n```\nError: UPGRADE FAILED: cannot patch \"qdrant\" with kind StatefulSet: StatefulSet.apps \"qdrant\" is invalid: spec: Forbidden: updates to statefulset spec for fields other than 'replicas', 'ordinals', 'template', 'updateStrategy', 'persistentVolumeClaimRetentionPolicy' and 'minReadySeconds' are forbidden\n```\n\nChecking status shows:\n\n```\nhelm status qdrant-0 -n qdrant\nNAME: qdrant\nLAST DEPLOYED: Sat Dec 16 14:38:50 2023\nNAMESPACE: qdrant\nSTATUS: failed\nREVISION: 3\nNOTES:\nQdrant v1.7.2 has been deployed successfully.\n\nThe full Qdrant documentation is available at https://qdrant.tech/documentation/.\n\nTo forward Qdrant's ports execute one of the following commands:\n  export POD_NAME=$(kubectl get pods --namespace qdrant -l \"app.kubernetes.io/name=qdrant,app.kubernetes.io/instance=qdrant\" -o jsonpath=\"{.items[0].metadata.name}\")\n\nIf you want to use Qdrant via http execute the following commands\n  kubectl --namespace qdrant port-forward $POD_NAME 6333:6333\n\nIf you want to use Qdrant via grpc execute the following commands\n  kubectl --namespace qdrant port-forward $POD_NAME 6334:6334\n\nIf you want to use Qdrant via p2p execute the following commands\n  kubectl --namespace qdrant port-forward $POD_NAME 6335:6335\n```\n\nBut confirming the version number of the deployed qdrant service indicates that it is still at `1.6.1`\n\nany ideas?"
  },
  {
    "threadId": "1177426906614866001",
    "name": "nested payload index",
    "messages": "payload as the flowing:\n{\n        \"new_tag\": {\n          \"actor\": [  { \"weight\": \"5\"} ]\n}\nquery filter is \n    \"filter\": {\n        \"must\": [\n          { \"key\": \"new_tag.actor[].weight\", \"match\": { \"value\": \"5\" } }\n        ]\n    }\nThe problem at hand is how to create an index for the query."
  },
  {
    "threadId": "1149354934152732733",
    "name": "Before you ask...",
    "messages": "**Reporting issues on Discord**\n\nBefore you create a report, first check whether a similar issue has already been brought up by someone else. \nCheck the troubleshooting docs:\nhttps://qdrant.tech/documentation/guides/common-errors/ \nGo to the GitHub issue search and look through open and closed issues. If the issue has been fixed, try to reproduce it using the latest version of Qdrant.\n\n**Required information**\n\nSummarize the issue. Describe what you are trying to do and how you are being blocked. Let us know what should have been the result.\n\nAre you using Qdrant in local mode, with Docker or Qdrant Cloud? If you are using Qdrant Cloud, please provide the cluster id.\n\nWhich Qdrant version are you running? Sometimes the issue happens because you are not running the latest version.\n\nHow are you connecting to Qdrant? Are you using a client?\n\nProvide your current database configuration. What are the current collection settings? How many vectors in a collection? In case of a distributed setup, describe shards, replication factor etc.\n\nWhat is your server configuration? If your question is related to performance, we need to know the operating system version, CPU, RAM size and storage details.\n\nIs there anything showing in the server logs? Also, provide screenshots of monitoring or log excerpts that illustrate the issue.\n\n**Reproduce the issue**\n\nWe need reproducible code or a script that demonstrates the problem. Please give us step-by-step instructions to recreate the situation. For errors or problems with queries:\nShow the exact code of the query or the request.\nPaste the exact output of the response or the error message you are getting back.\n\nTry to recreate the problem in a controlled environment. This helps you confirm that the issue is consistent. If possible, identify the specific conditions that trigger the problem.\n\nWhat were you doing when the error happened? Is there a common pattern?\nHave you attempted any workarounds?"
  }
]