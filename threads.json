[
  {
    "threadId": "1236669839137833011",
    "name": "Selective queries: Retrieve only a few fields, all with non-repeated condition",
    "messages": "There is this custom categories scenario which I am working on, which put simply, I need to retrieve some products based on some pre-defined filters. \nPreviously I discussed some of the aspects of this challenge, but now I am facing something new. \nSo, other than the product list itself, which I only need to use the scroll api with filters, I also need to extract the attributes of all the products that are in this custom category, and send them to client as well. \nIf I want to clarify more on that, imagine we have 10.000 candidate products that fits in the filters defined by the admin. e.g., \"women swim suits\". Now, for attribute filters, user should be able to filter based on size, color, material, and etc.  These filters should be inclusive of all th attributes of all the candidates of this specific custom category filter. But extracting them from all 10000 candidates every time the request is called is not the best solution. \n## What are my options?\nSo, one possible way to handle this is using redis cash (as the custom categories are limited, we can use redis and just update the filters to include newly added products periodically). But still, we would need to first retrieve all the possible candidates that fits in that custom category, and then process them in python to extract their attributes. \nSo, one may think that I have to just retrieve all the points at first, then separate their attributes. It may work, because I will give these to celery workers to run in a specified time in the background. But, can I just do something to only retrieve the needed attribute fields, and instead of repeating color=green for 3000 products, for example, just get color=green once, without the need to repeat it?\n\nSomething similar can be done in pg for example, using DISTINCT or something."
  },
  {
    "threadId": "1236427673056706682",
    "name": "Hello all,",
    "messages": "Hello all, I just started using Qdrant cloud today using Python. When I try to use the API keys, they keep expiring. I had to get a new key every 10-20 mins. This is not normal. Am I missing something?"
  },
  {
    "threadId": "1236361130209972244",
    "name": "Python client: Using Scroll API with Payload Order By and Offset in Qdrant Python Client",
    "messages": "Hello all, I've encountered a scenario where I needed to apply sorting and pagination to my scroll requests. I found that I can use `order_by.start_from` on the Qdrant's UI, but I don't know how to apply that into my code (I am using python).\nPlease see the uploaded images for detail information. Thank you for help"
  },
  {
    "threadId": "1236249916175814747",
    "name": "Combine Full-Text Search with Semantic Search",
    "messages": "Hello,\nI think about combining semantic search with a full-text search. I noticed that the `search` method has two parameters: `query_vector` and `query_filter`. As far as I know, it works as an \"and\"-condition. So the data is first filtererd by `query_filter` and then ranked using the `query_vector`. \nIs there a way to use it in an \"or\"-condition: So the points are returned that were either found by the query vector or the full-text filter? Or do I have to use the `scroll` method first for the full-text search and combine the results from the `search` function?\nMany thanks for any advice how to impement such as system."
  },
  {
    "threadId": "1236238897852190791",
    "name": "vector.on_disk vs optimizer.memmap_threshold_kb",
    "messages": "Hey everyone,\n\nI found these two parameters for keeping vectors on disk. But the doc has some contradictions regarding how these both work. On one hand, it says that memmap_threshold_kb must be activate to enable storing vectors on disk. But on the otherhand, if `on_disk` is configured in the vector, the doc says that it will be stored on disk. \n\nSo do they both need to be set? Or is it sufficient to only set `on_disk`?\n\nThanks!"
  },
  {
    "threadId": "1236218017210433546",
    "name": "Vanna collection",
    "messages": "Hello, i'm using vanna with qdrant\ni have many collection in qdrant and in want to know how can i use qdrant with vanna to train read different database but not in the same collection to be rag more precise\nand i suspect their is some mistake in rag qdrant / vanna because when i'm training all ddl go to documentation, and i don't provide documentation \ni could give more details after\ni use the code provide by wanna and them tell me is from qdrant dev team"
  },
  {
    "threadId": "1236065223387906130",
    "name": "DEADLINE_EXCEEDED during upsert points operation",
    "messages": "Hi there! I'm using java driver `1.9.0` against Qdrant free tier to being developing. And when executing upsert points operation, I get this error,\n```\nio.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.894704011s. Name resolution delay 0.085216232 seconds. [closed=[], open=[[buffered_nanos=366183167, buffered_nanos=483288, remote_addr=ded78a51-8370-47d8-adb0-6147f0fcbba2.us-east4-0.gcp.cloud.qdrant.io/35.245.15.233:6334]]]\n    at io.grpc.Status.asRuntimeException(Status.java:537)\n    at io.grpc.stub.ClientCalls$UnaryStreamToFuture.onClose(ClientCalls.java:538)\n    at io.grpc.internal.DelayedClientCall$DelayedListener$3.run(DelayedClientCall.java:489)\n    at io.grpc.internal.DelayedClientCall$DelayedListener.delayOrExecute(DelayedClientCall.java:453)\n    at io.grpc.internal.DelayedClientCall$DelayedListener.onClose(DelayedClientCall.java:486)\n    at io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:574)\n    at io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:72)\n    at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:742)\n    at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:723)\n    at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n    at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n    at java.base/java.lang.Thread.run(Thread.java:1583)\n    5429 ERROR [grpc-default-executor-11] QdrantClient Upsert operation failed\n```\n\nWhat am I missing here? Any help is appreciated - Thank you!"
  },
  {
    "threadId": "1235972119985131711",
    "name": "Qdrant Search",
    "messages": "Hello !\nI have 3 different collections in qdrant db is there any way that I specify more than one collection in qdarnt search I am using sparse search\n   search_result = connection.search(\n        collection_name=\"jira-data\",\n        query_vector = embeddings,\n        score_threshold = 0.5,\n        limit = 3,\n    )"
  },
  {
    "threadId": "1235964544594083945",
    "name": "Partition by payload",
    "messages": "Hello,\nI've seen that you can partition by payload, but can you take a payload that is a keyword list as a value for partitioning? \n\nexemple of my payload:\n```\npayload: {\n  idContent: 1,\n  channelIds: [1, 2], I \n  geographies: [],\n  insights: [],\n  companies: [],\n  sectors: []\n}\n```\n\nAnd my creation of collection:\n```ts\n  await client.createCollection(QDRAND_COLLECTION, {\n    vectors: { size: 256, distance: 'Cosine' },\n    hnsw_config: {\n      payload_m: 16,\n      m: 0,\n    },\n  });\n\n  client.createPayloadIndex(QDRAND_COLLECTION, {\n    field_name: 'channelIds',\n    field_schema: 'keyword',\n  });\n```"
  },
  {
    "threadId": "1235938898669076490",
    "name": "Embeddings and QDrant for Image Quality Control and Labelling",
    "messages": "So we deploy models on Jetson Orins at the edge, there are several overlapping features I want to implement, and I could do with knowing where QDrant fits and where it doesn't. So I'll outline the very rough architecture I'm currently picturing. \n\nOne part is a scatter plot visualization of all of our images, I don't see much need for QDrant to be involved here, in my mind it's just for Engineering to decide how to store it to make it easy to integrate into our UI. I'll run some dimensionality reduction like UMAP on the edge device, and we'll upload that straight to the Database. \n\nBut then I'd also plan to store raw, larger embeddings in QDrant, we'll use it to classify / alert for images with poor quality (a variety of different core issues), and to find more images that look similar to something that's caused a model error. \n\nAny thoughts at all? \n\nFor the time-being we'll just be using image level embeddings, but down the line we'll add object level embeddings.\n\n<@844295650400534599> ?"
  },
  {
    "threadId": "1235840144821583892",
    "name": "How to integrate the Hybrid search with langchain and qdrant",
    "messages": "I have integrated the Hybrid search with langchina and qdrant db for the BM25 data getting stored inmemory, so i want replace that with vector db so may i know how is this possible to store the chunks for BM25 instead of Inmemory to vector db"
  },
  {
    "threadId": "1235834854466392114",
    "name": "Qdrant-Spark write jobs fail",
    "messages": "Hi All,\n\nI have around 20 jobs that writes data from databricks to qdrant, parallely. I keep running into an error that's not very informative.\n\nERROR: **ResponseHandlingException: Timed Out**\n\nHowever, if I run these jobs one by one I'm not getting the error. Is there a work around for this?"
  },
  {
    "threadId": "1235673367177068725",
    "name": "Qdrant Migration",
    "messages": "We have currently deployed Qdrant v1.8.1 with 3 nodes, which has around 5 collections and each of them has 100k chunks. When I did helm upgrade to Qdrant v1.9.0, it was successfully recreated new deployment with Qdrant version 1.9.0 with all the collections, which is a very good feature. \n\nMy question is how does helm upgradtion happens on the Qdrant ? My all collections are in-memory. When helm upgrade happens, lets say first it creates new pod (pod-qdrant-3) on new cluster and deletes pod-qdrant-3 from existing the cluster. But hoe does it copies collections from deployment to new deployment which are in-memory ? If there is any detailed blog or some materials on this topic will be a great help to understand in a better way. Thanks you !!"
  },
  {
    "threadId": "1235615423626481787",
    "name": "package repo for binary",
    "messages": "Dear team,\n\nWe want to use qdrant binary for every release. Currently getting it grom github, but want to automate the process. Do you guys have package repository like jfrog, which we can mirror with our internal repo and pull down latest versions? Really appreciate the help"
  },
  {
    "threadId": "1235583421334552647",
    "name": "Custom categories: Qdrant or not?",
    "messages": "I am facing a new challenge in our application, which is related to the custom categories, generated by our admins, to better provide categorized products for our users. I have concluded the details fully in this question: https://datascience.stackexchange.com/questions/128926/retrieving-products-based-on-pre-existing-filters-the-fastest-way-possible\n\nIf it is better to copy the question text in here, please say so. Thank you very much for your help and consideration."
  },
  {
    "threadId": "1235578162986090527",
    "name": "Qdrant for searches in multiple vector fields",
    "messages": "Hi everyone!\n\nI discovered Qdrant recently during a vector database research and found the company transparency incredible along with everything you've already built on the platform! Congratulations to the team and everyone involved!!\n\nI'm looking for help to understand if it would be possible to use Qdrant for my use case. I have a search engine to find restaurants, bars, pubs and establishments in general, all in standardized JSON format. We do not vectorize all data into a single vector, we separate it by different data formats such as location, opening hours, location features, reviews, etc. We believe that the ideal option for our search is the possibility of searching in multiple vectors to find the location that has the best score among all the fields searched. Is it possible to build this solution on your platform?\n\nThanks!"
  },
  {
    "threadId": "1235576803821748304",
    "name": "Modifying the Score Function to Consider Publication Date",
    "messages": "Hello,\nI'd like to know if anyone knows of a way to modify the score function to include a publication date in the payload, to create a bias between score and publication date."
  },
  {
    "threadId": "1235564117360906281",
    "name": "Delete (drop) vector index",
    "messages": "Hi everyone!\nHow to delete vector index that is already exist? I tried https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/delete_field_index endpoint with \"vectors\" as `field_name` but it didn't delete the index"
  },
  {
    "threadId": "1235480629844508712",
    "name": "Is there support for read and write only nodes?",
    "messages": "We are running few models which require real time updates. Usually the CPU of this cluster is >10% but during indexing time the CPU spikes to ~60%. No of points in the collection is in the range of 300-500k only.\nWe want to route these updates to write only nodes and replicate to replica nodes eventually to minimize latency impact on serving/read nodes."
  },
  {
    "threadId": "1235150073684426864",
    "name": "#storing anything in RAM Qdrant ?",
    "messages": "Hey Iâ€™m building a project by using qdrant storing 50M above vectors in it I have deployed qdrant on AKS azure cloud but that Node of AKS memory going up and chowking system even I did payload_on_disk true and on_disk index is true so need help"
  },
  {
    "threadId": "1234885146365136998",
    "name": "Automatic Vector encoding in rust",
    "messages": "Beginner Question:   I was following this article https://qdrant.tech/documentation/tutorials/search-beginners/\nI was able to get it working with python, I wold like to use it with rust for python there is encoder library is there anything similar to that in rust"
  },
  {
    "threadId": "1234866117428645999",
    "name": "Query multiple documents in the same collection",
    "messages": "Hello, I am starting to use Qdrant. I have code to process multiple documents and import the vector data into Qdrant. Sometimes, I found that when I query documents in the same collection, I did not get all answers that I need. For example, if I have several documents describe project A. When I query \"could you describe project A from all documents\". The results I get is only project A description from one document, missing description of project A from other documents. If I create one collection for each document, and query the same question from each of the collections, it works. But this is not what I want. In such a case, I have to post process all the answers from different collections. Is this a limitation in Qdrant or I can fix this problem by setting some parameters?"
  },
  {
    "threadId": "1234831750539247721",
    "name": "Multitenancy with sub indexing",
    "messages": "Hi Team, i am looking for advice on an indexing strategy to get the best possible performance for my vector search. The documents only mention a single index for multitenancy. I am not sure what the right setup is for multi indexes.\n\nI have two collections. One to store messages  from chats of a bot (for long term memory) and one for documents (for RAG). \nMy data  is structured as shown in the little image.  A user can create multiple bots with bots.id. Each bot has its messages indexed in the messages collection. A user can also have multiple documents which are indexed in a documents collection.\n\nAs mentioned in the docs the indexing can become a bottleneck for multitenancy if all indexes are store in a global index. My current setup for the messages is as followed\n\n```\n// messages payload\n{\n  messageId: id,\n  content: content,\n  user_id: uid,\n  bot_id: botId,\n}\n\n//global index\nclient.createCollection(\"messages\", {\n ....\n  hnsw_config: {\n    m: 16,\n    ef_construct: 128,\n    payload_m: 0,\n    on_disk: false,\n  },\n});\n//index for bot_id\nclient.createPayloadIndex(\"messages\", {\n  field_name: bot_id,\n  field_schema: keyword,\n});\n```\n\nwhen searching for vectors the application will only look for the vectors associated with one bot at a time.\n\n```\nconst filterGlobal = {\n  must: [\n    {\n      key: \"bot_id\",\n      match: {\n        value: \"botId\",\n      },\n    },\n  ],\n};\n```\n\n\n90%  of the time the query will be limited to one bot at a time  (if searching multiple bots  their ids will always be known and passed in as filters) so  i think this  setup is not optimal? \nSo is it better to drop the global index and create sub indexes for each bot?"
  },
  {
    "threadId": "1234826896944926750",
    "name": "How to determine how relevant each of the returned vectors actually are",
    "messages": "I'd like to start a discussion on advanced techniques how to make the final determination on if each of the returned vectors are sufficiently relevant to the search query. I do notice that there is a similarity score returned with each vector but in practise trying to use this to filter the results seems to not work very well. Perhaps my technique/understanding/reasoning is flawed for using this property altho it seems that something more advanced might be required. \n\nWhat is your experience and do you have any advice?"
  },
  {
    "threadId": "1234694103938367569",
    "name": "Panic after a restart",
    "messages": "On v1.8.4, see panic below after restarting . Any ideas?\n\nCall stack attached"
  },
  {
    "threadId": "1234663462592974858",
    "name": "embedding models for Binary Quantization",
    "messages": "Which embedding models can be used. I read BQ only works effectively if the embedding model has a centered distribution."
  },
  {
    "threadId": "1234592398819197029",
    "name": "Authentication with JWT",
    "messages": "Hi Team, I upgraded to v1.9.0 to enable JWT authentication. It's disabled on left pane but I can able to access it through URL. I am not if I am missing something here. \n\nservice:\n  api_key: <api-key>\n  jwt_rbac: true"
  },
  {
    "threadId": "1234513033909243914",
    "name": "Distinct Documents Without Vector",
    "messages": "I know there is scroll for vector-less search/filter and group for grouping but that requires a vector array.\n\nHow would I do a distinct search on a field e.g. file_name, without providing a vector?"
  },
  {
    "threadId": "1234493043856379994",
    "name": "Distributed Qdrant behaviour",
    "messages": "Hi, I wanted to understand, can a distributed qdrant cluster have multiple primary nodes? If not, how can we make sure that a single primary node does not become single point of failure for entire cluster?"
  },
  {
    "threadId": "1234486214346670140",
    "name": "Upsert Qdrant infos",
    "messages": "Hey ! Quick one can we have the info about how much data was insert / updated / ignored during an upsert ?"
  },
  {
    "threadId": "1234432206013861934",
    "name": "Image Pair Data Loader",
    "messages": "Hi All!\n\nJust getting started on Quaterion and am in need of a little advice for my data loader. \nI want to fine tune an image embedding model for similarity search. \nMy training dataset consists of image pairs rather than groups. \n\nMy plan is to: \n* Use triplet loss\n* Apply augmentation to the anchor \n* Provide the positive example \n* Negative would be random example from the other known positives\n\nDoes this sound like a reasonable approach? Any advice on how to write my data loader?\n\nAppreciate any advice."
  },
  {
    "threadId": "1234403379992723487",
    "name": "Qdrant's hybrid-search backward compatibility",
    "messages": "Hi, we are using Qdrant as vector database as part of a RAG system using langroid. We wanted to add qdrant's SPLADE-based sparse retrieval ((ref)[https://qdrant.tech/articles/sparse-vectors/]).\n\nAs I understood, the only solution to add it is using each vector as a named vector. So, if we want to use dense+sparse retrieval for a collection, we have to add both dense and sparse as named vectors.\n\nThere is an issue of backward compatibility for our use case since we want to use a common code for retrieval from both dense and hybrid collections. If we already have a dense collection (& that's not a named vector), we cannot use the same retrieval code. We wanted to know if there is a workaround to use sparse retrieval along with being compatible for default dense-vector collections.\n\nYou can checked the PR I raised to langroid for adding sparse retrieval.\nPR: https://github.com/langroid/langroid/pull/450"
  },
  {
    "threadId": "1234143164776976405",
    "name": "Qdrant takes long time to run concurrent requests",
    "messages": "So, previously I have asked a similar question, but back then I did not have async or connection pool or something like this. Question link in SO is (https://stackoverflow.com/questions/78326924/qdrant-takes-25-second-to-retrieve-1000-single-products).\nBut now, I have implemented FastAPI, Async functions, and AsyncQdrantClient alongside with connection pooling using httpx `Limits`. But still, results are more or less the same. \nSo, let's see what I have done here:\n## 1-connection\nFirst of all, I want to create a client:\n```\nlimits = httpx.Limits(max_keepalive_connections=5, max_connections=10)\nqdrant_client = AsyncQdrantClient(\n    host=kwargs.get(\"host\", \"192.168.1.122\"),\n    port=kwargs.get(\"port\", 6333),\n    timeout=kwargs.get(\"timeout\", 100000),\n    # limits=limits,\n)\n```\nNote: adding or ignoring `limits` had no effect in the result. \n\n## 2-API setup\nI am using FastAPI with the following setup:\n```\nsearch_app = FastAPI()\n@search_app.on_event(\"startup\")\nasync def startup():\n    async_qd_client = qd_funcs.async_qdrant_client()\n    search_app.state.async_qd_client = async_qd_client\n```\nthen the api itself:\n```\n@search_app.get(\"/v1/test/{pk}/\")\nasync def test(pk: int, request=Request):\n    async_qd_client = search_app.state.async_qd_client\n    products = await qd_funcs.get_single_product_similars(\n        product_id=pk,\n        lang_int=1,\n        page=1,\n        per_page=1,\n        qd_client=async_qd_client,\n    )\n    return {\"product\": products}\n```"
  },
  {
    "threadId": "1234001282860322836",
    "name": "Search + Filtration alongside pagination",
    "messages": "I am using Qdrant + FastAPI, and I have a growing database of products, each containing 60 different fields. I have previously written recommendations api using Qdrant, and handled pagination in Qdrant side using `limit` and `offset` parameters. \nThe usage scenario is fairly straightforward in `recommend`; but there are some challenges in `search` api that I am not able to think of any solution.\nFirst of all, the `filter` api:\nSo when user searches for some query, I first embedd his query text, and use it's vector to call Qdrant search function; then I use a re-ranker model to evaluate similarity between the original query and the first-hand candidates returned by Qdrant search to furthor improve the similarity score (Although only 72 items of the first 1000 items (`limit` param) is sent to the re-ranker model). \n```\npayloads = search.search_query(\n    search_lang=search_lang,\n    search_query=translated_query,\n    limit=QDRANT_RESULT_LIMIT,\n)\ndata = search.rerank_results(\n  primary_results=data, search_query=translated_query\n)\n```\nThen after this, I run a helper function on products to extract filters. Filters, are a set of values extracted for different attributes, like color, material, price range, source country, and etc. that are used to filter the results after the search. \n```\nfunctions.get_or_extract_filters(\n    item_list=data[:250],\n    search_query=search_query,\n    search_lang=search_lang,\n    country=country,\n    currency=currency,\n)\n```\nAfter this, I save the filters on redis cache, and use the combination of `query + country + language + currency` as it's key, and also return the filters to the client side. Note that I have to generate the filters based on the search results and they are different based on the products list returned in search.\nNow that the user has completed first step of the search, he/she can filter the results more regarding their likings."
  },
  {
    "threadId": "1233828063939399791",
    "name": "Replace key name with other key.",
    "messages": "In my qdrant database there are 2 keys episodes, and episode , some data contains episode some contains episodes, I want all of them to become episode and standardize all of the database. If anyone has the idea please tell me."
  },
  {
    "threadId": "1233377237768867851",
    "name": "Can't restart Qdrant Cluster after out of memory issue",
    "messages": "I am facing an issue with my Qdrant Cluster. After encountering an out of memory (OOM) issue, I am unable to restart the Qdrant Cluster. When I try to restart the cluster, it fails to start up and I'm seeing error messages related to the OOM issue.\nAny help or advice on troubleshooting and resolving this problem would be greatly appreciated. Please let me know if you need any additional information from me."
  },
  {
    "threadId": "1233361450437836861",
    "name": "Wait for indexing of small update to index",
    "messages": "Hi,\nIn the change logs of version 1.9.0, it is stated that \"Remove vectors_count from collection info because it is unreliable\".\nI have been using `vectors_count` to check if the indexing of the new vectors I added has been finished on top of the status (GREEN, YELLOW) of the collection.\nThe reason I have been using `vectors_count` is because from time to time I have a very small update to the index that will not trigger the indexing because it is smaller than the default `indexing_threshold`. So basically if I see that the status is green but `vectors_count` >  `indexed_vectors_count`,  I temporarly change the `indexing_threshold` to force the indexing.\n\nWith `vectors_count` gone/not being reliabale, anyone has an alternative or a good strategy for this case?\nIs setting `indexing_threshold` at basically 1 all the time viable?\n\nNote that I cannot afford not to index those new vectors until new ones come in so that the default `indexing_threshold` is trigerred."
  },
  {
    "threadId": "1233359068484079697",
    "name": "Pool of clients, does it make sense?",
    "messages": "My api has to work with different databases, which are PG (for retrieving data regarding user info, product source country, etc), Qdrant (which is my main product db, implemented to use semantic search and recommendation scenarios), MongoDB (which is used for logs) and Redis (which I use for caching results). \nMy first approach was to create async clients for each one, and share it through the api (which causes each api to create a new client, and close it when it is finished:\n```\n@rec_app.get(\"/v1/products/rec/\", status_code=200)\nasync def recommend(\n    request: Request,\n    product_id: int = None,\n    language: str = DEFAULT_SEARCH_LANG,\n    currency: str = DEFAULT_CURRENCY,\n    country: str = DEFAULT_COUNTRY,\n    category: int = None,\n    page: int = 1,\n    per_page: int = 10,\n    authorization: str = Header(None),\n):\n  # create an async mongoDB client to use in all the api\n    async_mongo_client = mongo_funcs.get_async_mongo_client()\n\n    # create an async qdrant client to use in all the api\n    async_qd_client = qd_funcs.async_qdrant_client()\n\n    # create an async redis client to use in all the api\n    async_redis_client = await redis_funcs.get_redis_client()\n\n    # Create an async PostgreSQL client to use in all API endpoints.\n    async_pg_conn = await product_funcs.pg_connect()\n\n... use the clients thorugh the api, and close them at the end:\nasync_mongo_client.close()\n    await async_qd_client.close()\n    await async_pg_conn.close()\n```\nBut then I did some more research, finding out that for pg for example, I can create async connection pool, and use it application level instead of creating single async connections in api level. \nI wonder if I can do something similar with qdrant client, as there will be a lot of users visiting these pages simultaneously, hence a lot of clients will be created."
  },
  {
    "threadId": "1233353583794196521",
    "name": "Qdrant filtration -> prioritize a condition, but not inforce it",
    "messages": "Hello dear Qdrant support team. There is a specific scenario in our filtration and search. So, we have a field in our payload named `source_country`:\n```\n{\n  ... other fields in payload,\n  \"source_country\": \n    {\n      \"id\": 45,\n      \"iso\": \"cn\", \n      \"name\": \"china\"\n    },\n  ... other fields in payload,\n}\n```\nWe also may have other source countries, like Turkey, etc. The filtration  that I want to add, is to retriece the china products at first, but also return others as well. I just want to prioritize the china source, without excluding other countries. \nThe reason I want to do this with Qdrant and not after retrieving the results, (which makes more sense to just re-order the results myself), is due to the limitation I am facing for pagination. I am using FastAPI with Qdrant, which forces me to handle pagination in Qdrant side using `offset` and `limit` keywords in `search` and `recommend` apis. So, I am only getting `limit` number of products each time, which is equal to `per_page` param sent by the client, which is 12 or 10 mostly. \nSo, re-ordering 12 products is not going to work here. \nIf you think my approach is wrong in other places, and I can do something else entirely to not face this prioritizing issue, please let me know. If more details is needed, just tell me what you need to know in order to offer help.\n\nP.S. I checked this `order_by` payload key doc in the scroll api, (I am not using scroll, but anyway), it says:\n\"When you use the order_by parameter, pagination is disabled.\"\n\nThanks in advance for your time and help."
  },
  {
    "threadId": "1233317569159495743",
    "name": "Pricing of Qdrant in AWS Marketplace",
    "messages": "What is per unit usage? I want to understand what unit refers to exactly?\n\nThanks"
  },
  {
    "threadId": "1233084359863435356",
    "name": "Qdrant memory utilization monitoring",
    "messages": "We have a Grafana dashboard monitoring Qdrant memory utilization. It alerts when the container available memory is < 10%. Specifically, we are looking at the `container_memory_rss` metric. \nIn the past, we noticed Qdrant would perform poorly when it ran out of memory, so we added this alert as a leading indicator. However, the past few times this alert has gone off, Qdrant was performing normally even though it was using ~95% of available memory. Our workaround at this time is to restart our Qdrant service.\nI'm looking for advice on the following:  \n1) I'm assuming Qdrant is using all of its available memory as a cache. Is there a way to cap this cache so that it stops growing after some limit?\n2) It looks like this is all working as designed/intended. Is there a better metric to observe to alert on qdrant performance?"
  },
  {
    "threadId": "1233075174153130014",
    "name": "Variables in Qdrant console",
    "messages": "How do I create variables in the console to consume them.\nEx.: collection_name = \"collection_test\"\n\nPOST collections/<collection_name >/points/scroll\n\nThank you in advance for your efforts to help."
  },
  {
    "threadId": "1233070618983923732",
    "name": "Slow Search Speed",
    "messages": "Hi,\nWe have a database of around 80 million dense vectors. All 7 payload fields are indexed. (int/float/keyword fields only)\nSearch queries with a limit of 500 and a few filters take randomly between 5-30 seconds. \nWe have a fairly low read volume for these larger requests but a continuous stream of around 30/minute small searches.\nWe have a write rate of around 200 points/minute (batched)\nI have tried to improve the search speed with these params (returning one payload field and id only)\n ```\"search_params\": models.SearchParams(\n hnsw_ef=16,\n exact=False,\n quantization=models.QuantizationSearchParams(rescore=False),\n indexed_only=False,\n )```\nThe server specs are: \nr4.4xlarge - https://instances.vantage.sh/aws/ec2/r4.4xlarge\nCPUs 16\nMem(gb)    122.0\nI imagine there is a more optimal setup with more nodes/replicas/shard config?\nConfig:\n```{\n  \"params\": {\n    \"vectors\": {\n      \"\": {\n        \"size\": 768,\n        \"distance\": \"Cosine\",\n        \"hnsw_config\": {\n          \"m\": 16,\n          \"ef_construct\": 100,\n          \"full_scan_threshold\": 10000,\n          \"on_disk\": false\n        },\n        \"quantization_config\": {\n          \"scalar\": {\n            \"type\": \"int8\",\n            \"always_ram\": false\n          }\n        },\n        \"on_disk\": true\n      }\n    },\n    \"shard_number\": 4,\n    \"replication_factor\": 1,\n    \"write_consistency_factor\": 1,\n    \"on_disk_payload\": true\n  },\n  \"hnsw_config\": {\n    \"m\": 16,\n    \"ef_construct\": 100,\n    \"full_scan_threshold\": 10000,\n    \"max_indexing_threads\": 0,\n    \"on_disk\": false\n  },\n  \"optimizer_config\": {\n    \"deleted_threshold\": 0.2,\n    \"vacuum_min_vector_number\": 1000,\n    \"default_segment_number\": 0,\n    \"max_segment_size\": null,\n    \"memmap_threshold\": 20000,\n    \"indexing_threshold\": 50000,\n    \"flush_interval_sec\": 5,\n    \"max_optimization_threads\": null\n  },\n  \"wal_config\": {\n    \"wal_capacity_mb\": 32,\n    \"wal_segments_ahead\": 0\n  },\n  \"quantization_config\": null\n}```"
  },
  {
    "threadId": "1233033792592285739",
    "name": "ResponseHandlingException",
    "messages": "Hey i am running a cluster and having trouble inserting points into my vector store. It was working few days ago on a test cluster now its not and wonder if i misconfigured something. \n\nHere is how im initialising my client and vector store. I am using llamaindex for ingestion.\n\ndef returnClient():\n    return QdrantClient(\n        url=\"https://a9bf2042-7061-4e54-9e4b-561dfd41c4af.us-east-1-0.aws.cloud.qdrant.io\", \n        api_key=os.getenv(\"QDRANT_API_KEY\"),\n        port=6333\n    )\n\ndef returnVectorStore(collectionName):\n    client = returnClient()\n    return QdrantVectorStore(client=client, collection_name=collectionName)\n\n    vector_store = returnVectorStore(indexName)\n\npipeline = IngestionPipeline(\n        documents=documents,\n        transformations=[\n            SentenceSplitter(chunk_size=512, chunk_overlap=102),\n            Settings.embed_model\n        ],\n        vector_store=vector_store\n    )\n\n    pipeline.run(documents=documents, show_progress=True)"
  },
  {
    "threadId": "1232724635024625744",
    "name": "Hi, I'm starting with Qdrant today and need some java client examples",
    "messages": "I did look at https://github.com/qdrant/java-client but there is not enough examples there. When I try to add maven dependency of 1.9.0 in my project which is having JDK 21, I'm getting the following error and unsure what other dependencies are needed to be added in my project. I thought only the below is sufficient, but it looks like that's not the case,\n```\n        <dependency>\n            <groupId>io.qdrant</groupId>\n            <artifactId>client</artifactId>\n            <version>1.9.0</version>\n        </dependency>\n```\nAny help is appreciated! Thank you in advance!"
  },
  {
    "threadId": "1232635710893326366",
    "name": "building index on array payload.",
    "messages": "My payload  contains array as field  .i.e. item_sub_ids = ['id1', 'id2', 'id3']. I want to build the index based on id values in the ids field to improve the search time but it remains the same even after indexing. Here I add the search query and indexings. \n\n**Search Query: **\ndef get_search_result(vector, item_id, collection_name=collection_name, \n                             no_of_vecs=50, with_vectors=False, params=None):\n \n    if not vector:\n        print(f\"invalid vector {vector}\")\n        return None\n    url = f\"{base_url}/collections/{collection_name}/points/search\"\n    data = {\n        \"vector\": vector,\n        \"limit\": no_of_vecs,\n        \"with_vectors\": with_vectors,\n        \"with_payload\": [\"pid\", 'item_sub_ids'],\n        \"filter\": {\n            \"must\": [\n                {\n                    \"key\": \"item_sub_ids\",\n                    \"match\": {\n                        \"value\": item_id\n                    }\n                }\n            ]\n        }\n        ,\n        \"params\": params\n    }\n    \n    resp = req.post(url=url, headers=headers, data=json.dumps(data))\n    return resp\n\n**Indexing**:\nurl=f\"{base_url}/collections/{collection_name}/index\"\ndata = {\n    \"field_name\": \"item_sub_ids\",\n    \"field_schema\": \"keyword\"\n}\nrequests.put(url=url, headers=headers, data=json.dumps(data)).json()"
  },
  {
    "threadId": "1232354587852148797",
    "name": "The Read Operation Timeout : Error",
    "messages": "within 8 hours is my project evaluation, and i am getting this error, but as i do a page referesh it start working, and suddenly atio the process and give this error, i dont know why.\n\nError : \nreturn self.api_client.request(\n  File \"D:\\Practice\\server\\myenv\\lib\\site-packages\\qdrant_client\\http\\api_client.py\", line 76, in request\n    return self.send(request, type_)\n  File \"D:\\Practice\\server\\myenv\\lib\\site-packages\\qdrant_client\\http\\api_client.py\", line 93, in send\n    response = self.middleware(request, self.send_inner)\n  File \"D:\\Practice\\server\\myenv\\lib\\site-packages\\qdrant_client\\http\\api_client.py\", line 202, in __call__\n    return call_next(request)\n  File \"D:\\Practice\\server\\myenv\\lib\\site-packages\\qdrant_client\\http\\api_client.py\", line 105, in send_inner\n    raise ResponseHandlingException(e)\nqdrant_client.http.exceptions.ResponseHandlingException: The read operation timed out"
  },
  {
    "threadId": "1232351605765181460",
    "name": "Supported dspy versions",
    "messages": "Hi guys, is someone using dspy here? \n\nbecause when you install dspy-ai\n\nit installs `dspy-ai version 2.4.5`\n\n\nbut when you install `pip install dspy-ai[qdrant]`\n  it downgrades dspy-ai to `2.0.4`\n and it raises error `ModuleNotFoundError: No module named 'dspy.retrieve.qdrant_rm'`\n\n\nany idea or clue which versions work together?"
  },
  {
    "threadId": "1232340736821821614",
    "name": "Checking on qdrant for running locally on Apple silicon",
    "messages": "I have an M2 Studio Max (96 GB integrated memory), and I'm wondering if qdrant is my solution for vector store on Apple silicon. I read an article that indicates that there are some ARM-based Docker images - which is great. I'm wondering if anyone has experience with them. This is for personal work, and it doesn't have to be so blazing fast that the queries are imperceptible. Stretch breaks are nice."
  },
  {
    "threadId": "1232201799822213130",
    "name": "What's the recommended way to upgrade official Helm chart with minimal data loss",
    "messages": "Hi community,\nI'm using the offical Qdrant Helm chart from GitHub page and recently I plan to upgrade the qdrant version to leverage the performance boost.\nAs I'm cautious about the data, I planed to implement a green blue deployment - I created a new release of the latest Qdrant (Chart verion 0.8.4) and I copied data volume from the existing Qdrant(Chart veresion 0.2.10) to the override corresponding one.\ndata transferring was carried by https://github.com/utkuozdemir/pv-migrate\n\nThe new qdrant cluster managed to load collections but it failed consensus check. Please see the logs in the attachment.\nIn short it's \"2024-04-23T04:59:14.408019Z ERROR qdrant::startup: Panic occurred in file src/main.rs at line 307: Can't initialize consensus: Failed to recover Consensus from existing Raft state: Failed to recover from any known peers\"\n\nCan anyone suggest if it's expected error since I used the data from old Helm release for new ones?\n\nBesides, what's the recommended way to upgrade qdrant version / official qdrant Helm chart?"
  },
  {
    "threadId": "1232041362602328084",
    "name": "what is the Maximum collection size can be in quadrant?",
    "messages": "what is the Maximum collection size in the quadrant that we can store?"
  },
  {
    "threadId": "1232005924638687242",
    "name": "How to avoid duplicate results when indexing news phrases by phrase?",
    "messages": "I'm facing an issue with indexing news phrases by phrase. The problem is that I have multiple phrases from the same news article, and when I search, I want to avoid having multiple results from the same article based on `idContent` who is in payload showing up separately. To solve this, I'm trying to merge the results with the same `idContent` together after search. However, this is causing an unexpected issue.\n\nWhen I request 30 results, I'm only getting 9 unique results because 21 of them are being merged with other news articles with the same idContent. Is there a way to avoid this reduction in results per page while still merging duplicate results from the same news article? Any help or suggestions would be greatly appreciated!"
  },
  {
    "threadId": "1231993829394223184",
    "name": "Copy vector from one collection to another collection",
    "messages": "Hello, anyone know if it's possible to copy vector from one collection to another ? \nFor example i'm trying to move a single file and it's associated vector from a collection to another collection. \nAfter a lot of time, It seem to me, i cannot do it without reencoding a second time my file.\nIs it possible without reencoding ?"
  },
  {
    "threadId": "1231949212208599040",
    "name": "Visualization fails on cloud console",
    "messages": "Something is tripping up the visualization process in the cloud dashboard for one of my collections:\n```\n{\n  \"limit\": 50, \n  \"vector_name\": \"vec_Message_huggingface_TaylorAI_bge-micro-v2_dim384\"\n}\n```\nI can consent to have someone look into the logs (https://d57afa86-62c8-4b17-9eb5-6143a248abb7.us-east4-0.gcp.cloud.qdrant.io:6333/dashboard#/collections/vdf_2024_9-11/visualize)"
  },
  {
    "threadId": "1231796528377499720",
    "name": "Retrieve all points in collection",
    "messages": "I need to retrieve all of the points in my collection together with payloads and vectors. I've written something like this\n`all_points = qdrant_client.get_collection(collection_name).points_count\nrecords_qdrant = qdrant_client.scroll(\n    collection_name=collection_name,\n    scroll_filter=models.Filter(\n        must_not=[\n            models.IsEmptyCondition(is_empty=models.PayloadField(key=\"document\"),)\n        ]\n    ),\n    limit=all_points,\n    with_payload=True,\n    with_vectors=True,\n)`\nIt is working but feels a bit strange. Especially because of this dummy filter condition I added. Field \"document\" will always have value, so effectively this filter is always returning true.\nIs there better way to achieve the same result?"
  },
  {
    "threadId": "1231633658461556786",
    "name": "undo payload indexing",
    "messages": "I want to undo a payload key which I indexed earlier, cant figure out how to do that now ?\nhow should i undo payload indexing ??"
  },
  {
    "threadId": "1231591873894813788",
    "name": "vector none",
    "messages": "I want to load a PDF file into Qdrant.\nSplit into pages and multiple chunks per page.\nThat works just fine with the client.add()  method.\nI can also create an embedding generator object.\nThe resulting Qdrant collection show  POINTS with the vector as None.\nWhat am I missing here?"
  },
  {
    "threadId": "1231587849757790289",
    "name": "uploading point on no existing collection",
    "messages": "when I add a point using collection name of which is not existing in the qdrant instance, it does not give me any error. Is there a way to handle this or should I do that on a seperate layer out of qdrant."
  },
  {
    "threadId": "1231526608985260122",
    "name": "Multiple vector search",
    "messages": "I have a collection with 200M points, each point has 2 types of vectors. I want to search with the first vector and get N points, then search with the second vector but only from the N points.\nIs there a way to do that? to create a filter based on a previous search?\nmaybe search with the first vector and get N point IDS and then make another search and filter for the N IDS?"
  },
  {
    "threadId": "1231319697496998009",
    "name": "Hybrid Search (NotImplementedError: Subclasses must implement this method)",
    "messages": "Hey Folks\nI am new to the vector db space. exploring qdrant\n\nI followed this article\nhttps://qdrant.tech/documentation/tutorials/hybrid-search-fastembed/\n\nto Create a Hybrid Search Service with Fastembed\n\nwhen i run this \n\n```client.add(\n    collection_name=\"startups\",\n    documents=documents,\n    metadata=metadata,\n    parallel=0,  # Use all available CPU cores to encode data. \n    # Requires wrapping code into if __name__ == '__main__' block\n)``` \n\nI am getting this error\n\n```NotImplementedError: Subclasses must implement this method``` \n\nPython Version: Python 3.11.8\nOS: Mac (M1 chip)\n\ncan anyone please advice on this   ðŸ˜ƒ  \n\nThank you"
  },
  {
    "threadId": "1231302763686662186",
    "name": "how to seach subsequence in payload key in database ?",
    "messages": "So, I want to perform search based on subsequence in the database but I couldnt find any method, for ex: if the query text is \"nar\", then I want to filter all the points where \"certain_key\" has \"nar\" as its subsequence, How should I achieve that ?\nI want to use this feature in Auto searchComplete in the frontend side, shall I use text-indexing will it eat up my RAM ? I have 25K records, and counting, so I dont want to waste my RAM either.\n\nAny Help or Advice is appreciated."
  },
  {
    "threadId": "1231148076258037791",
    "name": "Qdrant create_collection vs recreate_collection",
    "messages": "I want to create a collection if the collection does not exist. Out of the two methods mentioned which one does a check if a collection already exsist and if it does not exsist create one.\n\nI think it should be create_collection so i wanted to create one like below:\n\n`qdrant.create_collection(collection_name=collection_name,\n                                   vectors_config = models.VectorParams\n                                   (\n                                   size=model.get_sentence_embedding_dimension(),\n                                   distance=models.Distance.COSINE,\n                                    )\n                                    )`\n\nBut it give me an error:\n\n**create_collection = grpc.CreateCollection(\nTypeError: bad argument type for built-in operation**\n\nThe same parameters for recreate_collection works fine."
  },
  {
    "threadId": "1231077437278785636",
    "name": "Replicated cluster failing after losing single node",
    "messages": "ðŸ‘‹  \nWhat exactly are the guarantees of runing in cluster mode?\n\nFor context, we have 8 nodes. \n\nWe have one collection, 8 shards, with a replication factor of two.\n\n\nRecently we upgraded our kubernetes cluster in place, and it caused a catastrophic failure of qdrant.\n\nWe were only able to get back up and running by a complete fresh qdrant cluster reindexing from source.\n\n\nWe have a pod disruption budget as well. It appears to happen after the first node has been replaced, other nodes get into a crashloopbackoff. It's strange.\n\nI have a hunch this is related to the fact that we are storing the data on ephemeral storage now, and not a network connected ssd, because it was too slow.\n\nSo my question is - is qdrant _supposed_ to be able to recover shards from its peers? Or am I misunderstanding how qdrant in cluster mode is supposed to work?\n\n\nWhen we were using persistent volumes, we were able to delete a node and bring it back - but it seems when using local disk we cant currently. Curious as to why"
  },
  {
    "threadId": "1231038537764306965",
    "name": "Regarding handling of duplicated vectors",
    "messages": "Hi!\nI've noticed this GitHub issue - https://github.com/qdrant/qdrant/issues/1788 regarding handling of duplicated vectors. I'd like to get better understanding what are current limitations and whether you have any experience with regards to any threshold when this issue might start being noticeable?\n\nI have DB of product reviews and I noticed there are instances where I'll get similar reviews, meaning that distance between both records will be 0.0. Those occur because reviewer just copy-pasted same text in multiple places.\n\nAt the moment it is really small subset of **0.4%** of total DB so I don't think I should be very concerned. But anyhow I'm asking just in case."
  },
  {
    "threadId": "1230856406077407332",
    "name": "Count API with Vector Search",
    "messages": "I am currently implementing vector-search. For that I am using the `search` API  to return all points that exceed a certain `score_threshold`. I do not want to display all datapoints at once but implement a server-side pagination. I am aware of the `limit` and `offset` parameters, however, I do not know how many datapoints match my criteria. Therefore, I cannot compute the number of pages in advance. \nI am currently exploring the `count` API which I would like to call before the `search` API in order to know how many pages I need. However, I saw that the `count` API does not support a vector-based search. \nDo you have any ideas how I could build a vector-based search with server-side pagination?"
  },
  {
    "threadId": "1230842329137418310",
    "name": "Hybrid Search with keywords",
    "messages": "Hi guys. I have followed an example of hybrid search using llama index and Qdrant vector store. I am using splade for sparse vectors and allmptnet for dense vectors. Although, I would like to have results that match for specific terms, such as IDs. \n\nFor example, if the user asks:\nWhen the product XYZ was inserted?\n\nI would like to have results only about XYZ. Now I cannot guarantee that XYZ is even on the retrievals. Is that a way to force that?"
  },
  {
    "threadId": "1230841583641956412",
    "name": "Document Hierarchy",
    "messages": "Hi guys. I have been working on a RAG project. I need some clarification on how I can structure my data (chunks) in a hierarchical way. I would like to have a tree of documents. For example, if I have documents the talk about Dogs and Cats, I would like to have leaf nodes (sub documents) for each breed of Dog and Cat. \nIf the user asks \"What is the oldest golden retriever alive?\", the system should retrieve only the chunks of golden retrievers, which are inside Dogs index. Is there a way to do that in Qdrant?\n\nThanks in advance"
  },
  {
    "threadId": "1230571603591561277",
    "name": "Hybrid search",
    "messages": "Hi, I am trying to implement Hybrid search in qdrant, I have posts from a social network I am building and I am trying to build a post recommendation system based on dense and sparse embeddings, the issue is the in the qdrant documentation I see that the only way to create a hybrid query  is to batch search the dense embeddings and sparse embeddings and only after to merge them. the issue is that with this approach I rarely have overlapping results that I can weight properly. it is basically separated queries and I loose the benefits of the hybrid vectors. \nis there other way to query hybrid embeddings?"
  },
  {
    "threadId": "1230517852243628032",
    "name": "Aleph Alpha integration",
    "messages": "Hello, i am trying to integrate Aleph Alpha in google collab, and i am getting this issue: \n```\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-12-26efa69a3081> in <cell line: 18>()\n     28         }\n     29         query_request = SemanticEmbeddingRequest(**query_params)\n---> 30         query_response = await aa_client.semantic_embed(request=query_request, model=model)\n     31 \n     32         # Finally store the id, vector and the payload\n\n2 frames\n/usr/local/lib/python3.10/dist-packages/aleph_alpha_client/aleph_alpha_client.py in _raise_for_status(status_code, text)\n     81             raise BusyError(status_code, text)\n     82         else:\n---> 83             raise RuntimeError(status_code, text)\n     84 \n     85 \n\nRuntimeError: (403, '{\"error\":\"Access forbidden\",\"code\":\"UNAUTHORIZED\"}')\n```\ni first started by downloading and unzipping the dataset: \n```\n!wget http://images.cocodataset.org/zips/train2017.zip\n\n!unzip -q train2017.zip\n\n!ls train2017 | head\n```\nand this is the code i ran that gave me the error:"
  },
  {
    "threadId": "1230517323496951889",
    "name": "Max supported throughput on upsert",
    "messages": "I have been experimenting with the max throughput with which I can upload points to the qdrant cluster.\nFor the sake of experiment:\nI have a 15 node cluster - 64G nodes on qdrant cloud\n\nCluster config: \n'{\n   \"name\":\"temp\",\n   \"vectors\":{\n      \"vector\":{\n         \"size\":768,\n         \"distance\":\"Cosine\"\n      }\n   },\n   \"on_disk_payload\":true,\n   \"hnsw_config\":{\n      \"on_disk\":true,\n      \"payload_m\": 16,\n      \"m\": 0\n   },\n   \"quantization_config\":{\n      \"product\": {\n            \"compression\": \"x16\",\n            \"always_ram\": true\n        }\n   },\n    \"optimizers_config\": {\n        \"memmap_threshold\": 20000,\n        \"indexing_threshold\": 20000\n   },\n   \"shard_number\": 30\n}'\n\nI am upserting a batch of 1500 points with payload (5 fields) at each request. (using REST API for upserts)\n\nI am able to hit 20-26 rps with latency close to 1000ms. Any suggestions on the cluster config or things i could try out to increase this throughput? \n\nWhile i understand upsert seems a computationally expensive operation at qdrant, Aim is to be able to hit close to 50rps for upserts."
  },
  {
    "threadId": "1230493147562246155",
    "name": "What's the time complexity for the upsert and delete operation of a single point in Qdrant ?",
    "messages": "I am  curious what's the time complexity (average and worst case) for upsert and delete operation in Qdrant ? Is it same as the complexity of insertion, searching and deletion in Qdrant i.e long(n).  What are the best practices for performing these operations in Qdrant. Currently I am using the REST API for batch insertion and deletion."
  },
  {
    "threadId": "1230468490868031578",
    "name": "ID field with spark connector",
    "messages": "as you wrote here, it's should be possible to add id field with a simple string.\nI got error when I'm trying to do that. \nusing maven 2.2.0"
  },
  {
    "threadId": "1230455651860484157",
    "name": "using API-key auth",
    "messages": "I am trying to add api-key to authunticate using qdrant instance, and this is what I did:\n- added this in the custom_confid.yaml outside the qdrant docker in config folder with the down text\n- added cert.pem and key.pem at tls folder outside qdrant docker\n- using the down command to start qdrant docker container [tls is false as I am trying first to use api-key then will try tls] [api-key is different from the one inside the client http request to see my request refused]\n- the problem is that when tls and https are false, there is no effect of using api-key at all. And when they are open it gives me errors like wronge version of ssl or illigal request\n```\ndocker run -p 6333:6333 \\\n    -v $(pwd)/qdrant_storage:/qdrant/storage:z \\\n    -v $(pwd)/tls:/qdrant/tls \\\n    -v $(pwd)/config/custom_config.yaml:/qdrant/config/custom_config.yaml \\\n    qdrant/qdrant \\\n    ./qdrant --config-path config/custom_config.yaml\n```\n```\nclient = QdrantClient(db_machine_ip,\n                       port=6333, \n                       api_key=\"12345\" , https=False)\n```\n \n\n```\nservice:\n  # Set an api-key.\n  # If set, all requests must include a header with the api-key.\n  api-key: 123456\n  #\n  # If you enable this you should also enable TLS.\n  # (Either above or via an external service like nginx.)\n  # Sending an api-key over an unencrypted channel is insecure.\n   # Enable HTTPS for the REST and gRPC API\n  enable_tls: false\n# Check user HTTPS client certificate against CA file specified in tls config\n  verify_https_client_certificate: false\n\n# TLS configuration.\n# Required if either service.enable_tls or cluster.p2p.enable_tls is true.\ntls:\n  # Server certificate chain file\n  cert: ./tls/cert.pem\n\n  # Server private key file\n  key: ./tls/key.pem\n\n```"
  },
  {
    "threadId": "1230424637129625672",
    "name": "error sparse vectors is none for hybrid search",
    "messages": "Hi ! I am following this article https://qdrant.tech/documentation/tutorials/hybrid-search-fastembed/ to implement hybrid search on Qdrant cloud. \n\nHowever, I am getting error \"sparse_vector_field_name in collection_info.config.params.sparse_vectors\" - Argument NoneType is not iterable. \n\nFetching 7 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 3276.80it/s]\nFetching 9 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<?, ?it/s]\n  0%|          | 0/2 [00:00<?, ?it/s]exception occurred\nTraceback (most recent call last):\n  File \"\\hybrid_search_qd.py\", line 83, in add_data\n    self.qdrant_client.add(\n  File \"hybrid-search\\lib\\site-packages\\qdrant_client\\qdrant_fastembed.py\", line 496, in add\n    self._validate_collection_info(collection_info)\n  File \"hybrid-search\\lib\\site-packages\\qdrant_client\\qdrant_fastembed.py\", line 369, in _validate_collection_info\n    sparse_vector_field_name in collection_info.config.params.sparse_vectors\nTypeError: argument of type 'NoneType' is not iterable\nargument of type 'NoneType' is not iterable\n  0%|          | 0/2 [00:00<?, ?it/s]\n\nOn debug, i found that, in the function _validate_collection_info, collection_info.config.params.sparse_vectors is None.  I have called get_fastembed_sparse_vector_params in recreate_collection. Any idea?"
  },
  {
    "threadId": "1230399845676744805",
    "name": "Moving data after changing the embedding model",
    "messages": "Hi everyone! I hope that you're doing great!\nWe've switched from using an embedding model to another that has different victor dimensions. \nNow, we need to move the data from the collection with the previous vector dimension to a new one.\nWe're going to make a script that retrieves the points along with the payload, adds the point to a json file, embed the points again with the new model and then upsert them to the new collection and delete from the previous collection.\nDoes this sound good? Also, How do we retrieve all of the points? Couldn't find a start at and stop at in the docs"
  },
  {
    "threadId": "1230226876912505024",
    "name": "Compressed file ended before the end-of-stream marker was reached",
    "messages": "I'm currently using Resume-Matcher, a project on github which uses the qdrant library, and I keep running into the same EOF error in fast_embed even after switching python versions and redownloading the necessary modules. Let me know if I should provide more details, and I'm using macOS (mac m1, 16 gb ram). I'm using Qdrant locally, using qdrant_client-1.8.2 on python 3.11. I've also tried this on python 3.9.6."
  },
  {
    "threadId": "1229878684068876338",
    "name": "single-node snapshot loading fail due to Panic (using qdrant binary)",
    "messages": "I am using qdrant binary on windows wsl2 linux, simple snapshot creation works, but reload fails every time, this means that any ctrl-c pause on the qdrant server would completely destroy existing data since snapshot cannot be reloaded, is it expected? log trace attached"
  },
  {
    "threadId": "1229870165991362671",
    "name": "What language does FastEmbed support?",
    "messages": "Is there a model, which supports Russian language? I wanted to use FastEmbed, but not sure If it supports Russian language or not."
  },
  {
    "threadId": "1229857226294825020",
    "name": "Shard per tenant  VS Partition (Payload filtering) per tenant",
    "messages": "Why would I want to use payload filtering over a shard per tenant? What are the semantics around shards? Wouldn't I want to restrict searches/writes by tenant? About about moving tenant with ever growing vectors to their own node? \nPlease go into the technical details behind why payload filtering is more suited than shards. \n\nBest, LIam"
  },
  {
    "threadId": "1229854287514046465",
    "name": "Having issues with the deployment of Hybrid Cloud on GKE",
    "messages": "Hello.\n\nI was trying the new Hybrid Cloud approach.\n\nWas able to run all the installation comands.\n\nBut I get this error on the pod on Google Cloud - Back-off pulling image \"registry.cloud.qdrant.io/qdrant/qdrant-cloud-agent:1.0.0\": ImagePullBackOff\n\nWhat can I do?\n\nThank you"
  },
  {
    "threadId": "1229777476268458095",
    "name": "Point Sets as Versions",
    "messages": "Hey, short use case question:\nWe have a multi-tenant system which requires strong isolation, no downtime and optimize for cost efficieny. Tenants are everything but equally sized. Embedding models may be diverse but searches will be local to one embedding model.\n\nForm the docs I think a multi-node, single collection per embedding model setup is advised. We should then build a payload index over a tenant_id. Tenants will often have below 100k relevant dense vectors per search.\n\n**Now: We need to support multiple versions of VectorStores. In the common case, a set of vectors is extended by a small set of additions, deletions.** The customer needs to query both only the old version as well as only the new version. The trivial implementation would assign a versions: [] array string/int field to every Point and perform a MATCH ANY version_8978 clause in every search. \n-> Is this the best configuration for our use case? \n-> Are there known performance penalties for such a set membership pre-filtering?\n-> Do you have a better idea about how to encode versions? (e. g. since_version, until_version values, that at least reduce duplications for consecutivly included points per version?)"
  },
  {
    "threadId": "1229747802649923645",
    "name": "Kubernetes deployment fails to restore collection or snapshot (only full snapshots render a problem)",
    "messages": "Restarting Qdrant Pod fails to restore collection with full snapshot doesn't work while snapshot for each collections work -- leading to infinite CrashLoopBackOff with following message:\n\n```\nâ”‚ Stream closed EOF for default/qdrant-0 (qdrant)  \nRestoring snapshot into new collection in Kuberenes environment fails with:\nERROR qdrant::startup] Panic occurred in file lib/storage/src/content_manager/consensus/consensus_wal.rs at line 22: called `Result::unwrap()` on an `Err` value: Os { code: 11, kind: WouldBlock, message: \"Resource temporarily unavailable\" }\n```"
  },
  {
    "threadId": "1229384432968794122",
    "name": "creating config.yaml on local docker qdrant-storage",
    "messages": "I have a qdrant run as docker storage folder in my local machine. I want to add the api-key line in the config.yaml as mentioned in the documentation, but I can not found the config.yaml file in my folder hierarchy. As I understand, this file is different from the config file of each collection."
  },
  {
    "threadId": "1229362281297612810",
    "name": "How to persist data accross restarts and updates on Google Cloud",
    "messages": "Hello everyone.\n\nI've deployed qdrant on google cloud, used directly the docker hub official image on a google cloud run.\n\nWhat happens is that I think sometimes the service restarts and everything is deleted.\n\nHow can I persist everything?"
  },
  {
    "threadId": "1228867044829696050",
    "name": "fine-tuned BGEm3 custom sparse embedding",
    "messages": "I have a fine-tunned custom bgem3 model for sparse embeddings is there a way to integrate into qdrant and use that instead of spade?\nhere's the example\n\nhttps://huggingface.co/BAAI/bge-m3"
  },
  {
    "threadId": "1228461268554350592",
    "name": "Wildcard Search in Payload?",
    "messages": "I'm having a really difficult time \"querying\" my vectorized data by payload text.  I am using the /scroll and /search endpoints and trying to perform a wildcard search on payload key=text value like '%hello world%' (i know this isn't a valid api, but just trying to explain what i'm looking to achieve).  i've tried several iterations of filter, but not getting any results.  the entire sentence in the \"text\" payload is \"i am luke.  hello world.  goodbye\".  thanks!"
  },
  {
    "threadId": "1228368024869015623",
    "name": "For performance optimization, can we remove indexing from certain payload (not all)?",
    "messages": "I have 1 million data points of summary, and other data as a payload, i want to remove the indexing from ONLY SUMMARY  so it'll search faster does qdrant has any facility like this?\n\nalso can we add data type to qdrant payload? like date, string etc.?\n\nthanks in advance ðŸ™‚"
  },
  {
    "threadId": "1228267085277167636",
    "name": "I've deleted some points based on filtering but still its showing in the qdrant collection.",
    "messages": "code for deletion: \n\nfor doc_type in filtered_document_types:\n    client.delete(\n        collection_name=\"Doctype_dataset_1m\",\n        points_selector=models.FilterSelector(\n            filter=models.Filter(\n                must=[\n                    models.FieldCondition(\n                        key=\"Doctype\",\n                        match=models.MatchValue(value=doc_type),\n                    ),\n                ],\n            )\n        ),\n    )\n\nBefore and after collection:\n\n\nwill it take time to remove the points nd vectors from collection? or qdrant just doesn't refer those points in future but points stays there?"
  },
  {
    "threadId": "1228239379290394625",
    "name": "How to know QDRANT_MAIN_URL",
    "messages": "I have a cluster with 3 nodes (image). I wonder how to connect to the cluster using the Python qdrant_client. While searching, I saw this (img). It has 3 ips for 3 nodes (\"https://node-x.my-cluster.com:6333\") and one main url (\"https://my-cluster.com:6333\"). How can I find this URL in my settings?"
  },
  {
    "threadId": "1228207522482094190",
    "name": "Cannot restore a snapshot",
    "messages": "I have 2 setup of qdrant:\n- One with 1 node (no cluster enable) (1).\n- One with 3 nodes (cluster enabled) (2).\nI create a snapshot with setup (1) -> file.snapshot. Then, in setup (2), I upload that snapshot. Yesterday, everything worked fine. But when I recreated the cluster today, it only had information about collection, but there was no data.\nCan you help me with this? Thanks!"
  },
  {
    "threadId": "1228005880868634695",
    "name": "Slow search performance with is_empty even with index?",
    "messages": "Hi, I am struggling to identify the performance issue causing is_empty to slow down searches even with an index created over the field (present on 1% of docs).\n\nI assume its probably a configuration error:\n\n```\n{\n    \"status\": \"green\",\n    \"optimizer_status\": \"ok\",\n    \"vectors_count\": 36417058,\n    \"indexed_vectors_count\": 36417058,\n    \"points_count\": 36059989,\n    \"segments_count\": 20,\n    \"config\":\n    {\n        \"params\":\n        {\n            \"vectors\":\n            {\n                \"size\": 512,\n                \"distance\": \"Cosine\"\n            },\n            \"shard_number\": 3,\n            \"replication_factor\": 2,\n            \"write_consistency_factor\": 1,\n            \"on_disk_payload\": true\n        },\n        \"hnsw_config\":\n        {\n            \"m\": 16,\n            \"ef_construct\": 100,\n            \"full_scan_threshold\": 10000,\n            \"max_indexing_threads\": 0,\n            \"on_disk\": true\n        },\n        \"optimizer_config\":\n        {\n            \"deleted_threshold\": 0.2,\n            \"vacuum_min_vector_number\": 1000,\n            \"default_segment_number\": 0,\n            \"max_segment_size\": null,\n            \"memmap_threshold\": 20000,\n            \"indexing_threshold\": 20000,\n            \"flush_interval_sec\": 5,\n            \"max_optimization_threads\": 1\n        },\n        \"wal_config\":\n        {\n            \"wal_capacity_mb\": 32,\n            \"wal_segments_ahead\": 0\n        },\n        \"quantization_config\": null\n    },\n    \"payload_schema\":\n    {\n        \"group_id\":\n        {\n            \"data_type\": \"keyword\",\n            \"points\": 36057546\n        },\n        \"label\":\n        {\n            \"data_type\": \"keyword\",\n            \"points\": 449696\n        }\n    }\n}\n```\n\nThe queries that I am trying to run are:\n- a recommend operation only over documents with `label`s AND a filter against a specific group\n- a scroll operation over documents with a `label` and a specific group id.\n\nWe thought creating an index would help resolve this issue, but so far it has not."
  },
  {
    "threadId": "1227911787333029958",
    "name": "How to speed up Creating new shard replicas",
    "messages": "Hi all!\nWhen I add a new node to the cluster, I have to move data from the old node to the new one. The process is fine, but when I check the log, I see this (image). Perhaps my server is only using one CPU for this process. Actually, I feel the process is a bit slow. Can you help me with this?\nThanks."
  },
  {
    "threadId": "1227891656058212372",
    "name": "How to log as json format?",
    "messages": "I deployed qdrant server in kubernetes using helm chart.\nI want to set qdrant logs to json format.\nBut current(default) option is string format like this\n`[{timestamp} INFO  {logger}] {ip} \"POST {url} HTTP/1.1\" 200 132 \"-\" \"python-httpx/0.27.0\" 0.000221`\nIs it possible to change the log on the qdrant server to json format?"
  },
  {
    "threadId": "1227807132552925236",
    "name": "\"Wrong input: Vector params for fast-bge-small-en-v1.5 are not specified in config\"",
    "messages": "Hi, \nI've indexed data into Qdrant Cloud using the rust client, wtih fastembed-rs -- indexing and querying data works great, the cluster is healthy and everything is smooth. \n\nI'm now trying to access the same cluster using the python qdrant client with the fastembed mixin and get the error message above. I've tried setting the model on the client with `qdrant_client.set_model(\"BAAI/bge-small-en-v1.5\")` but still get the same error. \n\nCan you point me in the right direction on where I need to configure things?"
  },
  {
    "threadId": "1227700250681151508",
    "name": "health checks on 1.7.1",
    "messages": "I have qdrant deployed to production on version 1.7.1 (readyz health check unavailable).\nWe are aware that upgrading the version from 1.7.1 -> 1.7.2 is a way to fix the problem with the readyz health check. \nThis upgrade adds some downtime for us which Iâ€™d like to avoid. \nGiven that, are there any other alternatives for us to get these health checks working?"
  },
  {
    "threadId": "1227633508139139103",
    "name": "Shard Not Active Error",
    "messages": "Hi, \nI am trying to implement distributed qdrant. I have already qdrant with several collections and cluster INFO looks like this: \n`{\n  \"result\": {\n    \"status\": \"enabled\",\n    \"peer_id\": 2979974762066354,\n    \"peers\": {\n      \"2979974762066354\": {\n        \"uri\": \"http://172.31.24.252:6335/\"\n      },\n      \"1418878837160198\": {\n        \"uri\": \"http://172.31.24.177:6335/\"\n      }\n    },\n    \"raft_info\": {\n      \"term\": 9607,\n      \"commit\": 257,\n      \"pending_operations\": 0,\n      \"leader\": 2979974762066354,\n      \"role\": \"Leader\",\n      \"is_voter\": true\n    },\n    \"consensus_thread_status\": {\n      \"consensus_thread_status\": \"working\",\n      \"last_update\": \"2024-04-10T14:48:21.798830364Z\"\n    },\n    \"message_send_failures\": {}\n  },\n  \"status\": \"ok\",\n  \"time\": 0.00000901\n}`\n\nI updated my collections replication factor to 2, and I want to have 2 nodes with 1 replace each. So I want to copy my shards to new node by following command: \n`POST /collections/test_collection/cluster\n{\n  \"replicate_shard\": {\n    \"shard_id\": 0,\n    \"from_peer_id\": 2979974762066354,\n    \"to_peer_id\": 1418878837160198\n  }\n}`\n\nbut I got this error: \n`{\n  \"error\": \"Bad request: Shard 0 is not active on peer 2979974762066354\"\n}`\nError as a log: \n\n`2024-04-10T14:52:56.673689Z  WARN storage::content_manager::consensus_manager: Failed to apply collection meta operation entry with user error: Bad request: Shard 0 is not active on peer 2979974762066354    `\n\n\nHowever, it looks active when I look info section from qdrant web UI.  Any idea why I am getting this error ?\n\n\nLeader node version : v1.8.1\nFollower node version : 1.8.4\nBoth served on docker containers in differrent servers"
  },
  {
    "threadId": "1227277948923543654",
    "name": "Hybrid search",
    "messages": "Hi I am exploring Qdrant for my company's use. I would like to be able to create a standard inverted-index for full text search, and vectors for semantic search. Ideally, the framework selected would be able to perform a hybrid search with ranking (and deduplication) itself, as well as be able to automatically create the inverted index from a json payload (like how meilisearch does) with weightings (e.g. title more important than body).\n\nI see hybrid search is on your public roadmap (https://github.com/qdrant/qdrant/blob/master/docs/roadmap/README.md#core-milestones). Is this something you aim to do? Sparse vectors with splade look interesting but I would prefer a regular full-text search + vectors.\n\nMeilisearch-like sparse vector capabilities built into Qdrant would be amazing (though meilisearch is limited to 10 words per query so that's not great for RAG). I think Weaviate can do this but from testing I prefer Qdrant for it's speed and DX.\nThanks"
  },
  {
    "threadId": "1227218889356607550",
    "name": "fastembed query",
    "messages": "Hi all,\n\nThis query is regarding using fastembed with qdrant.\nAm checking the following link:\nhttps://qdrant.github.io/fastembed/\nwith default embedding it advices  \"query\" and \"passage\" prefixes for the input text.\n\nMy question is that, when we use the query method, example:\nsearch_result = client.query(\n    collection_name=\"demo_collection\",\n    query_text=\"This is a query document\"\n)\n\nshould we add the 'query: ' prefix for query_text as well?"
  },
  {
    "threadId": "1227211327852777492",
    "name": "Qdrant Custom Prefix Not working when accessing the dashboard",
    "messages": "My current configuration: I have istio setup for routing stuff. I have created an virtual service to route it to the actual service. Istio hits the virtual service. When i try to access the url \nhttps://dummy/qdrant/prefix/dashboard\nit just drops the prefix `qdrant/prefix`\n\nFrom the request:\nauthority: dummy\n:method: GET\n:path: /dashboard/manifest.json\n:scheme: https\n\nalso while it says https in the logs of istio it shows http 2 which is grpc\nand the /dashboard is only giving an issue\n\nit works fine with port forwarding or whenever i assign an ip\n\nThe python client does not work when i use the whole given url but when i pass prefix as a seperate param it works fine"
  },
  {
    "threadId": "1227089302207467520",
    "name": "migrate consistently fails with count mismatch error",
    "messages": "We are moving cloud providers and I am trying to use the migrate method in qdrant client to migrate our 4 node cluster but it's erroring saying the source count of points doesn't match the destination count.\n\nWhat would cause an error like this? It's strange to me because it's wait true on the add points and I have its write consistency and replicas set to 2 so it's definitely writing them down in multiple places. I'm tempted to delete the assert"
  },
  {
    "threadId": "1226942235400999045",
    "name": "Deadline exceeded on Upserts that don't set a deadline on the client request.",
    "messages": "I'm using the golang client to insert records in a qdrant cluster running on a kubernetes in a distributed deployment. My requests don't have a deadline set, and gRPC defaults to no deadline per their docs, but I'm seeing upsert requests timeout on healthcheck with the error below. Is there a way to control this timeout, say disable it / increase it? Also curious how healthchecks get issued? Does the client healthcheck the node before issuing the upsert request, or is this an internal request in the cluster?\n\nrpc error: code = DeadlineExceeded desc = Timeout: Timeout error: Deadline Exceeded: status: DeadlineExceeded, message: \"Healthcheck timeout 2000ms exceeded\", details: [], metadata: MetadataMap { headers: {} }\n\nThank you!"
  },
  {
    "threadId": "1226910655035080785",
    "name": "How do I delete a metadata keyword within a payload - child_keywords",
    "messages": "When I was building my rag. I created what I thought would be helpful but seems it actually made the retreival worse.\n\n```\nchild_node.metadata[\"child_keywords\"] = child_node_keywords_list\n```\n\n```\nchild_keywords\n[\n0:\"keyword1\"\n1:\"keyword2\"\n2:\"keyword3\"\n3:\"keyword4\"\n]\n```\n\nIs there a way to delete just this metadata field without deleting the entire point/payload and or having to recreate the entire rag? \n\nThe one issue I could see is that the child keyword is still inside the `_node_content`"
  },
  {
    "threadId": "1226782983915307059",
    "name": "MMR search with QdranrClient",
    "messages": "Hi,\n\nI don't see an implementation of \"MMR\" search with \"QdrantClient\" API (https://github.com/qdrant/qdrant-client/blob/master/qdrant_client/qdrant_client.py#L242). Do we have search_type=\"mmr\" to be added in the future release.\n\nIs this feature only available for orchestrators like Langchain/LLAMA-Index, or can we apply that with OOB client as well.\n\nIn langchain, we can do the search in the following way:\n\nqdrant.max_marginal_relevance_search (FOR MMR)\nqdrant.similarity_search (FOR SIMILARITY search)\n\nhttps://python.langchain.com/docs/integrations/vectorstores/qdrant/\n\nAny help appreciated."
  },
  {
    "threadId": "1226777655328636928",
    "name": "Access qdrant docker with in other application docker",
    "messages": "How to access my qdrant docker runinng with a mounted path access it within other docker ?"
  },
  {
    "threadId": "1226185059204202526",
    "name": "How to use Qdrant for classification?",
    "messages": "I have a list of topics, e.g: [expenses, profit, risks] etc.\n\nI have a big pdf (around 500 pages). \n\nI want to classify each page in that pdf; e.g.:\n\npage-1: [expenses, risks]\npage-2: [profit]\npage-3: []\n\netc.\n\nIs it possible to acheve with Qdrant?"
  },
  {
    "threadId": "1225480019552239626",
    "name": "Update only id for points in the collection",
    "messages": "Hi, I have a question, Is there a way to update / change the id of each point in a collection and remains everything else the same? Or is there a proper way to do this?\nAfter checking https://qdrant.tech/documentation/concepts/points/?q=points#update-vectors, \nit seems the update vector (following example): id=1 and id=2 are the points to be updated, but there's no place to  put the new ids? say if I want to update this two points (id=1 and id=2) to (id=3 and id=4) while keeping everything else untouched, what's the best way to do it other than retrieve the points (and vector) and re-upload to the collection?\n\n\nExample in the doc for python client:\n`client.update_vectors(\n    collection_name=\"{collection_name}\",\n    points=[\n        models.PointVectors(\n            id=1,\n            vector={\n                \"image\": [0.1, 0.2, 0.3, 0.4],\n            },\n        ),\n        models.PointVectors(\n            id=2,\n            vector={\n                \"text\": [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2],\n            },\n        ),\n    ],\n)`"
  },
  {
    "threadId": "1221804967803752478",
    "name": "Having one vector column for multiple text columns on Qdrant - V2",
    "messages": "I previously asked a question regarding \"Having one vector column for multiple fields\" asking if the method I tried is correct or not. (Please go to the link: https://stackoverflow.com/questions/78095982/having-one-vector-column-for-multiple-text-columns-on-qdrant). Now I have another idea thinking I can do this with separate columns and weight distribution. I do not know why, but your input is appreciated here:\nhttps://stackoverflow.com/a/78219166/4429265"
  },
  {
    "threadId": "1219814428753526904",
    "name": "Sparse Vectors on disk and other optimization tips for large scale processing?",
    "messages": "I'm running into trouble with resource limitations on my data processing pipeline.\nCurrently running a processing pipeline on computer with 64GB of RAM. Once you add in chunking / embedding processes while also hosting a qdrant image with a large amount of vectors the resource usage really starts to add up.\nI've tried to set everything to 'on disk'. Despite this the RAM still seems to increase. I am then experiencing various errors where things cannot process which appear to be related to limited resource availbility.\n\nSo far I have set\n- Dense vector to disk: True\n- Payload to disk: True\n- HNSW index to disk: True\n- Sparse vectors: Can sparse vectors be stored on disk rather than RAM? It's not clear what they are by default.\n\nCan anyone advise on the above or any other tips on how to manage data creation / upload pipelines while keeping resource usage low.\nScale of the dataset is in the order of (10M to 100M+ dense and sparse vectors). Can qdrant be set to an 'upload only' mode during processing?\n\nKind regards"
  },
  {
    "threadId": "1219624192169869392",
    "name": "Problem with node",
    "messages": "We had a problem in a node (we have a master and another node) and no data replication. The extra node is not able to get consensus with the master, but the data is in the disk.  We also tried upgrading both nodes to 1.8.2 (before they were at 1.7.3) and it is throwing the following error:\n\nI attach the output of all errors and raft states.\n\nCan anyone help on how could we solve it?"
  },
  {
    "threadId": "1217327445904986162",
    "name": "How to check Shards count and which Shard is on Which node",
    "messages": "I have qdrant running in AKS with helm.\nAKS has 2 Nodes but my 2STS(statefulsets) deployed automatically on same node.\nDevelopers pushed 1k collections with 10shards.\n1.Now we want to see shards count and which collection and Nodes have shards?\n2.in documentation mentioned that Node*2 Shards but where is it configured, how to check?\n3.i have only 2pods running on same node,what is the use of maintaining other node? Only for shards? Shards are in disk level or pod level or CPU/RAM?"
  },
  {
    "threadId": "1209414347227660369",
    "name": "Creating a snapshot for restoring in another server",
    "messages": "Hi,\nFirst, Thanks for the amazing product!\nI am using Qdrant Docker for my application. I have deployed a docker container in an ec2 instance(dev server) . Now I am trying to build my application in a new ec2 instance (think stage server), in this I want the same data and embeddings I've created in dev server. I want to restore the snapshot in my stg server.\nJust like how I can take a backup .tar or .sql file and restore it in another server, I want to be able to restore my qdrant data in the same way. How can I acheive this?"
  },
  {
    "threadId": "1202894907098791936",
    "name": "Shard move error",
    "messages": "Im moving shared from one cluster node to another cluster not - Getting this error \"No space left on device (os error 28)\"\nIm running qdrant on docker.\nQuestion - \nIts transfer dependent on OS disk storage of source or destination.\n\n\nLogs\n2024-02-02T07:43:11.041818Z ERROR collection::shards::transfer::driver: Failed to transfer shard us_fto_collection_v7:2 -> 6881351561678924: Service internal error: Can't move file from /qdrant/./storage/tmp/us_fto_collection_v7-shard-2-2024-02-02-07-06-32.snapshot-6HFH3x to ./snapshots/us_fto_collection_v7/shards/2/collection_v7-shard-2-2024-02-02-07-06-32.snapshot due to No space left on device (os error 28)"
  },
  {
    "threadId": "1149354934152732733",
    "name": "Before you ask...",
    "messages": "**Reporting issues on Discord**\n\nBefore you create a report, first check whether a similar issue has already been brought up by someone else. \nCheck the troubleshooting docs:\nhttps://qdrant.tech/documentation/guides/common-errors/ \nGo to the GitHub issue search and look through open and closed issues. If the issue has been fixed, try to reproduce it using the latest version of Qdrant.\n\n**Required information**\n\nSummarize the issue. Describe what you are trying to do and how you are being blocked. Let us know what should have been the result.\n\nAre you using Qdrant in local mode, with Docker or Qdrant Cloud? If you are using Qdrant Cloud, please provide the cluster id.\n\nWhich Qdrant version are you running? Sometimes the issue happens because you are not running the latest version.\n\nHow are you connecting to Qdrant? Are you using a client?\n\nProvide your current database configuration. What are the current collection settings? How many vectors in a collection? In case of a distributed setup, describe shards, replication factor etc.\n\nWhat is your server configuration? If your question is related to performance, we need to know the operating system version, CPU, RAM size and storage details.\n\nIs there anything showing in the server logs? Also, provide screenshots of monitoring or log excerpts that illustrate the issue.\n\n**Reproduce the issue**\n\nWe need reproducible code or a script that demonstrates the problem. Please give us step-by-step instructions to recreate the situation. For errors or problems with queries:\nShow the exact code of the query or the request.\nPaste the exact output of the response or the error message you are getting back.\n\nTry to recreate the problem in a controlled environment. This helps you confirm that the issue is consistent. If possible, identify the specific conditions that trigger the problem.\n\nWhat were you doing when the error happened? Is there a common pattern?\nHave you attempted any workarounds?"
  }
]